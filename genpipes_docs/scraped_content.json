[
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/",
    "title": "GenPipes v6.0.0 Documentation",
    "content": "GenPipes v6.0.0 Documentation\nUsage Change Effective v5.x onward\nUsers of GenPipes v4.x or prior releases must carefully read the usage changes listed below.\nWhat has Changed?\nEffective v5.0 onward, the GenPipes distribution is based on Python packaging and not on Python modules as before. Also Python 2.7 users must upgrade to Python v3.11.1 or higher to use v5.x GenPipes pipelines.\nThe latest GenPipes v6.0 supports only Python v3.12.0 and above.\nTo run any GenPipes pipelines, follow the new command syntax (effective GenPipes v5.x onward):\nuser@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nuser@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nWhen using GenPipes deployed on the Digital Research Alliance Canada servers such as Rorqual, a new variable ‘GENPIPES_INIS’ is introduced for streamlining access to the config files in the genpipes commands. In the future, ‘MUGQIC_PIPELINES_HOME’ will be deprecated.\n$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini\n$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini\nPlease note that the old variable, ‘MUGQIC_PIPELINES_HOME’ will still be accessible and is still in use for instructions on how to deploy GenPipes locally, in the cloud, or in a container.\nA new Long Read DNA Sequencing pipeline is now available in v6.0 that supports two protocols:\nThe Nanopore pipeline available in v5.0 and earlier releases is no longer supported. The same functionality is now available as one of the supported protocols in the new Long Read DNA Sequencing pipeline.\nThe RNA Sequencing (De Novo) pipeline has been updated in v5.0 release.\nThe EpiQC pipeline, HiC-Seq pipeline, and AmpliconSeq qiime protocol have been deprecated starting v5.0 onward.\nThe DNA-Seq high coverage pipeline and the TumorPair pipeline have been merged into a single workflow DNA-Seq.\nThe Methylseq pipeline has a new protocol option using the gemBS aligner in addition to Bismark.\nGenome build GRCh38 (human) is now the default reference genome for all pipelines, but other versions or species can be selected via config files, as before.\nMarkdown style reports have been deprecated for all pipelines starting v5.0 onward and replaced entirely with MultiQC reports.\nGenPipes is developed and financed by the Canadian Centre for Computational Genomics (C3G).\nC3G is a core platform affiliated with McGill University, with broad expertise in bioinformatics. It offers bioinformatics analysis and HPC services for the life sciences research community. Our services include bespoke pipeline development, service for a fee analyses, as well as an extensive suite of software solutions in the -omics fields. GenPipes is our open-source workflow and pipeline platform. For more information about our team and funding, please check our website.\nFighting COVID-19\nGenPipes offers the CoVSeq pipeline (for short-read sequencing, e.g. Illumina) and Nanopore_CoVSeq pipeline (for long-read sequencing) to help researchers analyze viral sequences and detect mutations quickly, with the goal of preventing the spread of new strains. See GenPipes CoVSeq Pipeline User Guide, GenPipes Nanopore CoVSeq Pipeline User Guide for details.\nGenPipes is a flexible Python-based framework that facilitates the development and deployment of multi-step genomic workflows, optimized for High-Performance Computing (HPC) clusters and the cloud. It offers 10 open-source, validated and scalable pipelines for various genomics applications.\nGenPipes documentation is organized to address the needs of new users as well as seasoned users and contributors. Refer to the Documentation Map for details on how GenPipes documentation is organized.\nTable of Contents\nGet Started\nDeploying GenPipes\nUsing GenPipes\nContributions\nPublications\nTest Datasets\nC3G Resources\nDevelopment\nRelease Notes\nRelease Instructions\nDeveloper Guide\nTroubleshooting Guide\nDocumentation",
    "code_blocks": [
      "user@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "user@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini",
      "$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini"
    ],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/index.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/gp_usecases.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/gp_faq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/how_to_deploy_genpipes.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/index.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/user_guide.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/tutorials/list_tutorials.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/support/how_to_get_support.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/community/channels.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/community/contributing.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/citation.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/publications.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/workshops.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/testdataset.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/compute_resources.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/development/release_notes.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/development/gp_release_instructions.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/development/dev_guide.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/development/troubleshooting_guide.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/documentation/about.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/documentation/genpipes_doc_archmap.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/documentation/docs_changelog.html"
    ],
    "scraped_at": "2025-08-26T09:59:08.446868",
    "content_length": 3952,
    "word_count": 556
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/index.html",
    "title": "About",
    "content": "Usage Change Effective v5.x onward\nUsers of GenPipes v4.x or prior releases must carefully read the usage changes listed below.\nWhat has Changed?\nEffective v5.0 onward, the GenPipes distribution is based on Python packaging and not on Python modules as before. Also Python 2.7 users must upgrade to Python v3.11.1 or higher to use v5.x GenPipes pipelines.\nThe latest GenPipes v6.0 supports only Python v3.12.0 and above.\nTo run any GenPipes pipelines, follow the new command syntax (effective GenPipes v5.x onward):\nuser@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nuser@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nWhen using GenPipes deployed on the Digital Research Alliance Canada servers such as Rorqual, a new variable ‘GENPIPES_INIS’ is introduced for streamlining access to the config files in the genpipes commands. In the future, ‘MUGQIC_PIPELINES_HOME’ will be deprecated.\n$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini\n$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini\nPlease note that the old variable, ‘MUGQIC_PIPELINES_HOME’ will still be accessible and is still in use for instructions on how to deploy GenPipes locally, in the cloud, or in a container.\nA new Long Read DNA Sequencing pipeline is now available in v6.0 that supports two protocols:\nThe Nanopore pipeline available in v5.0 and earlier releases is no longer supported. The same functionality is now available as one of the supported protocols in the new Long Read DNA Sequencing pipeline.\nThe RNA Sequencing (De Novo) pipeline has been updated in v5.0 release.\nThe EpiQC pipeline, HiC-Seq pipeline, and AmpliconSeq qiime protocol have been deprecated starting v5.0 onward.\nThe DNA-Seq high coverage pipeline and the TumorPair pipeline have been merged into a single workflow DNA-Seq.\nThe Methylseq pipeline has a new protocol option using the gemBS aligner in addition to Bismark.\nGenome build GRCh38 (human) is now the default reference genome for all pipelines, but other versions or species can be selected via config files, as before.\nMarkdown style reports have been deprecated for all pipelines starting v5.0 onward and replaced entirely with MultiQC reports.\nGenPipes is an open-source Python-based tool for managing Next Generation Sequencing (NGS) genomics workflows. It comes with a set of high-quality analysis pipelines, optimized for high-performance computing (HPC) and cloud environments.\nSince 2013, these pipelines have been tested, refined, and widely used. GenPipes simplifies complex genomics analysis, making it accessible not just for bioinformatics pros but also for students and researchers. It offers flexible, reliable tools for standard genomic studies and can be extended for advanced research.\nPerfect for anyone needing fast, proven bioinformatics solutions.\nYou can learn more about GenPipes, its target audience, sponsors, how the project has evolved over the years and license information in the following section.\nWhy GenPipes?\nTarget Audience\nRelease Notes\nYou can refer to the detailed list of changes for past and current releases of GenPipes through the Release Notes.\nGetting Help\nGenPipes is under active development. If you need help, would like to contribute, or simply want to talk about the project with the team members, we have a number of open channels for communication.\nTo report bugs or file feature requests: use the GenPipes issue tracker. For details, see the GenPipes Support Page.\nTo talk about the project with people in real time: join the GenPipes Channel on Google Groups. For more details refer to the Channels page.\nTo contribute code or documentation changes: submit a pull request on GenPipes GitHub repository.\nFor citations, publications, workshops,test data sets and other resources, visit Resources page.",
    "code_blocks": [
      "user@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "user@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini",
      "$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini"
    ],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/why_genpipes.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/target_audience.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/sponsors.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/history.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/license.html"
    ],
    "scraped_at": "2025-08-26T09:59:09.647699",
    "content_length": 3866,
    "word_count": 566
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/gp_usecases.html",
    "title": "Use Cases",
    "content": "GenPipes offers several genomic analysis pipelines that can be used for various bioinformatics analyses.\nThese can be utilized for performing a wide range of standardized and tailored analysis. Following is a brief summary that highlights potential use case for each of the GenPipes Pipelines.\nC3G has extensive experience analyzing data from various sequencing applications with state of the art computation methods. It offers various services to next-generation sequencing researchers who wish to use these pipelines for advanced genomic analysis. For customized and case-by-case analysis service, submit your request today.\nFollowing is a summary of real-life application use case for each of the GenPipes Pipelines:\nDetecting COVID-19 Mutations\nGenPipes CoVSeQ Pipeline has been used as part of CoVSeQ project for genome sequencing to detect COVID-19 gene mutations.  CoVSeQ deals with real-time tracking of Quebec SARS-CoV-2 evolution.\nCoVSeQ is a partnership between the Institut National de Santé Publique du Québec (INSPQ) and the McGill Genome Centre to sequence the viral genome of Quebec patients with COVID-19 disease. The viral samples are taken from a Quebec viral biobank, termed the CoVBanQ, which is hosted in the Laboratoire de Santé; Publique du Québec (LSPQ).\nYou can obtain a real-time snapshot of evolving SARS-CoV-2 populations in Québec and access interactive data visualizations. It is meant for virologists, epidemiologists, public health officials and citizen scientists. Through interactive data visualizations, CoVSeQ allows exploration of continually up-to-date datasets, providing a novel surveillance tool to the scientific and public health communities.\nFor more details, visit CoVSeQ Website and Sequencing Workflows.\nGenPipes Release version 4.1 supports a new sequencing pipeline to support Nanopore CoVSeQ analysis.  For details, visit Nanopore CoVSeQ Pipeline.\nRNA Sequencing\nThe RNA Sequencing Pipeline helps to quantify transcripts and genes using a reference genome and test for differential expression across experimental conditions. It is also possible to call single nucleotide variants (SNVs), detect fusion gene events and assess alternative splicing events.\nDenovo RNA Sequencing\nThe De Novo RNA Sequencing Pipeline assembles reads to transcripts in the absence of a reference genome, annotates transcripts and tests for differential expression. This is well suited to organisms that are not yet characterized.\nmiRNA Sequencing\nThis pipeline can be used to quantify known miRNAs, discovers novel ones using a reference genome and performs differential expression analysis. Additional analysis may include pathway testing, target candidates analysis or miRNA editing.\nSARS-CoV-2 Genome Sequencing\nThe SARS-CoV-2 genome Sequencing Pipeline is designed for COVID-19 Coronavirus research and surveillance, enabling complete genome sequencing of the new SARS-CoV-2 virus responsible for the COVID-19 pandemic.\nSARS-CoV-2 Nanopore CoVSeQ\nThe SARS-CoV-2 Nanopore CoVSeQ Pipeline is designed to implement ARTIC SARS-CoV2 protocol, Version 4 / 4.1 (V4.1), using Nanopolish. This protocol is closely followed in GenPipes Nanopore sequencing pipeline with majority of changes related to technical adaptation of the protocol to be able to run in a High Performance Computing (HPC) environment.\nDNA Methylation\nThe Methylation Pipeline helps to analyze data coming from bisulfite converted DNA assayed by various sequencing assays such as RRBS, CpG capture, whole genome sequencing or microarrays. Our analysis computes methylation levels and performs differential analysis between experimental conditions.\nAmplicon Sequencing\nThe Amplicon Sequencing Pipeline can process Illumina, PacBio pyrotags amplicons from the 16S, 18S or ITS amplicons. OTUs are picked and diversity is analyzed within and between communities. Further analyses include differential abundance testing or metagenome functional content prediction.\nDNA Sequencing\nThe DNA Sequencing Pipeline offers state of the art DNA-seq analyses detects and annotates variants in whole exomes, whole genomes or high coverage amplicons. The analysis can also be pushed further by assisting with variant prioritization, or perform advanced cancer related analysis.\nChIP Sequencing\nThe ChIP Sequencing Pipeline helps in analyzing DNA fragments from immunoprecipitated chromatin by calling alignment peaks on the genome, annotating the said peaks and performing additional analyses such as motif enrichment and discovery. Designed experiments can be analyzed by testing for differential binding between experimental conditions.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:10.850025",
    "content_length": 4624,
    "word_count": 646
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/gp_faq.html",
    "title": "FAQ",
    "content": "Queries related to GenPipes and its usage are categorized broadly as follows:\nGenPipes Developers\nC3G Resources\nGenPipes HPC Jobs\nWhat is GenPipes?\nWhere can I find GenPipes Citation reference and text?\nWhere can I access latest GenPipes source code?\nDo I need to deploy GenPipes locally on a server or in the cloud to use it?\nGenPipes requires familiarity with Unix. As a Windows user, what do I need to setup to use GenPipes?\nWhere do I find tips on troubleshooting GenPipes runtime and usage issues?\nFor new developers, are there any GenPipes development and troubleshooting tips?\nTo create a new CCDB account, what should I fill in the form field: ‘position’?\nWhat email ID should I use if I am an external collaborator?\nMy account is activated. How do I learn more about Compute Canada Servers and resources available?\nMy account is activated but I cannot login into Rorqual server or any other node such as Nibi, Fir and Narval? What is wrong?\nWhat is the best place to report GenPipes bugs?\nI was trying to use GenPipes deployed in a Docker Container. What <tag> value should I use?\nHow do I run GenPipes locally in my infrastructure using a containerized setup? Is a scheduler setup necessary?\nGenPipes 3.4 RNA Sequencing Pipeline moved from using Trimmomatic to Skewer. Why? What does this change mean for GenPipes users?\nHow do I modify parameters/ options for specific tools or change computational resources that require a job to run?\nPytest command on CCDB server results in command not found error. Pytest install fails.\nC3G Resource Usage\nWhere can I find more details on CCDB servers, file system, usage guidelines?\nIs there a list of software installed on Digital Research Alliance servers?\nWhat are modules and why do we need them for GenPipes?\nWhat are GenPipes genomes? Where can I access them from?\nWhat is meant by test dataset? Where can I find available test datasets?\nGenPipes HPC Jobs\nHow do I run GenPipes pipelines when the number of steps or jobs exceeds HPC site queue limit?\nIn case of an error or job timeout, do I need to re-run the entire GenPipes Pipeline script over again or is there a smarter way to submit only the failed jobs?",
    "code_blocks": [],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_general.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_new_users.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_gp_dev.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_c3g_res.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_hpc.html"
    ],
    "scraped_at": "2025-08-26T09:59:12.090809",
    "content_length": 2166,
    "word_count": 376
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/how_to_deploy_genpipes.html",
    "title": "Deploying GenPipes",
    "content": "Deploying GenPipes\nThis document lists various ways by which GenPipes can be accessed and installed by users. Check out various available deployment options, if you wish to install and setup GenPipes on your infrastructure.  If you wish to use pre-installed GenPipes deployed on Compute Canada infrastructure, refer to Accessing GenPipes on Digital Research Alliance Canada servers.\nDeployment Options\nDRAC Deployment\nLocal Deployment\nContainer Deployment\nCloud Deployment",
    "code_blocks": [],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_options.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/access_gp_pre_installed.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_gp_local.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_gp_container.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_gp_cloud_gcp.html"
    ],
    "scraped_at": "2025-08-26T09:59:13.300309",
    "content_length": 473,
    "word_count": 66
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/index.html",
    "title": "Using GenPipes",
    "content": "Using GenPipes\nWelcome to the GenPipes Using GenPipes Guide!\nThis guide shows you how to get started with running genomic analysis pipelines supported by GenPipes, including how to deploy and access the software, reference genomes, tune and customize the pipelines, specify input configurations and setup the environment as per the pipeline pre-requisites.\nIf you wish to learn more about real life applications of GenPipes and its use cases refer to the GenPipes Applications section in the left hand side navigation pane.\nThis guide consists of the following topics to assist you in understanding and using GenPipes for solving complex bioinformatics research problems.\nIntroduction\nGenomic Analysis with GenPipes\nAnalyzing GenPipes Results\nTroubleshooting",
    "code_blocks": [],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/what_is_genpipes.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/using_gp.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/gp_job_results.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/troubleshooting_rt_issues.html"
    ],
    "scraped_at": "2025-08-26T09:59:14.415013",
    "content_length": 759,
    "word_count": 111
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/user_guide.html",
    "title": "User Guide",
    "content": "User Guide\nUsage Change Effective v5.x onward\nUsers of GenPipes v4.x or prior releases must carefully read the usage changes listed below.\nWhat has Changed?\nEffective v5.0 onward, the GenPipes distribution is based on Python packaging and not on Python modules as before. Also Python 2.7 users must upgrade to Python v3.11.1 or higher to use v5.x GenPipes pipelines.\nThe latest GenPipes v6.0 supports only Python v3.12.0 and above.\nTo run any GenPipes pipelines, follow the new command syntax (effective GenPipes v5.x onward):\nuser@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nuser@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nWhen using GenPipes deployed on the Digital Research Alliance Canada servers such as Rorqual, a new variable ‘GENPIPES_INIS’ is introduced for streamlining access to the config files in the genpipes commands. In the future, ‘MUGQIC_PIPELINES_HOME’ will be deprecated.\n$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini\n$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini\nPlease note that the old variable, ‘MUGQIC_PIPELINES_HOME’ will still be accessible and is still in use for instructions on how to deploy GenPipes locally, in the cloud, or in a container.\nA new Long Read DNA Sequencing pipeline is now available in v6.0 that supports two protocols:\nThe Nanopore pipeline available in v5.0 and earlier releases is no longer supported. The same functionality is now available as one of the supported protocols in the new Long Read DNA Sequencing pipeline.\nThe RNA Sequencing (De Novo) pipeline has been updated in v5.0 release.\nThe EpiQC pipeline, HiC-Seq pipeline, and AmpliconSeq qiime protocol have been deprecated starting v5.0 onward.\nThe DNA-Seq high coverage pipeline and the TumorPair pipeline have been merged into a single workflow DNA-Seq.\nThe Methylseq pipeline has a new protocol option using the gemBS aligner in addition to Bismark.\nGenome build GRCh38 (human) is now the default reference genome for all pipelines, but other versions or species can be selected via config files, as before.\nMarkdown style reports have been deprecated for all pipelines starting v5.0 onward and replaced entirely with MultiQC reports.\nGenPipes is an open-source, next generation sequencing analysis framework written in Python. Developed at the Canadian Centre for Computational Genomics (C3G), GenPipes facilitates writing and execution of multi-step genomics processing pipelines on HPC clusters. It offers several validated bioinformatics pipelines to analyze many kinds of -omics data, including DNA (including tumor-pair), RNA, SARS-CoV2, Ch-IP, Methylation and Hi-C sequencing, to name a few.\nGet Started\nNew users can get started by referring to the Get Started Guide. The seasoned users can directly jump into the Pipeline Reference Guide.\nThe latest GenPipes Version is 6.0.0. Refer to the release notes for more information regarding what is new, which pipelines were updated and what issues are fixed in this release.\nTo contribute, see Getting involved.\nHappy genomic analysis with GenPipes!\nQuick Start\nGenPipes Check List\nChoose GenPipes Deployment Option\nGet GenPipes\nRun GenPipes\nGetting Help on GenPipes\nGetting Involved\nPipelines Reference\nAmplicon Sequencing Pipeline\nChIP Sequencing Pipeline\nCoV Sequencing Pipeline\nDNA Sequencing Pipeline\nLong Read DNA Sequencing Pipeline\nMethylation Sequencing Pipeline\nNanopore CoVSeQ Pipeline\nRNA Sequencing Pipeline\nRNA Sequencing (De-Novo) Pipeline\nRNA Sequencing (Light) Pipeline\nDNA Sequencing (High Coverage) Pipeline (Merged)\nEpiQC Pipeline (Deprecated)\nHiC Sequencing Pipeline  (Deprecated)\nTumor Pair Sequencing Pipeline (Merged)\nGetting Involved\nContributing to GenPipes\nTracking GenPipes Issues and Feature Requests\nReporting a bug or issue",
    "code_blocks": [
      "user@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "user@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini",
      "$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini"
    ],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/quickstart_gp.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/gp_checklist.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/choose_gp_dep.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/get_gp.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/run_gp.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/find_help.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/get_involved.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipeline_ref.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_ampliconseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_chipseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_covseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_dnaseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_longread_dnaseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_wgs_methylseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_nanopore_covseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_rnaseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_rnaseq_denovo.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_rnaseq_light.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_dnaseq_highcov.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_epiqc.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_hicseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_tumourpair.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/get_involved.html"
    ],
    "scraped_at": "2025-08-26T09:59:15.634064",
    "content_length": 3844,
    "word_count": 542
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/tutorials/list_tutorials.html",
    "title": "Tutorials",
    "content": "GenPipes is a flexible Python-based framework that facilitates the development and deployment of multi-step genomic workflows, optimized for High-Performance Computing (HPC) clusters and the cloud.  It offers open sourced, several validated and scalable pipelines for various genomics applications.\nThere are multiple ways to deploy and run GenPipes. For details, see GenPipes Deployment Guide.\nThe GenPipes tutorials listed below demonstrate how GenPipes can be deployed and run for first time users.  Please note that the tutorials correspond to the GenPipes deployment type.  If you are deploying GenPipes on the cloud, then refer to the GenPipes in the Cloud tutorial below.  Use the first tutorial listed below if you are accessing GenPipes pre-deployed on the Digital Research Alliance of Canada,formerly Compute Canada, servers.\nTutorial: Using C3G Deployment\nTutorial: Using Google Cloud Platform (GCP) Deployment\nTutorial: Using Local Containerized Deployment\nPipeline Usage Examples",
    "code_blocks": [],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/tutorials/genpipes_tutorial.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/tutorials/genpipes_in_the_cloud.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/tutorials/genpipes_in_the_container.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/tutorials/pipeline_usage_examples.html"
    ],
    "scraped_at": "2025-08-26T09:59:16.761365",
    "content_length": 992,
    "word_count": 140
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/support/how_to_get_support.html",
    "title": "Need Help?",
    "content": "Need Help?\nGenPipes is released as open-source software. For details about its license policy, see GenPipes License. The GenPipes developers offer continuous support through GenPipes Google Forum. You can drop an email to the GenPipes Help Desk for any support issues.\nGenPipes is sponsored by the Canadian Centre for Computational Genomics (C3G). Besides the Google Forum and Help Desk, you can attend C3G Open Door Sessions. This facility is available only to local (McGill University, Montréal) bioinformaticians, students and researchers. You need to fill a form and RSVP in order to attend a session. For details visit C3G Open Door link.\nReporting Bugs\nIf you are facing any issues in deploying or using GenPipes, please report bugs via GenPipes Support Email.\nMessages should not be sent directly to our team members. The generic e-mail addresses above are viewable by all of us and facilitate the follow-up of your request.\nChoose a meaningful subject for your message.\nInclude the pipeline version number in your message (and the commit number if applicable).\nProvide the following information relevant to the problem encountered: the python command, the bash submission script, the output (job_outputs//.o) file,\nAn error message or code snippet illustrating your request is normally very useful.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:17.992461",
    "content_length": 1308,
    "word_count": 203
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/community/contributing.html",
    "title": "Contributions",
    "content": "Contributions\nThe GenPipes project team welcomes contributions from the community!\nWe are thrilled to receive your inputs and try to process them as fast as we can.\nThere are multiple ways in which you, as a user of GenPipes can contribute.\nGenPipes User feedback and inputs related to usage, documentation or issues, if any.\nGenPipes Developer inputs in the form of code fixes and new feature requests, forks and code evolutions\nTechnical Writer contributions in the form of improvisations, new content etc.\nGenPipes Usage related Contributions\nFor all GenPipes usage related feedback and contributions, please visit Get involved with GenPipes.\nGenPipes Developer Contributions\nPlease refer to the GenPipes Developers Guide for guidelines on how to submit code, make feature requests and change requests into GenPipes sources.\nTechnical Writer Contributions\nIf you are a technical writer and would like to submit your contributions for GenPipes documentation, please refer to the guidelines regarding contributing to GenPipes documentation.\nThank you for your inputs!",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:20.431740",
    "content_length": 1072,
    "word_count": 160
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/citation.html",
    "title": "Citations",
    "content": "This section gives instructions on how to cite GenPipes Publications and lists examples of citing articles.\nGenPipes Citation\nWhen citing GenPipes for a publication, please cite the following article in you paper:\nYou can download the citation from the Download section below. A complete listing of all GenPipes publications is available in the Publications section under GenPipes Resources Navigation link on the left hand side of this documentation.\nCitation Examples\n(June 2019) Colorectal cancer-derived extracellular vesicles induce transformation of fibroblasts into colon carcinoma cells. See PDF here.\n(May 2018) Whole Genome Sequencing for Mutation Discovery in Rare Neuro-developmental Disorders. Refer to the PDF here.\n(June 2019) GenPipes: an open-source framework for distributed and scalable genomic analysis. GigaScience, Volume 8, Issue 6,.\nBioRxiv GenPipes Paper DOI.\nDownload GenPipes Citation\nPlain Text.\nDownload Method Descriptions\nFollowing are the methods descriptions for the DNA-seq and RNA-seq pipelines. GenPipes users can use these descriptions of methods in their manuscripts.\nRefer to the method description for DNA Sequencing Pipeline\nRefer to the method description for RNA Sequencing Pipeline",
    "code_blocks": [
      "Refer to the method description",
      "Refer to the method description"
    ],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/_downloads/ab3cecf277b37843b7db6e94d24962bc/gp_citation.txt",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/_downloads/e31e49871beea7c2ff32168ed8cbe836/gp_citation.bib"
    ],
    "scraped_at": "2025-08-26T09:59:21.548952",
    "content_length": 1229,
    "word_count": 173
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/publications.html",
    "title": "Publications",
    "content": "Publications\nThis document contains information on available current publications and other media such as scientific papers, citations, videos etc. that can help new GenPipes users build a better understanding of GenPipes and how it works.\nPapersVideosPresentations\nGigaScience Article June 2019: GenPipes: an open-source framework for distributed and scalable genomic analysis.\nBiorxiv Archives Nov 2018 : GenPipes Paper.\nPlease be aware that these videos were produced using older versions of GenPipes, their content might not reflect the way the current version of GenPipes is run. Please refer to the other documentation for the most up-to-date information.\nRefer to the video below to learn how to run C3G-GenPipes on Digital Research Alliance of Canada (DRAC), formerly Compute Canada, cluster.\nAmplicon Sequencing generates metagenomics output. The Metagenomics Figures video shows how to build custom OTU figures and tables from C3G’s metagenomics output.\nAmplicon Sequencing pipeline Trimmomatic16S Step generates 16S analysis data that can be examined and plotted as demonstrated in 16S analysis video.\nChIP Sequencing Slides.\nDNA Sequencing Slides.\nRNA Sequencing Slides.",
    "code_blocks": [
      "ChIP Sequencing Slides",
      "DNA Sequencing Slides",
      "RNA Sequencing Slides"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:22.677523",
    "content_length": 1183,
    "word_count": 169
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/workshops.html",
    "title": "Workshops",
    "content": "This document contains GenPipes training resources from recent workshops. Canadian Centre for Computational Genomics (C3G) facilitates, organizes and collaborates with establishments to assist interested folks in acquisition of knowledge in bioinformatics analysis.  Visit the website for latest information on upcoming workshops.\nPlease be aware that these workshops were created using previous versions of GenPipes. As such, they might not reflect the syntax of the latest versions. Please consult the documentation for the most up-to-date information.\nLecturesPracticalsExtrasClick here for Lectures related to the RNA Sequencing Analysis Workshop Lectures.\nClick here for Practicals related to the RNA Sequencing Analysis Workshop.\nClick here for other RNA Sequencing Analysis Workshop resources.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:23.874461",
    "content_length": 800,
    "word_count": 107
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/resources/testdataset.html",
    "title": "Test Datasets",
    "content": "Test Datasets\nYou can execute various GenPipes Pipelines using the following types of data:\nThe real data which is generated from your genomic analysis instruments and then measured, sampled and read into various specified bioinformatics data formats.\nTest datasets that are available in the absence of real genomic analysis data.\nTest Dataset in the context of GenPipes refers to the dataset that needs to be analyzed by one of the GenPipes Pipelines. It can either be real data or sample data that is used to run the pipeline. Test dataset refers to some dataset that user can use to have hands-on on the pipeline. It is typically a smaller datasets (for e.g., one chromosome only and few samples for instance) so that the test runs of the pipelines using sample data get completed quickly say for demonstration purposes.\nTest dataset is different from readset file which is input to the pipeline.  For other kinds of inputs required for GenPipes pipelines, see here.\nIn contrast to the test dataset, a Readset File in the context of GenPipes actually describes the dataset (test dataset or real dataset) so that the pipeline can understand the type of data and process it.  Readset file is provided as input to almost all the GenPipes pipelines. Readset file contains information about the data to analyze; the path of the raw files, the type of sequencing, the name of the samples, etc.\nPlease remember to use the correct dataset for the respective GenPipes pipelines.  The table below lists the test dataset download link for each of the GenPipes pipeline. Do not use the test dataset specified for a different pipeline.\nGenPipes Pipeline\nTest Dataset\nAmplicon Seq\nDownload Amplicon Seq Dataset\nDownload ChIP Seq Dataset\nDownload DNA Seq Dataset\nLong Read DNA\nDownload Long Read DNA Seq Dataset\nNanopore Covseq\nDownload Nanopore Covseq Dataset\nDownload RNA Seq Dataset\nDownload Methyl Seq Dataset\nThe CoV Seq test dataset is not publicly available due to privacy restrictions. Contact support for further queries about CoV Seq Test dataset.\nTest Dataset Usage Examples\nFor various GenPipes pipelines, you can refer to usage examples and commands for issuing pipeline jobs using various options in the individual pipeline reference guide listed above or a short summary here.\nBioinformatics resources\nIf you are looking for Bioinformatics resources such as available genomes with FASTA sequence, aligner indices and annotation files listed on Bioinformatics resources C3G website page, you can download those from the public repositories using scripts provided in GenPipes Repository.\nYou can also download the latest test datasets from Computational Genomics website download page.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:25.101381",
    "content_length": 2689,
    "word_count": 429
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/development/release_notes.html",
    "title": "Release Notes",
    "content": "Release Notes\nThe first release of GenPipes into open source was made in 2013. GenPipes 2.0.0 dropped in 2014, and by Q2 2017, the community had run around 3,000 analyses, processing 100,000 samples.\nSince then, the number of GenPipes runs utilizing C3G HPC resources have grown significantly.\nThe current release of GenPipes is 6.0.0.\nv6.xv5.xv4.xv3.xv2.x\nGenPipes 6.0.0 released on April 15, 2025\nGenPipes 5.1.0 released on Feb 4, 2025\nGenPipes 5.0.3 released on Dec 10, 2024\nGenPipes 5.0.2 released on Dec 5, 2024\nGenPipes 5.0.1 released on Sept 24, 2024\nGenPipes 4.6.1 released on May 22, 2024\nGenPipes 4.6.0 released on May 1, 2024\nGenPipes 4.5.0 released on Feb 10, 2024\nGenPipes 4.4.5 released on Nov 14, 2023\nGenPipes 4.4.4 released on Sept 11, 2023\nGenPipes 4.4.3 released on Aug 24, 2023\nGenPipes 4.4.2 released on June 22, 2023\nGenPipes 4.4.1 released on Mar 14, 2023\nGenPipes 4.4.0 released on Mar 9, 2023\nGenPipes 4.3.2 released on Dec 8, 2022\nGenPipes 4.3.1 released on Oct 4, 2022\nGenPipes 4.3.0 released on June 15, 2022\nGenPipes 4.2.0 released on June 1, 2022\nGenPipes 4.1.2 released on Feb 17, 2022\nGenPipes 4.1.1 released on Feb 10, 2022\nGenPipes 4.1.0 released on Feb 7, 2022\nGenPipes 4.0.0 released on Dec 9, 2021\nGenPipes 3.6.2 released on Nov 11, 2021\nGenPipes 3.6.1 released on Oct 4, 2021\nGenPipes 3.6.0 released on Sept 8, 2021\nGenPipes 3.5.0 released on July 13, 2021\nGenPipes 3.4.0 released on May 3, 2021\nGenPipes 3.3.0 released on Feb 22, 2021\nGenPipes 3.2.0 released on Jan 26, 2021\nGenPipes 3.1.5 released on Jan 16, 2020\nGenPipes 3.1.4 released on Mar 26, 2019\nGenPipes 3.1.3 released on Dec 18, 2018\nGenPipes 3.1.2 released on Nov 22, 2018\nGenPipes 3.1.0 released on Apr 9, 2018\nGenPipes 3.0.0 released on Nov 30, 2017\nGenPipes 2.3.0 released on Feb 28, 2017\nGenPipes 2.2.1 released on Dec 19, 2016\nGenPipes 2.2.0 released on Feb 9, 2016\nGenPipes 2.1.1 released on Apr 14, 2015\nGenPipes 2.1.0 released on Feb 5, 2015\nGenPipes 2.0.2 released on Jan 13, 2015\nGenPipes 2.0.1 released on Dec 17, 2014\nGenPipes 2.0.0 released on Dec 12, 2014\nGenPipes ChangeLog\nSee Genpipes GitHub repository changelog.\nv5.x or lower\nSee Bitbucket repository for the GenPipes ChangeLog.\nSince we use git, there are many ways to get the details in many formats.\nOne of our preferred ways is to use a script that lists the commits by tag and author, written by the author of the Ray assembler: Sébastien Boisvert.\nAre you enjoying Genpipes?\nWe’d love to hear your inputs, feedback, if any!",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:27.553125",
    "content_length": 2501,
    "word_count": 429
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/development/dev_guide.html",
    "title": "Developer Guide",
    "content": "Developer Guide\nThis guide provides information for developers who are interested in contributing to GenPipes sources.\nRefer to GenPipes sources in GitHub.\nUse a local dev branch in order to make your changes / modifications / enhancements and issue a Pull Request from your local dev branch to GenPipes dev branch.\nYour PR will be reviewed by GenPipes Dev team reviewers. If the changes are acceptable, those will be merged into the GenPipes/dev branch.\nFor any further queries regarding contributing to GenPipes, refer to README.md file.\nYou may also wish to refer to the GenPipes Coding Guidelines.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:29.914512",
    "content_length": 602,
    "word_count": 97
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/documentation/about.html",
    "title": "About Docs",
    "content": "About Docs\nThis GenPipes documentation is continuously written, corrected, edited, and revamped by\nmembers of the GenPipes community.\nIt is edited via text files in the\nreStructuredText markup\nlanguage and then compiled into a static website/offline document using the\nopen-source Sphinx and ReadTheDocs tools.\nYou may also want to refer to the GenPipes New Documentation Architecture Map for more insights on how GenPipes documentation is organized.\nUser feedback is valuable to us.\nFor any documentation feedback, queries or any issues, please refer to GenPipes Documentation Project and open issues with the following label:\ndocumentation\nThank you for your valuable inputs.\nYou can contribute to GenPipes documentation.\nContribution Guidelines\nWe are happy to receive your contributions to GenPipes documentation!\nBefore you begin to make edits to GenPipes documentation, we would strongly recommend that you discuss and share your plans through a GitHub issue with the GenPipes documentation owner(s), especially for more ambitious contributions.\nHere are some guidelines to contribute to GenPipes documentation:\nFork the repository, deploy Sphinx and its dependencies.\nFamiliarize yourself with the organization and structure of GenPipes documentation in terms of information architecture. Refer to the GenPipes documentation map to understand how the content sources are organized. Get familiar with the TOC and information flow.\nUse references within the text instead of duplicating information.  Use search feature to identify the keyword and their mapping in the TOC.\nFollow the GenPipes Writing style guidelines while adding or updating content into GenPipes documentation.\nBefore you delete any content, make sure it is not relevant in some user context.\nMake sure, any content you add is targeted and tailored to the needs of GenPipes audience - new users or seasoned GenPipes users.\nReferences\nUsing Sphinx and ReadTheDocs for building documentation of Python projects - a very informative reference.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:32.256031",
    "content_length": 2017,
    "word_count": 296
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/documentation/genpipes_doc_archmap.html",
    "title": "Site Map",
    "content": "GenPipes has several documents to help users. This document describes how GenPipes documentation is laid out and organized to assist the user, be it a new GenPipes user or a seasoned one, as well as documentation contributor (technical writer, content developer, author) point of view.\nIf you are a GenPipes user, refer to the User’s map below that will help you know where to look for things.  For documentation contributors there is a separate map which provides a high level overview of how the content sources are organized and find your way to editing or updating them for the end user.\nAudience Focus\nThis map highlights various sections of GenPipes documentation indicating which ones are targeted at new users and those that are meant for seasoned GenPipes users and developers.\nFigure:  GenPipes Documentation Map for Users\nInformation Architecture\nThis map is meant for GenPipes documentation contributors.  It highlights the GenPipes source code directories and the documentation components that are generated using sources in the specific folders as shown by the arrows.\nThe tip of the yellow arrow points to the sub-folder that contains restructured text and markdown files used to generate that particular section of the document visible in the navigation bar.\nFigure:  GenPipes Documentation Map for Documentation Contributors",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:33.464956",
    "content_length": 1345,
    "word_count": 209
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/documentation/docs_changelog.html",
    "title": "Changelog",
    "content": "This documentation release corresponds to GenPipes version 6.0.0.\ncleanup by shaloo at 2025-08-15 05:03:59\nFix: #318 accessing DRAC GenPipes 6.0 update by shaloo at 2025-08-14 05:50:24\nFix: #317 documentation IA update by shaloo at 2025-08-13 11:47:22\nFix: #314 GenPipes docs v6.0 pipelines in sync, refactored, older ones in deprecated section by shaloo at 2025-08-11 03:44:30\nFix: #314 wflow schema deprecated and 6.0 pipelines refactored and layout bugs fixed by shaloo at 2025-08-07 13:45:24\nFix #315 and clean up steps section for all pipelines by shaloo at 2025-08-06 15:53:26\nMerge branch ‘shaloo/review-fix-6.0’ by shaloo at 2025-08-06 06:23:31\nFix: 301 review issues are fixed, for larger ones ticket opened by shaloo at 2025-08-06 06:23:04\nRefs: #301 available pipeline fixes by shaloo at 2025-08-06 04:25:38\nUpdate Copyright year by shaloo at 2025-08-05 12:01:41\nCommit Tracker\nThere were untracked files when this was compiled.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:34.721986",
    "content_length": 940,
    "word_count": 140
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/why_genpipes.html",
    "title": "Why GenPipes?",
    "content": "Why GenPipes?\nGenomic sequencing has become an indispensable tool for modern bioinformatics researchers in their quest to understand biological processes. Next generation sequencing (NGS) is not only complex but is also compute and data intensive. It requires efficient handling of high performance computing infrastructure, managing scalability, flexibility and capability to handle massive amounts of genome reference data while managing intermediate results and inter-dependency between several serial and parallel analysis processes.\nGenPipes is a python based bioinformatics tool comprising of several pre-built pipelines that addresses genomic analysis needs for bioinformatics researchers. It is available for public use as open-source software. GenPipes was developed at the Canadian Centre for Computational Genomics (C3G). It offers a wide array of genomic sequencing pipelines including RNA-Seq, ChIP-Seq, Whole Genome Sequencing (WGS), Exome sequencing, Bisulfite sequencing, Hi-C, capture Hi-C, metagenomics and SARS-CoV-2 genome sequencing pipeline.\nGenPipes Features\nMultiple Schedulers\n- GenPipes is optimized for HPC processing. Currently, it supports 4 schedulers - Slurm, PBS/Torque, Batch, Daemon.\nOptimal Job Execution Time\n- GenPipes minimizes overall job analysis time by job dependency model that leverages job parallelism. This enables jobs to become active and executed as soon as the dependencies are met.\nSmart Relaunching of Jobs\n- Through smart tracking of job progress, GenPipes can determine which jobs failed and at which steps. This helps to relaunch the jobs at the exact point in time, just before the last failure, automatically.\nParameter Encapsulation\n- GenPipes is a flexible framework that allows user adjustments. It implements a superposed configuration system to reduce the time required to set-up or modify parameters needed during the analysis.\nDiverse Inputs\n- GenPipes is flexible in terms of multiple choice of input files for the analysis. It allows users to skip specific steps in the pipeline if they consider them unnecessary.\nCustomizable Workflows\n- GenPipes limits wastage of expensive HPC resources and time as it allows customizable steps in different pipelines enabling users to plug and play with customized pipeline steps.\nKey Differentiators\nGenPipes is different from other analysis platforms, workbenches and workflow management systems (WMS) in terms of the following capabilities:\nFlexibility: Easy to modify and configure, multiple type of deployments available – local (containerized), cloud (GCP) and GenPipes deployment hosted on the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, servers, support for multiple job schedulers\nScalability: Optimized for large scale data analysis, simple to scale up or down in terms of processing and data access needs.\nBuilt-in Pipelines: Pre-built, tested on multiple computing infrastructures, robust industry standard benchmark driven and production quality genomic analysis pipelines for various bioinformatics analyses.\nComparing GenPipes with other NGS Solutions\nGenPipes’ strength lies in its robust WMS that comes with one of the most diverse selection of analysis pipelines that have been thoroughly tested. The pipelines in the framework cover a wide range of sequencing applications. The pipelines are end-to-end workflows running complete bioinformatics analyses. While many available pipelines conclude with a bam file or run limited post-bam analysis steps, the pipelines included in GenPipes are extensive, often having as many as 40 different steps that cover a wide range of post-bam processing.\nFor a tabular comparison of available solutions for NGS see here.\nGenPipes is compatible with HPC computing, as well as cloud computing, and includes a workflow manager that can be adapted to new systems. GenPipes also provides job status tracking through JSON files that can then be displayed on a web portal (an official portal for GenPipes will be released soon). GenPipes’ available pipelines facilitate bioinformatics processing, while the framework makes it flexible for modifications and new implementations.\nGenPipes developers offer continuous support through a Google forum page and a help desk email address (pipelines@computationalgenomics.ca). Since the release of version 2.0.0 in 2014, a community of users has run GenPipes to conduct approximately 3,000 analyses processing ∼100,000 samples.\nTo learn more about what GenPipes is and how it works, refer to the Get Started Guide.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:35.920812",
    "content_length": 4537,
    "word_count": 653
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/target_audience.html",
    "title": "Target Audience",
    "content": "Target Audience\nGenPipes is primarily geared towards Next Generation Sequencing analysis.\nThere are two kinds of target audience of GenPipes documentation:\nEnd-UsersDevelopersResearchers and analysts working in bioinformatics and gene-sequencing computational processing. GenPipes’ strength lies in its simple interface that makes it very easy for students and researchers in need of sophisticated, yet easy to use bioinformatics workflow management tool with built-in pipelines for various genomic analyses.\nSoftware experts and implementers who would like to improve and enhance the GenPipes framework.\nOur key focus in GenPipes documentation efforts is to ensure that onboarding GenPipes is intuitive and easy for beginners and also for seasoned users such that they can quickly figure out where to look if they need information on a specific feature or GenPipes capability.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:37.101866",
    "content_length": 878,
    "word_count": 125
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/sponsors.html",
    "title": "Sponsors",
    "content": "GenPipes is developed by the Canadian Centre for Computational Genomics (C3G). It offers genomic researchers a simple method to analyze different types of data, customizable to their needs and resources, as well as the flexibility to create their own workflows.\nC3G is a core platform affiliated with McGill University, with broad expertise in bioinformatics. It offers bioinformatics analysis and HPC services for the life sciences research community. Our services include bespoke pipeline development, service for a fee analyses, as well as an extensive suite of software solutions in the -omics fields. GenPipes is our open-source workflow and pipeline platform.\nC3G is core platform affiliated with McGill University and The Victor Phillip Dahdaleh Institute of Genomic Medicine. The main objectives of C3G are:\nDeveloping Open-Source software to facilitate bioinformatics analysis. See here for details.\nProviding analysis services to the genomics community.\nTraining graduate students and researchers through bioinformatics workshops and internships.\nProviding support for large scale genomics projects like CanCOGeN, Terry Fox Research Institute’s Marathon of Hope Cancer Centres Network, IHEC, and BRIDGET, among others.\nYou can learn more about C3G and their partners on the website.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:38.319537",
    "content_length": 1292,
    "word_count": 186
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/about/history.html",
    "title": "History",
    "content": "The GenPipes project evolved from the McGill University and Génome Québec Innovation Centre (MUGQIC) genomic analysis project called MUGQIC Pipelines. The centre focuses on genomics applied to populations and its impact on chronic diseases, aging, cancer and genomic responses to environment.\nGenPipes is the result of the centre’s initiative to develop frameworks that can be used for whole-genome sequencing, as the primary tool to perform genetic analyses in humans and other species.\nThe following figure shows how GenPipes has evolved over the years, key releases in its evolutionary timeline and the upcoming new features under development. It also shows how the framework itself has grown from supporting only a few pipelines at the start to 14+ genomic analysis pipelines.  The number of GenPipes runs utilizing C3G HPC resources have grown significantly as well. Since the release of version 2.0.0 in 2014, a community of users has run GenPipes to conduct approximately 3,000 analyses processing ∼100,000 samples.\nThe status of the repository is also indicated in terms of active branches, forks, contributors and total downloads of the open source.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:39.512515",
    "content_length": 1158,
    "word_count": 178
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_general.html",
    "title": "General",
    "content": "What is GenPipes?\nGenPipes is an open-source Python framework for large scale genomics analysis. Refer to What is GenPipes section in GenPipes documentation for details.\nWhere can I find GenPipes Citation reference and text?\nRefer to GenPipes Citation for citing and example text for GenPipes Citation.\nWhere can I access latest GenPipes source code?\nSee Download GenPipes section of Local Deployment documentation.\nDo I need to deploy GenPipes locally on a server or in the cloud to use it?\nYou do not need to necessarily deploy GenPipes to use it.  You can use pre-installed GenPipes on Digital Research Alliance Canada servers. For that you need to have CCDB account. See DRAC Deployment for details. If you wish to deploy GenPipes locally on your server (bare metal or virtual) or within a container, check out GenPipes Deployment and Access Guide for various deployment options and how to setup and access GenPipes.\nGenPipes requires familiarity with Unix. As a Windows user, what do I need to setup to use GenPipes?\nIf you are a Windows user, you need to get a working Unix command line interface to start using GenPipes.  Windows users have three main options to get a working Unix command line:\nUbuntu on Windows (only available on Windows 10)\nAlso, if you plan to run genomics pipelines, you need a genomics data viewer such as Integrative Genomics Viewer, a graphical, high-performance visualization tool for genomics data. For details, refer to instructions to setup your windows laptop.\nWhere do I find tips on troubleshooting GenPipes runtime and usage issues?\nSee Troubleshooting GenPipes Usage issues section of GenPipes Get Started Guide for details.\nFor new developers, are there any GenPipes development and troubleshooting tips?\nSee Troubleshooting GenPipes development issues section of GenPipes Developers Guide for details.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:41.877040",
    "content_length": 1852,
    "word_count": 294
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_new_users.html",
    "title": "New Users",
    "content": "To create a new CCDB account, what should I fill in the form field: ‘position’?\nChoose an appropriate option in the form, for example:\nexternal collaborator\nFor the CCRI field, use your sponsor’s Digital Research Alliance of Canada, formerly Compute Canada,  Role Identifier (CCRI) as input.\nThe CCRI has a structure similar to this: abc-123-01.\nWhat email ID should I use if I am an external collaborator?\nGMail or your work ID should be fine as long as you provide the name of your institution (or college in case of students).\nMy account is activated. How do I learn more about Compute Canada Servers and resources available?\nSee Digital Research Alliance Documentation.\nDigital Research Alliance of Canada (DRAC) was formerly known as Compute Canada.\nMy account is activated but I cannot login into Rorqual server or any other node such as Nibi, Fir and Narval? What is wrong?\nCheck out the current CCDB server status here. Many a times, not being able to log in might just be due to system unavailability.\nPlease note that if you try to log in 3 or more times consecutively with a wrong password, your account gets deactivated and your IP address might get blacklisted. You would need to write to Digital Research Alliance Support to get that reversed.\nWhat is the best place to report GenPipes bugs?\nGenPipes v6.x\nThe preferred way to report GenPipes bugs is on our GitHub issue page : https://github.com/c3g/GenPipes/issues.\nGenPipes v5.x\nThe preferred way to report GenPipes bugs is on our Bitbucket issue page : https://bitbucket.org/mugqic/genpipes/issues.\nYou can always reach us by email at mailto:pipelines@computationalgenomics.ca or use Google Group for GenPipes for any requests.\nFor more details, visit GenPipes Support and GenPipes Channels page in this documentation.\nI was trying to use GenPipes deployed in a Docker Container. What <tag> value should I use?\nYou can use the “latest” tag or one of the tags listed at GenPipes Docker Hub:. If you omit the <tag> Docker will use “latest” by default.\nHow do I run GenPipes locally in my infrastructure using a containerized setup? Is a scheduler setup necessary?\nGenPipes pipelines use scheduler’s calls (qsub, sbatch) for submitting genomic analysis compute jobs. If you plan to use GenPipes locally using your infrastructure, inside a container, you need to run the GenPipes pipeline python scripts using the “batch mode” option.  For local containerized versions of GenPipes, this is the preferred way of running the pipelines, if you don’t have access to a scheduler locally such as SLURM or PBS.\nThis is how you can run GenPipes pipelines such as DNA Sequencing Pipeline, refer to the command below:\ngenpipes dnaseq -c dnaseq.base.ini dnaseq.batch.ini -j batch -r your-readsets.tsv -d your-design.tsv -s 1-34 -t mugqic -g genpipes-file-script.sh\nbash genpipes-file-script.sh\nPlease note, there is a disadvantage to running GenPipes Pipelines without a scheduler.  In the batch mode, which is configured using the “-j batch” option, all the jobs would run as a batch, one after another, on a single node.  If your server is powerful enough, this might be your preferable option.  Otherwise, if you would like to take advantage of GenPipes’ job scheduling capabilities, you need to install a job scheduler locally in your infrastructure so that GenPipes can work effectively.  We recommend SLURM scheduler for GenPipes.\nGenPipes 3.4 RNA Sequencing Pipeline moved from using Trimmomatic to Skewer. Why? What does this change mean for GenPipes users?\nIn addition to why, there are additional queries that we received. So we will respond to all of these together:\nIn RNA-seq skewer,\na. What does the untrimmed read pairs available after processing refer to?\nb. If a large proportion of the reads are untrimmed does this mean the adapter sequence is wrong and how to troubleshoot this issue?\nWhy the switch?\nThe switch from trimmomatic to skewer was based on benchmarking. Skewer had improved F1 score across numerous truth sets over trimmomatic.\nUntrimmed Reads and Troubleshooting\nUntrimmed read pairs refers to read pairs which did not require quality 3’ trimming i.e. the quality was above 25 phred score or were above the size selection criteria of 50 bp after trimming and/or adapter removal.\nTypically when the fastqc are generated after sequencing the adapters are removed, but in some cases the adapter remains. You can use fastqc on the raw reads to visualize the proportion of these. Also if you are unsure check that the adapters you are using are inline with sequencer and libraries you are using.",
    "code_blocks": [
      "external collaborator",
      "genpipes dnaseq -c dnaseq.base.ini dnaseq.batch.ini -j batch -r your-readsets.tsv -d your-design.tsv -s 1-34 -t mugqic -g genpipes-file-script.sh\n\nbash genpipes-file-script.sh"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:43.128303",
    "content_length": 4584,
    "word_count": 743
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_gp_dev.html",
    "title": "Developers",
    "content": "Developers\nHow do I modify parameters/ options for specific tools or change computational resources that require a job to run?\nGenPipes is fairly flexible in terms of the tooling used. Users can specify requisite parameters to use specific tools such as schedulers, python language version etc.\nTo change computational resources or use specific tools, you would need to create a custom config or ini file. Add the corresponding section you need to modify and change the parameters.\nFor example, if you want to change parameters of the macs2_callpeak step in the ChIPSeq pipeline, first create a new text file called custom.ini or you can use any name you prefer. Copy what is already in the chipseq.base.ini for this step and paste it in your custom.ini file.\n[macs2_callpeak]\n# Mandatory for module_macs2=mugqic/MACS2/2.2.7.1\nmodule_python=mugqic/python/3.7.3\n# The arbitrary shift in bp (Default for ATAC-Seq 75 ; Default for ChIP-Seq Narrow mark 0 ; Default for ChIP-Seq Broad mark 0)\n# The arbitrary extension size in bp (Default for ATAC-Seq 150 ; Default for ChIP-Seq Narrow mark 200 ; Default for ChIP-Seq Broad mark 200)\n# Pvalue cutoff for peak detection (Default for ATAC-Seq 0.01 ; Default for ChIP-Seq Narrow mark `not set` ; Default for ChIP-Seq Broad mark `not set`)\nother_options =\ncluster_mem = 32G\ncluster_cpu = 2\ncluster_walltime = 12:00:0\nThe content above are the default settings, you can change them as shown below. For example, if you want to filter MACS2 peaks by FDR 0.01, then you should add -q 0.01 to the other_option section. Also, you can change the memory and cluster walltime to 64GB and 24 hours respectively. You can remove all the parameters you don’t want to change. The final file will look as shown below.\n[macs2_callpeak]\nother_options =-q 0.01\ncluster_mem = 64G\ncluster_walltime = 24:00:00\nNow save the updated custom.ini file and mention it on the command line after the cluster ini file when you run GenPipes. The order of the custom ini file is important to override default parameters. Therefore, please make sure to add your custom.ini file at the end of all the other ini files.\ngenpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini $GENPIPES_INIS/common_ini/Rorqual.ini custom.ini -r readset.chipseq.txt -d design.chipseq.txt -s 1-20 -g chipseqScript.sh\nbash chipseqScript.sh\nPytest command on CCDB server results in command not found error. Pytest install fails.\nA new developer trying to setup and run GenPipes tests found the following issues:\nI am trying to run some python test cases using \"pytest\" on |key_ccdb_server_name|cluster.\nI am running into \"pytest command not found \" error.\nI googled that error and found that it might be due to older version of setup tools.\nI tried to upgrade it and I'm seeing this error now.\nOnce your account is activated, you can login in CCDB servers such as Rorqual, Nibi, Fir and Narval.  However, these are National Systems on a shared grid and users don’t have permission to install or upgrade the software there.\nFor more information on what software is installed on Compute Canada infrastructure, refer to https://docs.alliancecan.ca/wiki/Available_software.",
    "code_blocks": [
      "[macs2_callpeak]\n# Mandatory for module_macs2=mugqic/MACS2/2.2.7.1\nmodule_python=mugqic/python/3.7.3\n# The arbitrary shift in bp (Default for ATAC-Seq 75 ; Default for ChIP-Seq Narrow mark 0 ; Default for ChIP-Seq Broad mark 0)\nshift=\n# The arbitrary extension size in bp (Default for ATAC-Seq 150 ; Default for ChIP-Seq Narrow mark 200 ; Default for ChIP-Seq Broad mark 200)\nextsize=\n# Pvalue cutoff for peak detection (Default for ATAC-Seq 0.01 ; Default for ChIP-Seq Narrow mark `not set` ; Default for ChIP-Seq Broad mark `not set`)\npvalue=\nother_options =\ncluster_mem = 32G\ncluster_cpu = 2\ncluster_walltime = 12:00:0",
      "[macs2_callpeak]\nother_options =-q 0.01\ncluster_mem = 64G\ncluster_walltime = 24:00:00",
      "genpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini $GENPIPES_INIS/common_ini/Rorqual.ini custom.ini -r readset.chipseq.txt -d design.chipseq.txt -s 1-20 -g chipseqScript.sh\n\nbash chipseqScript.sh",
      "I am trying to run some python test cases using \"pytest\" on |key_ccdb_server_name|cluster.\nI am running into \"pytest command not found \" error.",
      "I googled that error and found that it might be due to older version of setup tools.\nI tried to upgrade it and I'm seeing this error now."
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:44.350301",
    "content_length": 3160,
    "word_count": 500
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_c3g_res.html",
    "title": "C3G Resource Usage",
    "content": "C3G Resource Usage\nWhere can I find more details on CCDB servers, file system, usage guidelines?\nYou can visit Digital Research Alliance of Canada, formerly Compute Canada, Technical Documentation Wiki Site to learn more about the list of available servers, systems and services available, How-to guides and usage policy.\nIs there a list of software installed on Digital Research Alliance servers?\nSee list of available software and globally deployed modules on Digital Research Alliance of Canada servers.\nWhat are modules and why do we need them for GenPipes?\nGenPipes is developed in Python. Modules in Python are a way to load software or tools just in time, when a program needs them.\nThe modules that come along with GenPipes allow you to use bioinformatics tools like Samtools, Homer, MACS2, without installing them yourself.\nFor details on why we need modules for GenPipes and which ones are required, deployed and pre-installed the Digital Research Alliance Canada Servers, see the list of available modules.\nWhat are GenPipes genomes? Where can I access them from?\nGenPipes pipelines are used for genomic analysis and they require reference genomes. C3G, in partnership with Digital Research Alliance Canada, maintains several genomes that are available on several HPC centres including Rorqual, Nibi, Fir and Narval. In addition to the FASTA sequence, many genomes include aligner indices and annotation files.\nTo access these genomes, you need to add the following lines to your .bashrc file:\n## GenPipes/MUGQIC genomes and modules\nexport MUGQIC_INSTALL_HOME=/cvmfs/soft.mugqic/CentOS6\nTo explore the available genomes, you can type:\nls $MUGQIC_INSTALL_HOME/genomes/species/\nC3G, in partnership with Compute Canada, maintains several genomes that are available on several HPC centres. For a list of available genomes, visit Bioinformatics resources - genomes. In addition to the fasta sequence, many genomes include aligner indices and annotation files.\nSee genome files here.\nWhat is meant by test dataset? Where can I find available test datasets?\nGenPipes pipelines can be run using your sequencing instruments generated, measured, sampled read datasets in respective formats as required by individual pipelines or test datasets.  Refer to GenPipes Test Datasets for various available test datasets that can be used to run various GenPipes pipelines, in case you don’t have your own dataset to be processed.",
    "code_blocks": [
      "## GenPipes/MUGQIC genomes and modules\nexport MUGQIC_INSTALL_HOME=/cvmfs/soft.mugqic/CentOS6",
      "ls $MUGQIC_INSTALL_HOME/genomes/species/"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:45.529279",
    "content_length": 2428,
    "word_count": 367
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/faq/faq_hpc.html",
    "title": "GenPipes HPC Jobs",
    "content": "GenPipes HPC Jobs\nHow do I run GenPipes pipelines when the number of steps or jobs exceeds HPC site queue limit?\nMost HPC sites impose resource sharing constraints. One such constraint is limiting the number of jobs submitted to the scheduler wait queue. In such environments user can overcome the queue limits and continue to run GenPipes Pipelines even if the job has more steps than the limit.\nGenPipes provides utilities such as `chunk_genpipes.sh` that can take pipeline commands as input and chunks them so that each chunk consists of jobs which are within the specified queue limits for a given HPC environment.\nFor example, the chipseq pipeline commands can be chunked as follows:\nM_FOLDER=path_to_folder\ngenpipes chipseq <options> --genpipes_file chipseq_script.sh\nchunk_genpipes.sh chipseq_script.sh $M_FOLDER -n 15\nHere, `-n 15` input specifies that the maximum number of jobs in a chunk is 15.  This is an optional parameter.  By default, the chunk size is 20.\nYou can use the `submit_genpipes` GenPipes utility to submit jobs smartly to the scheduler and use scheduler `watch` command to monitor the status of these job chunks.\nsubmit_genpipes $M_FOLDER\nFor details, refer to Submitting GenPipes Pipeline runs and see genpipes/utils in the source tree.\n`chunk_genpipes.sh` script should be invoked only once for a pipeline run whereas the `submit_genpipes` script can be run multiple times during a pipeline run for checking the job status.\nIn case of an error or job timeout, do I need to re-run the entire GenPipes Pipeline script over again or is there a smarter way to submit only the failed jobs?\nWhen there is an error or timeout with the scheduler, the user can avoid canceling all GenPipes jobs and re-submit the entire pipeline script again.\nGenPipes utilities such as `chunk_genpipes.sh` and `submit_genpipes` can be used to smartly chunk pipeline command script and submit these chunks to the HPC scheduler queue instead of the full pipeline run script at once.The `submit_genpipes` script can be used to smartly submit jobs to the scheduler and then use the scheduler’s `watch` command to monitor status of job runs. Only the job chunks that timeout or encounter error can be re-submitted to the scheduling queue.\nFor details, refer to Submitting GenPipes Pipeline runs and see genpipes/utils in the source tree.",
    "code_blocks": [
      "M_FOLDER=path_to_folder\n\ngenpipes chipseq <options> --genpipes_file chipseq_script.sh\n\nchunk_genpipes.sh chipseq_script.sh $M_FOLDER -n 15",
      "submit_genpipes $M_FOLDER"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:46.786429",
    "content_length": 2340,
    "word_count": 373
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_options.html",
    "title": "Deployment Options",
    "content": "Deployment Options\nThere are multiple ways to access GenPipes and get started with genomic analysis using the pipelines therein. The figure below represents the three options available to bioinformatics researchers to access GenPipes.\nRemotely access GenPipes deployed at Alliance infrastructure\nGenPipes deployment in the cloud - Google Cloud Platform (GCP)\nLocal deployment\nBare metal or virtual server\nGenPipes in a container\nAccessing GenPipes\nThe infographic below represents various mechanisms that can be used to access GenPipes today.\nObtaining GenPipes sources\nRefer to the latest GenPipes sources for instructions on downloading and setting up GenPipes.\nGenPipes on DRAC Infrastructure\nResearchers that have access to the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, servers need not deploy GenPipes for genomic analysis. They can simply login and access Digital Research Alliance servers that have pre-installed stable release of GenPipes.  For details see how to access GenPipes deployment on Digital Research Alliance Canada infrastructure. External users who do not have access to DRAC resources can apply for the same.\nThrough a partnership with the Digital Research Alliance Canada consortium, the pipelines and third-party tools have also been configured on 6 different Alliance HPC centers. This allows any Canadian researcher to use GenPipes along with the needed computing resources by simply applying to the consortium. To ensure consistency of pipeline versions and used dependencies (such as genome references and annotation files) and to avoid discrepancy between compute sites, pipeline set-up has been centralized to 1 location, which is then distributed on a real-time shared file system: the CERN (European Organization for Nuclear Research) Virtual Machine File System CVM FS.\nGenPipes deployment on GCP\nIf you need to run large scale genomic analysis that requires resource scaling, GenPipes can be deployed and accessed from cloud.  At present, Google Compute Platform (GCP) is supported.  You may require assistance from System Administrator or your local cloud expert to install and deploy GenPipes in the cloud before you can access and run genomic analysis pipelines provided by GenPipes.  For details refer to GenPipes installation guide section titled “How to deploy GenPipes in the cloud?”.\nLocal deployment\nIf you wish to deploy GenPipes locally using your own compute and storage infrastructure, you can refer to the GenPipes GitHub repository. You could either deploy it on your local server / workstation or try GenPipes in a container option.\nGenPipes can be installed from scratch on any Linux cluster supporting Python version 3.11.1 or higher by following the instructions in the README.md file. GenPipes can also be deployed via containers approach. A Docker image of GenPipes is available which simplifies the set-up process and can be used on a range of platforms, including cloud platforms. This allows system-wide installations, as well as local user installations via the Docker image without needing special permissions.\nLocal deployment option can be used for small scale genomic analyses using genome datasets available locally. The GenPipes in a container option is a self-contained image that offers GenPipes software, common reference genomes and all that is needed to run the pre-built analysis pipelines.  Bioinformatics researchers who are not familiar with container technology may require assistance from System Administrators in deploying a local copy of GenPipes software.  For details refer to GenPipes installation guide section for container deployment GenPipes in a container.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:48.189212",
    "content_length": 3675,
    "word_count": 545
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/access_gp_pre_installed.html",
    "title": "DRAC Deployment",
    "content": "DRAC Deployment\nIn this guide you will learn how to access GenPipes deployed on the servers hosted by the Digital Research Alliance of Canada (DRAC), formerly Compute Canada (CCDB). Once you have access to these servers you can begin using GenPipes for genomic analysis.\n1. Register for DRAC account\nGo to the DRAC website and Register to create a new account.\nAgree with the policy and submit your acceptance.\nFill in the required fields of the form and submit it.\nStudents / Post-Doc / Other Sponsored Users\nTo be eligible for accessing the resources offered by the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, the Principle Investigator (PI) of your laboratory must also have an account.\nYou will need the Compute Canada Role Identifier (CCRI) of your sponsor/PI. The CCRI has the form abc-123-01. Canadian academics have free access to DRAC resources.\nIt may take a couple of days to process your account registration request.\nDRAC account\nThe latest DRAC server access requires multi-factor authentication (MFA). Also, once your account is enabled, you may be required to request access to specific servers that deploy GenPipes.\nVisit https://ccdb.alliancecan.ca/me/access_systems for details.\nVerify that your access is enabled (green checkmark) for each HPC and cloud system you plan to use.\nVerify that your access is disabled for each HPC and cloud system you don’t plan to use.\nFor questions and concerns about the DRAC servers, please email support@tech.alliancecan.ca to reach the alliance tech support.\n2. Connect to DRAC Servers\nUnix / Linux / Mac or Windows (bash)Windows (PuTTY)\nOpen a bash shell or terminal on your local system and type the following command:\nssh myaccount@rorqual.alliancecan.ca\nEnter your Digital Research Alliance of Canada, formerly Compute Canada, account password.\nOpen puTTY.\nSelect “Session” on the left hand side panel\nSelect “SSH” and fill the “Host Name” entry with the following:\nrorqual.alliancecan.ca\nClick “Open”\nA terminal will open and ask you to connect using your CC account credentials.\nDRAC server rorqual\nReplace the server name, rorqual, in the command above with the desired cluster name.\nOnce connected, you are all set to use GenPipes deployed on the Digital Research Alliance of Canada, formerly Compute Canada, data centre.\nAvailable Software at DRAC\nCanadian Centre for Computational Genomics (C3G), in partnership with Digital Research Alliance of Canada (DRAC), formerly Compute Canada, offers and maintains a large set of bioinformatics resources for the community.\nFor a complete list of software currently deployed on several HPC centres, including Rorqual, Nibi, Fir and Narval, refer to Bioinformatics Resources and available software.\nSeveral reference genomes are also available. Make sure you have the environment setup to access these genomes.\n3. GenPipes Environment Setup\nAbacus, DRAC UsersMUGQIC AnalystsAll of the software and scripts used by GenPipes are already installed on several DRAC servers including Rorqual, Nibi, Fir and Narval.\nTo access these tools on the DRAC servers, add the tool path to your bash_profile.\nThe .bash_profile is a hidden file in your home directory that sets up your environment every time you log in. You can also use your .bashrc file.\nGenomes and modules used by the pipelines are pre-installed on a CVMFS partition mounted on all the DRAC server clusters in the path /cvmfs/soft.mugqic/CentOS6.\n.bashrc vs. .bash_profile\nFor more information on the differences between the .bash_profile and the .bashrc profile, consult this page.\n## open bash_profile\nnano $HOME/.bash_profile\nNext, you need to load the software modules in your shell environment that are required to run GenPipes. To load the GenPipes modules, paste the following lines of code and save the file, then exit (Ctrl-X):\n## GenPipes/MUGQIC genomes and modules\nexport MUGQIC_INSTALL_HOME=/cvmfs/soft.mugqic/CentOS6\nmodule use $MUGQIC_INSTALL_HOME/modulefiles\nmodule load mugqic/genpipes/<latest_version>\nexport JOB_MAIL=<my.name@my.email.ca>\nexport RAP_ID=<my-rap-id>\nYou can refer to the full list of modules available on the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, servers at the module page.\nJOB_MAIL and RAP_ID\nYou will need to replace the text in “<>” with your DRAC account specific information.\nJOB_MAIL is the environment variable that needs to be set to the email ID on which GenPipes job status notifications are sent corresponding to each job initiated by your account. It is advised that you create a separate email for jobs since you can receive hundreds of emails per pipeline. You can also de-activate the email sending option by removing the “-M $JOB_MAIL” option from the .ini files.\nRAP_ID is the Resource Allocation Project ID from DRAC. It is usually in the format: rrg-lab-xy OR def-lab.\nFor MUGQIC analysts, add the following lines to your $HOME/.bash_profile:\n## MUGQIC genomes and modules for MUGQIC analysts\nHOST=`hostname`;\nDNSDOMAIN=`dnsdomainname`;\nexport MUGQIC_INSTALL_HOME=/cvmfs/soft.mugqic/CentOS6\nif [[ $HOST == abacus* || $DNSDOMAIN == ferrier.genome.mcgill.ca ]]; then\nexport MUGQIC_INSTALL_HOME_DEV=/lb/project/mugqic/analyste_dev\nelif [[ $HOST == ip* || $DNSDOMAIN == m  ]]; then\nexport MUGQIC_INSTALL_HOME_DEV=/project/6007512/C3G/analyste_dev\nelif [[ $HOST == fir* || $DNSDOMAIN == fir.alliancecan.ca ]]; then\nexport MUGQIC_INSTALL_HOME_DEV=/project/6007512/C3G/analyste_dev\nelif [[ $HOST == rorqual* || $DNSDOMAIN == rorqual.alliancecan.ca ]]; then\nexport MUGQIC_INSTALL_HOME_DEV=/project/6007512/C3G/analyste_dev\nmodule use $MUGQIC_INSTALL_HOME/modulefiles $MUGQIC_INSTALL_HOME_DEV/modulefiles\nmodule load mugqic/genpipes/<latest_version>\nexport RAP_ID=<my-rap-id>\nAlso, set JOB_MAIL in your $HOME/.bash_profile to receive PBS/SLURM job logs:\nexport JOB_MAIL=<my.name@my.email.ca>\nPython Version\nGenPipes 5.x release onward has been verified for Python version 3.11.1 or higher. It no longer supports Python 2.7 version.\nVerify Version\nTo find out the latest GenPipes version available, once you have connected to your CC account, use the following command:\nmodule avail 2>&1 | grep mugqic/genpipes\nWhat is mugqic?\nPrevious version of GenPipes were named mugqic_pipelines and are still available for use.\nVerify Environment\nYou must ensure that your .bash_profile changes have taken effect before running genpipes command.\nWhen you make changes to your .bash_profile file, you will need to log out and then login again for these changes to take effect. Alternatively, you can run the following command in bash shell:\nsource $HOME/.bash_profile\nBy adding the lines related to module load and environment variable setting via export, you have set up the pipeline environment and are ready to use GenPipes!\nThis also gives you access to hundreds of bioinformatics tools pre-installed by our team. To explore the available tools, you can type the following command:\nmodule avail mugqic/\nFor a full list of all available software on DRAC servers, visit module page.\nCheck Tools Availability\nTo load a tool available on DRAC servers, for example - samtools, use the following command:\n# module add mugqic/<tool><version>\nmodule add mugqic/samtools/1.4.1\n# Now samtools 1.4.1 is available for use in your account environment. To check, run the following command:\nSeveral of the GenPipes pipelines may require referencing genomes. To access these pre-installed genomes available in:\n$MUGQIC_INSTALL_HOME/genomes/species/\nuse the following command to check all available genome species:\nls $MUGQIC_INSTALL_HOME/genomes/species\nAll genome-related files, including indices for different aligners and annotation files can be found in:\n$MUGQIC_INSTALL_HOME/genomes/species/<species_scientific_name>.<assembly>/\n## so for Homo Sapiens hg19 assembly, that would be:\nls $MUGQIC_INSTALL_HOME/genomes/species/Homo_sapiens.hg19/\nFor a complete list of all available reference genomes, visit genome page.\n4. Run genpipes Command\nNow you are all set to run GenPipes pipelines for genomic analysis. Refer to instructions in Using GenPipes for genomic analysis for example runs.  For specific pipelines supported by GenPipes, their command options refer to GenPipes User Guide.",
    "code_blocks": [
      "ssh myaccount@rorqual.alliancecan.ca",
      "rorqual.alliancecan.ca",
      "/cvmfs/soft.mugqic/CentOS6",
      "## open bash_profile\nnano $HOME/.bash_profile",
      "umask 0006\n\n## GenPipes/MUGQIC genomes and modules\nexport MUGQIC_INSTALL_HOME=/cvmfs/soft.mugqic/CentOS6\nmodule use $MUGQIC_INSTALL_HOME/modulefiles\nmodule load mugqic/genpipes/<latest_version>\nexport JOB_MAIL=<my.name@my.email.ca>\nexport RAP_ID=<my-rap-id>",
      "umask 0006\n\n## MUGQIC genomes and modules for MUGQIC analysts\n\nHOST=`hostname`;\n\nDNSDOMAIN=`dnsdomainname`;\n\nexport MUGQIC_INSTALL_HOME=/cvmfs/soft.mugqic/CentOS6\n\nif [[ $HOST == abacus* || $DNSDOMAIN == ferrier.genome.mcgill.ca ]]; then\n\n  export MUGQIC_INSTALL_HOME_DEV=/lb/project/mugqic/analyste_dev\n\nelif [[ $HOST == ip* || $DNSDOMAIN == m  ]]; then\n\n  export MUGQIC_INSTALL_HOME_DEV=/project/6007512/C3G/analyste_dev\n\nelif [[ $HOST == fir* || $DNSDOMAIN == fir.alliancecan.ca ]]; then\n\n  export MUGQIC_INSTALL_HOME_DEV=/project/6007512/C3G/analyste_dev\n\n\nelif [[ $HOST == rorqual* || $DNSDOMAIN == rorqual.alliancecan.ca ]]; then\n\n  export MUGQIC_INSTALL_HOME_DEV=/project/6007512/C3G/analyste_dev\n\nfi\n\nmodule use $MUGQIC_INSTALL_HOME/modulefiles $MUGQIC_INSTALL_HOME_DEV/modulefiles\nmodule load mugqic/genpipes/<latest_version>\n\n  export RAP_ID=<my-rap-id>\n\nAlso, set JOB_MAIL in your $HOME/.bash_profile to receive PBS/SLURM job logs:",
      "export JOB_MAIL=<my.name@my.email.ca>",
      "module avail 2>&1 | grep mugqic/genpipes",
      "source $HOME/.bash_profile",
      "# module add mugqic/<tool><version>\nmodule add mugqic/samtools/1.4.1\n\n# Now samtools 1.4.1 is available for use in your account environment. To check, run the following command:\nsamtools",
      "$MUGQIC_INSTALL_HOME/genomes/species/",
      "ls $MUGQIC_INSTALL_HOME/genomes/species",
      "$MUGQIC_INSTALL_HOME/genomes/species/<species_scientific_name>.<assembly>/\n## so for Homo Sapiens hg19 assembly, that would be:\nls $MUGQIC_INSTALL_HOME/genomes/species/Homo_sapiens.hg19/"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:49.454848",
    "content_length": 8234,
    "word_count": 1179
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_gp_local.html",
    "title": "Local Deployment",
    "content": "Local Deployment\nThis document explains how to deploy GenPipes locally on a bare metal or virtual server. See GenPipes in a container section to install GenPipes locally in a container.\nCheck out GenPipes Deployment Options Page for other available options to use GenPipes.\nStep 1: Download latest GenPipes sources and install with pip\nFirst of all, visit GenPipes the Download Page and get a copy of the latest stable release software.\nNote that the repo was earlier hosted on Bitbucket until v5.1.0 of GenPipes.\ngit clone https://github.com/c3g/GenPipes.git\ncd GenPipes\npip install .\ngit clone git@bitbucket.org:mugqic/genpipes.git\ncd <bitbucket_repo>\npip install .\nStep 2: Setup environment variables\nAdd the following lines in your your $HOME/.bash_profile: to set MUGQIC_PIPELINES_HOME to your local copy path. For example,\nexport MUGQIC_PIPELINES_HOME=/path/to/your/local/genpipes\nexport GENPIPES_INIS=$MUGQIC_PIPELINES_HOME/genpipes/pipelines\nStep 3: Accessing software modules and genomes needed for GenPipe\nGenPipes was formerly known as MUGQIC Pipelines. Genomic analysis executed using these pipelines requires genomes and software modules. You need to load the software modules in your shell environment. To do so, set the environment variable MUGQIC_INSTALL_HOME to the directory where you want to install those resources in your $HOME/.bash_profile as follows:\n## MUGQIC genomes and modules\nexport MUGQIC_INSTALL_HOME=/path/to/your/local/genpipes/resources\nmodule use $MUGQIC_INSTALL_HOME/modulefiles\nInstalling available modules\nSoftware tools and associated modules must be installed in $MUGQIC_INSTALL_HOME/software/ and $MUGQIC_INSTALL_HOME/modulefiles/.  Default software/module installation scripts are already available in $MUGQIC_PIPELINES_HOME/resources/modules/.\nInstall new modules\nTo install a new module or new software tool and associated modules semi-automatically, use the following instructions:\ncp $MUGQIC_PIPELINES_HOME/resources/modules/MODULE_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/modules/<my_software>.sh\nFollow the instructions in the file $MUGQIC_PIPELINES_HOME/resources/modules/<my_software>.sh and modify it accordingly.  Next you need to run the following command with No arguments. By default, it will download and extract the remote software archive, build the software and create the associated module, all in $MUGQIC_INSTALL_HOME_DEV if it is set.\n$MUGQIC_PIPELINES_HOME/resources/modules/<my_software>.sh\nIf everything executes OK with no error, you are ready to install the my_software module in production. Use the command:\n$MUGQIC_PIPELINES_HOME/resources/modules/<my_software>.sh MUGQIC_INSTALL_HOME\nPlease note there is no $ before MUGQIC_INSTALL_HOME specified as argument above!.\nNext, you need to check if the module is successfully installed and available for use by executing the following command:\nmodule avail 2>&1 | grep mugqic/<my_software>/<version>\nThis completes the software module setup for GenPipes execution. Next you need to make sure all required reference genomes are available in your local deployment. Refer to the next section if you wish to install additional genomes.\nInstalling genomes\nReference genomes and annotations must be installed in the following directory:\n$MUGQIC_INSTALL_HOME/genomes/\nDefault genome installation scripts are already available locally in the following directory:\n$MUGQIC_PIPELINES_HOME/resources/genomes/\nTo install all of the available genomes that are bundled with GenPipes package, use the following script:\n$MUGQIC_PIPELINES_HOME/resources/genomes/install_all_genomes.sh\nAll species related files are in the following directory:\n$MUGQIC_INSTALL_HOME/genomes/species/<species_scientific_name>.<assembly>/\nFor example, Homo Sapiens assembly GRCh37 genome directory hierarchy is as follows:\n$MUGQIC_INSTALL_HOME/genomes/species/Homo_sapiens.GRCh37/\n├── annotations/\n│   ├── gtf_tophat_index/\n│   ├── Homo_sapiens.GRCh37.dbSNP142.vcf.gz\n│   ├── Homo_sapiens.GRCh37.dbSNP142.vcf.gz.tbi\n│   ├── Homo_sapiens.GRCh37.Ensembl75.geneid2Symbol.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.genes.length.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.genes.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.GO.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.gtf\n│   ├── Homo_sapiens.GRCh37.Ensembl75.ncrna.fa\n│   ├── Homo_sapiens.GRCh37.Ensembl75.rrna.fa\n│   ├── Homo_sapiens.GRCh37.Ensembl75.transcript_id.gtf\n│   ├── Homo_sapiens.GRCh37.Ensembl75.vcf.gz\n│   ├── ncrna_bwa_index/\n│   └── rrna_bwa_index/\n├── downloads/\n│   ├── ftp.1000genomes.ebi.ac.uk/\n│   ├── ftp.ensembl.org/\n│   └── ftp.ncbi.nih.gov/\n├── genome/\n│   ├── bowtie2_index/\n│   ├── bwa_index/\n│   ├── Homo_sapiens.GRCh37.dict\n│   ├── Homo_sapiens.GRCh37.fa\n│   ├── Homo_sapiens.GRCh37.fa.fai\n│   └── star_index/\n├── Homo_sapiens.GRCh37.ini\nThe assembly name is the one used by the download source. For e.g. “GRCh37” is used for Ensembl.\nEach species directory contains a “.ini” file such as:\n<scientific_name>.<assembly>.ini\nAmong other things, this “.ini” file lists the assembly synonyms. In case of “hg19”, the contents of Homo_sapiens.GRCh37.ini are as shown below:\nscientific_name=Homo_sapiens\ncommon_name=Human\nassembly=GRCh37\nassembly_synonyms=hg19\nsource=Ensembl\ndbsnp_version=142\nInstall a new Genome\nNew genomes and annotations can be installed semi-automatically from Ensembl (vertebrate species), Ensemble Genomes (other species) or UCSC (genome and indexes only; no annotations).\nExample - how to set up genomes for Chimpanzee:\nRetrieve the species scientific name on Ensemble Genomes or UCSC :\nPan troglodytes\nRetrieve the assembly name:\nEnsembl: “CHIMP2.1.4”\nUCSC: “panTro4”\nRetrieve the source version:\nEnsembl: “78”\nUCSC: unfortunately, UCSC does not have version numbers. Use panTro4.2bit date formatted as “YYYY-MM-DD”: “2012-01-09”\nNext, copy the template file to a new file name using the scientific name.\ncp $MUGQIC_PIPELINES_HOME/resources/genomes/GENOME_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh\nFor example, in case of Ensembl, use the following command:\ncp $MUGQIC_PIPELINES_HOME/resources/genomes/GENOME_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/genomes/Pan_troglodytes.CHIMP2.1.4.sh\nIn case of genomes from UCSC, use the following command to copy the genome install instructions:\ncp $MUGQIC_PIPELINES_HOME/resources/genomes/GENOME_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/genomes/Pan_troglodytes.panTro4.sh\nNext, you need to modify the following file:\n$MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh\nPlease note that ASSEMBLY_SYNONYMS can be left empty but if you know that 2 assemblies\nare identical apart from chr sequence prefixes, document it.\nExample below shows the modifications for Ensembl:\nSPECIES=Pan_troglodytes   # With \"_\"; no space!\nCOMMON_NAME=Chimpanzee\nASSEMBLY=CHIMP2.1.4\nASSEMBLY_SYNONYMS=panTro4\nSOURCE=Ensembl\nExample below shows the modifications for UCSC:\nSPECIES=Pan_troglodytes   # With \"_\"; no space!\nCOMMON_NAME=Chimpanzee\nASSEMBLY=panTro4\nASSEMBLY_SYNONYMS=CHIMP2.1.4\nSOURCE=UCSC\nVERSION=2012-01-09\nNow you can run the following command to install the genome in $MUGQIC_INSTALL_HOME_DEV (by default). This will download and install genomes, indexes and, for Ensembl only, annotations (GTF, VCF, etc.).\nbash $MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh\nTo install it in $MUGQIC_INSTALL_HOME, run the following command:\nbash $MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh MUGQIC_INSTALL_HOME\nAdmin-only If the new genome has been installed in $MUGQIC_INSTALL_HOME_DEV, to deploy in $MUGQIC_INSTALL_HOME you can use the following command:\nrsync -vca --no-o --no-g --no-p --size-only -I -O --ignore-times $MUGQIC_INSTALL_HOME_DEV/genomes/species/<scientific_name>.<assembly> $MUGQIC_INSTALL_HOME/genomes/species/\nLastly, add the newly created “.ini” file to the genome configuration files for further use in subsequent genomic analysis pipeline runs by the following command:\ncp $MUGQIC_INSTALL_HOME/genomes/species/<scientific_name>.<assembly>/<scientific_name>.<assembly>.ini $MUGQIC_PIPELINES_HOME/resources/genomes/config/\nStep 4: Validating GenPipes local deployment\nYou are now all set to use GenPipes pipelines. For each pipeline, you can get help about its usage through the help command:\ngenpipes <pipeline_name> --help\nRunning pipelines requires other inputs such as Configuration File, Readset File and Design File. For details on how to run individual pipelines you can see Running GenPipes or GenPipes User Guide.\nIn case of any issues, you can try GenPipes Support or check out other communication channels to view latest discussions around using GenPipes by the community.\nYou may also want to check the latest GenPipes deployment and setup instructions listed in GenPipes README.md file.",
    "code_blocks": [
      "git clone https://github.com/c3g/GenPipes.git\ncd GenPipes\npip install .",
      "git clone git@bitbucket.org:mugqic/genpipes.git\ncd <bitbucket_repo>\npip install .",
      "export MUGQIC_PIPELINES_HOME=/path/to/your/local/genpipes\nexport GENPIPES_INIS=$MUGQIC_PIPELINES_HOME/genpipes/pipelines",
      "## MUGQIC genomes and modules\n\nexport MUGQIC_INSTALL_HOME=/path/to/your/local/genpipes/resources\nmodule use $MUGQIC_INSTALL_HOME/modulefiles",
      "cp $MUGQIC_PIPELINES_HOME/resources/modules/MODULE_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/modules/<my_software>.sh",
      "$MUGQIC_PIPELINES_HOME/resources/modules/<my_software>.sh",
      "$MUGQIC_PIPELINES_HOME/resources/modules/<my_software>.sh MUGQIC_INSTALL_HOME",
      "module avail 2>&1 | grep mugqic/<my_software>/<version>",
      "$MUGQIC_INSTALL_HOME/genomes/",
      "$MUGQIC_PIPELINES_HOME/resources/genomes/",
      "$MUGQIC_INSTALL_HOME/genomes/species/<species_scientific_name>.<assembly>/",
      "$MUGQIC_INSTALL_HOME/genomes/species/Homo_sapiens.GRCh37/\n├── annotations/\n│   ├── gtf_tophat_index/\n│   ├── Homo_sapiens.GRCh37.dbSNP142.vcf.gz\n│   ├── Homo_sapiens.GRCh37.dbSNP142.vcf.gz.tbi\n│   ├── Homo_sapiens.GRCh37.Ensembl75.geneid2Symbol.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.genes.length.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.genes.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.GO.tsv\n│   ├── Homo_sapiens.GRCh37.Ensembl75.gtf\n│   ├── Homo_sapiens.GRCh37.Ensembl75.ncrna.fa\n│   ├── Homo_sapiens.GRCh37.Ensembl75.rrna.fa\n│   ├── Homo_sapiens.GRCh37.Ensembl75.transcript_id.gtf\n│   ├── Homo_sapiens.GRCh37.Ensembl75.vcf.gz\n│   ├── ncrna_bwa_index/\n│   └── rrna_bwa_index/\n├── downloads/\n│   ├── ftp.1000genomes.ebi.ac.uk/\n│   ├── ftp.ensembl.org/\n│   └── ftp.ncbi.nih.gov/\n├── genome/\n│   ├── bowtie2_index/\n│   ├── bwa_index/\n│   ├── Homo_sapiens.GRCh37.dict\n│   ├── Homo_sapiens.GRCh37.fa\n│   ├── Homo_sapiens.GRCh37.fa.fai\n│   └── star_index/\n├── Homo_sapiens.GRCh37.ini\n└── log/",
      "<scientific_name>.<assembly>.ini",
      "[DEFAULT]\nscientific_name=Homo_sapiens\ncommon_name=Human\nassembly=GRCh37\nassembly_synonyms=hg19\nsource=Ensembl\nversion=75\ndbsnp_version=142",
      "cp $MUGQIC_PIPELINES_HOME/resources/genomes/GENOME_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh",
      "cp $MUGQIC_PIPELINES_HOME/resources/genomes/GENOME_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/genomes/Pan_troglodytes.CHIMP2.1.4.sh",
      "cp $MUGQIC_PIPELINES_HOME/resources/genomes/GENOME_INSTALL_TEMPLATE.sh $MUGQIC_PIPELINES_HOME/resources/genomes/Pan_troglodytes.panTro4.sh",
      "$MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh",
      "SPECIES=Pan_troglodytes   # With \"_\"; no space!\nCOMMON_NAME=Chimpanzee\nASSEMBLY=CHIMP2.1.4\nASSEMBLY_SYNONYMS=panTro4\nSOURCE=Ensembl\nVERSION=78",
      "SPECIES=Pan_troglodytes   # With \"_\"; no space!\nCOMMON_NAME=Chimpanzee\nASSEMBLY=panTro4\nASSEMBLY_SYNONYMS=CHIMP2.1.4\nSOURCE=UCSC\nVERSION=2012-01-09",
      "bash $MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh",
      "bash $MUGQIC_PIPELINES_HOME/resources/genomes/<scientific_name>.<assembly>.sh MUGQIC_INSTALL_HOME",
      "rsync -vca --no-o --no-g --no-p --size-only -I -O --ignore-times $MUGQIC_INSTALL_HOME_DEV/genomes/species/<scientific_name>.<assembly> $MUGQIC_INSTALL_HOME/genomes/species/",
      "cp $MUGQIC_INSTALL_HOME/genomes/species/<scientific_name>.<assembly>/<scientific_name>.<assembly>.ini $MUGQIC_PIPELINES_HOME/resources/genomes/config/",
      "genpipes <pipeline_name> --help"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:50.639288",
    "content_length": 8868,
    "word_count": 999
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_gp_container.html",
    "title": "Container Deployment",
    "content": "Container Deployment\nThis document covers details on how to deploy GenPipes locally on your infrastructure using the container mechanism. For more details on other available options to deploy and access GenPipes you may refer to GenPipes Deployment Options Page.\nYou can locally deploy GenPipes by creating a container that hosts all necessary software, configuration details to get you started with running GenPipes within the container. You only need $User privileges to deploy and use GenPipes locally in a container, no root privileges are needed for this option.\nGenPipes genomic analysis tools are designed to run on supercomputing infrastructure or HPC data centres such as Compute Canada servers. However, you can generate the pipeline scripts and run smaller experiments on a server with container technology. This mechanism is useful if you are a contributor to GenPipes code or wish to add a feature of your own in the code. Containers make it easy for you to debug and develop GenPipes on your local machine, if you do not have access to GenPipes deployed on Compute Canada servers.\nAnother use case where it makes sense to deploy GenPipes in a container is if you have a smaller dataset that you would like to run on your personal computer. However, since your computer does not have a job scheduler such as the SLURM or PBS that can be installed in a cluster, you will not be able to run GenPipes pipeline steps in parallel. You can run them in a sequential mode only through GenPipes in a container kind of deployment.\nGenPipes in a container allows pipeline steps run in a sequential order only as there are no schedulers such as SLURM or PBS in a container setup on a personal computer.\nStep 1: Install a compatible container technology on your local server\nGenPipes can be deployed either using Docker or Singularity based containers. Refer to the respective container technology tutorial and user manuals to deploy and check if your container setup is working locally.\nStep 2: Setup a GenPipes development environment\nThis step is required only if you wish to make some changes or modifications to GenPipes code before running the pipelines in a container deployment on your personal computer.\nIf you simply wish to run the latest GenPipes release in a container with your small dataset, you can skip this step and go to Step 3.\nOnce your container environment and requisite software is all setup and working, proceed to clone GenPipes somewhere locally under $HOME directory using the following command:\ngit clone https://bitbucket.org/mugqic/genpipes $HOME/some/dir/genpipes\nAdd the following line to your .bashrc file:\nexport GENPIPES_DEV_DIR=$HOME/some/dir/genpipes\nInstead of .bashrc, use ~/.zshrc file, if your personal computer is running Mac OSX.\nNext, use instructions below to start your GenPipes container.\nStep 3: Setup GenPipes in the container\nFor Docker, use the following command:\ndocker run --privileged -v /tmp:/tmp --network host -it -w $PWD -v $HOME:$HOME --user $UID:$GROUPS -v /etc/group:/etc/group  -v /etc/passwd:/etc/passwd  [ -v < CACHE_ON_HOST >:/cvmfs-cache/ ] c3genomics/genpipes:<TAG>\nFor Singularity, use the following command:\nsingularity run [ -B < /HOST/CACHE/ >:/cvmfs-cache/  ] docker://c3genomics/genpipes:<TAG>\nPlease note, <TAG> refers to one of the tagged GenPipes sources as listed at GitHub or Docker Hub. Choose ‘Tags’ to select the version that you wish to use for GenPipes.\n<CACHE_ON_HOST> can be any place in your computer that can be used to store the CVMFS cache. For example,\nCACHE_ON_HOST=\"-v ~/cvmfs-cache/:/cvmfs-cache/\"\n<CACHE_ON_HOST> will hold a cache for GenPipes in a container CVMFS system. It will hold the genomes and software that is used by GenPipes. This folder will grow with GenPipes usage. You can delete it in between usage, but keep in mind that once deleted it will need to be rebuilt by downloading data from the internet.\nFrom GenPipes Release 2.X.X onward, FUSE needs to be installed on the host where GenPipes is deployed. This is a requirement with the container technology itself, the only dependency of the system.\nIf you are using a Mac computer, first you will need to install macFUSE and SSHFS from here. In the case of Linux, Fuse typically comes preinstalled with the OS.\nAfter installing FUSE, run the following command:\ndocker run --rm  --device /dev/fuse --cap-add SYS_ADMIN  -v /tmp:/tmp -it -w $PWD -v $HOME:$HOME  - [ -v < CACHE_ON_HOST >:/cvmfs-cache/ ]  c3genomics/genpipes:<TAG>\nStep 4: Load GenPipes dependency modules in the container\nAs shown in previous steps, you can initiate the container process on your machine locally. Next, you need to load GenPipes module using the following command:\nmodule load dev_genpipes\nWith this command, GenPipes uses whatever commit branch that has been checked out in $HOME/some/dir/genpipes directory.\nVoila! Now you can use GenPipes inside the container just like you would use it locally on a server or on Compute Canada servers.\nFor each pipeline, you can get help about its usage through the help command:\ngenpipes $MUGQIC_PIPELINES_HOME/pipelines/<pipeline_name>/<pipeline_name> --help\nStep 5: Running GenPipes Pipelines in a container\nRunning pipelines requires other inputs such as Configuration File, Readset File and Design File. For details on how to run individual pipelines you can see Running GenPipes or GenPipes User Guide.\nYou need to make a note of the fact that GenPipes Pipelines use scheduler’s calls (qsub, sbatch) for submitting genomic analysis compute jobs. If you plan to use GenPipes locally using your infrastructure, inside a container, you need to run the GenPipes pipeline python scripts using the “batch mode” option.  For local containerized versions of GenPipes, this is the preferred way of running the pipelines, if you don’t have access to a scheduler locally such as the SLURM or PBS.\nThis is how you can run GenPipes pipelines such as DNA Sequencing Pipeline, refer to the command below:\ngenpipes dnaseq -c dnaseq.base.ini dnaseq.batch.ini -j batch -r your-readsets.tsv -d your-design.tsv -s 1-34 -t mugqic -g run-in-container-dnaseq-script.sh\nbash run-in-container-dnaseq-script.sh\nPlease note, there is a disadvantage to running GenPipes Pipelines without a scheduler.  In the batch mode, which is configured using the “-j batch” option, all the jobs would run as a batch, one after another, on a single node.  If your server is powerful enough, this might be your preferable option.  Otherwise, if you would like to take advantage of GenPipes’ job scheduling capabilities, you need to install a job scheduler locally in your infrastructure so that GenPipes can work effectively.  We recommend the SLURM scheduler for GenPipes.\nIn case of any issues, you can try GenPipes Support or check out other communication channels to view latest discussions around using GenPipes by the community.\nYou may also want to check the latest GenPipes deployment and setup instructions listed in the GenPipes README.md file.",
    "code_blocks": [
      "git clone https://bitbucket.org/mugqic/genpipes $HOME/some/dir/genpipes",
      "export GENPIPES_DEV_DIR=$HOME/some/dir/genpipes",
      "docker run --privileged -v /tmp:/tmp --network host -it -w $PWD -v $HOME:$HOME --user $UID:$GROUPS -v /etc/group:/etc/group  -v /etc/passwd:/etc/passwd  [ -v < CACHE_ON_HOST >:/cvmfs-cache/ ] c3genomics/genpipes:<TAG>",
      "singularity run [ -B < /HOST/CACHE/ >:/cvmfs-cache/  ] docker://c3genomics/genpipes:<TAG>",
      "CACHE_ON_HOST=\"-v ~/cvmfs-cache/:/cvmfs-cache/\"",
      "docker run --rm  --device /dev/fuse --cap-add SYS_ADMIN  -v /tmp:/tmp -it -w $PWD -v $HOME:$HOME  - [ -v < CACHE_ON_HOST >:/cvmfs-cache/ ]  c3genomics/genpipes:<TAG>",
      "module load dev_genpipes",
      "genpipes $MUGQIC_PIPELINES_HOME/pipelines/<pipeline_name>/<pipeline_name> --help",
      "genpipes dnaseq -c dnaseq.base.ini dnaseq.batch.ini -j batch -r your-readsets.tsv -d your-design.tsv -s 1-34 -t mugqic -g run-in-container-dnaseq-script.sh\n\nbash run-in-container-dnaseq-script.sh"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:51.848192",
    "content_length": 7003,
    "word_count": 1095
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/deploy/dep_gp_cloud_gcp.html",
    "title": "Cloud Deployment",
    "content": "Cloud Deployment\nThis document describes how to deploy GenPipes in the cloud. Following are the supported cloud providers where you can deploy and run GenPipes:\nFor more details on non-cloud deployment options for GenPipes you may refer to GenPipes Deployment Options Page.\nGenPipes on Google Cloud Platform (GCP)\nThis section describes how to deploy and run GenPipes using GCP infrastructure.\nPre-requisites for deploying GenPipes on GCP\nIf you are a seasoned GCP user and familiar with Google Cloud shell, you can skip this step.  For first time GCP users – you may want to try GCP free tier for GenPipes deployment. Follow these steps to create and access GCP account:\nCreate an account on GCP. For more instructions, check out GCP console page.\nGet acquainted with Google Cloud Shell. For more instructions, check out this page.\nCreate a new project. For detailed instructions see here.\nEnable Deployment Manager API for your project. For further details, visit this page.\nYou also need to make sure that billing is enabled (even for a GCP free try option).\nInstall GenPipes\nTo install GenPipes on GCP, use Google Cloud Shell Session and download the following install scripts:\ngit clone https://bitbucket.org/mugqic/cloud_deplyoment.git\nGenPipes requires Slurm for scheduling genomic analysis jobs on GCP compute servers. To setup Slurm on your GCP compute infrastructure, run the following commands in your Google Cloud Shell:\ncd cloud_deplyoment/gcp/\ngcloud deployment-manager deployments create slurm --config slurm-cluster.yaml\nOnce this command is done running, a configuration script is started to install SLURM on the cluster. It will take some time to complete the setup. You will be able to monitor the status of installation once you run the next command. The Cluster configuration is specified in slurm-cluster.yaml file. You can view it to see the controller and worker node setup. By default, only a single node is used for this GCP GenPipes deployment. See node_count value in slurm-cluster.yaml file.\nFrom here on, your GenPipes cloud is being deployed and your account is getting billed by Google.\nRemember to shut down the cluster when the analysis is done.\nAccess GenPipes Slurm Cluster on GCP\nUse the following command to log into the login node of your GCP Slurm cluster:\ngcloud compute ssh login1 --zone=northamerica-northeast1-a\nAfter running the command mentioned above, you are now on GenPipes cloud deployment login node.\nMonitoring GenPipes deployment in GCP\nThe installation is still running and once you log into the login node of your GCP Slurm cluster, you will see the following welcome message:\n*** Slurm is currently being installed/configured in the background. ***\nA terminal broadcast will announce when installation and configuration is complete.\nWait for the terminal broadcast. This can take up to 10 minutes. Once you have received it or when you log to the GCP Slurm Cluster login node and there is no warning message displayed as you login, you can go to the next step.\nValidate GenPipes on GCP cloud runtime environment\nNow that your GCP Slurm Cluster is up and running without any error or warning, you can launch any GenPipes pipeline using the command:\ngenpipes <pipeline_name> –help\nFor example, to check the help information for GenPipes ChIP Sequencing pipelines, try:\ngenpipes chipseq -h\nGenPipes Test Run in the cloud\nTo run ChIP Sequencing pipeline using test dataset, use the login node on your GCP Slurm Cluster and issue the following commands in Google Cloud shell corresponding to each step below:\nStep 1: Create a new test directory\nmkdir -p chipseq_test\ncd chipseq_test\nStep 2: Download test dataset and unzip it as shown below:\nwget https://datahub-90-cw3.p.genap.ca/chipseq.chr19.new.tar.gz\ngzip -d chipsseq.chr19.new.tar.gz\nStep 3: GenPipes ChIP Sequencing pipeline needs a configuration file to setup the parameters required by this pipeline. You can download it using the command:\nwget https://bitbucket.org/mugqic/cloud_deplyoment/raw/master/quick_start.ini\nStep 4: Create ChIP Sequencing pipeline execution command script as shown below:\nbash # You do not need this line if you did a logout login cycle\n# The next line generates the pipeline script\ngenpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini $GNEPIPES_INIS/common_ini/chipseq.rorqual.ini quick_start.ini -j slurm -r readsets.chipseqTest.chr22.tsv -d designfile_chipseq.chr22.txt -s 1-18 > chipseqScript.sh\nStep 5:  Now you can execute ChIP Sequencing pipeline using the following command:\nbash chipseqScript.sh\nStep 6: View Progress of your pipeline and jobs by using squeue command. For more Slurm commands and details on monitoring Slurm cluster, you can see Slurm documentation\nThere are several ways to check the status of your jobs in the queue.  Below are a few SLURM commands to make use of.  Use the Linux ‘man’ command to find loads of additional information about these commands as well.\nsqueue <options>\nwhere you can use the following options:\n-u username\n-p partition\nFor example:\n[shalz@ubuntu_srv:/$ squeue -u shaloo\nJOBID PARTITION  NAME         USER     ST       TIME  NODES NODELIST(REASON)\n92311  debug     test         shaloo   R        0:06      2 e06ne9s0e,c17n09\n88915  xyz       GPU_test     shaloo   PD       0:00      1 (Priority)\n91716  xyz       hell_te      shaloo   R        0:08      2 d19res0e,d16n08\n91791  xyz       hello_te     shaloo   PD       0:00      2 (Priority)\n91792  xyz       hello_te     shaloo   PD       0:00      2 (Priority)\nStep 7: Shutdown GCP compute resources (Very Important!!!)\nYou need to make sure that after your jobs are run, you need to shutdown your GenPipes Slurm Cluster on GCP otherwise you will continue to be billed for the same.  After all your jobs have run, use the following command to exit out of your login node Google Cloud shell session:\nThis command closes the Slurm Login node shell. You are now back on your cloud shell administrative server. You can shut down your GenPipes cloud cluster by running the following script:\ngcloud deployment-manager deployments delete slurm\nFurther information\nIf you run into any issues, please refer to Troubleshooting runtime issues section of this documentation and visit GenPipes Support page.\nFor advanced GCP cloud setup scenarios and for the latest updates on deploying GenPipes in the cloud, details regarding Slurm stand alone cluster setup, or multi-cluster federation setup or to burst out of on-premise cluster to GCP while running GenPipes, refer to the README.md file.",
    "code_blocks": [
      "git clone https://bitbucket.org/mugqic/cloud_deplyoment.git",
      "cd cloud_deplyoment/gcp/\ngcloud deployment-manager deployments create slurm --config slurm-cluster.yaml",
      "gcloud compute ssh login1 --zone=northamerica-northeast1-a",
      "*** Slurm is currently being installed/configured in the background. ***\nA terminal broadcast will announce when installation and configuration is complete.",
      "genpipes <pipeline_name> –help",
      "mkdir -p chipseq_test\ncd chipseq_test",
      "wget https://datahub-90-cw3.p.genap.ca/chipseq.chr19.new.tar.gz\ngzip -d chipsseq.chr19.new.tar.gz",
      "wget https://bitbucket.org/mugqic/cloud_deplyoment/raw/master/quick_start.ini",
      "bash # You do not need this line if you did a logout login cycle\n# The next line generates the pipeline script\ngenpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini $GNEPIPES_INIS/common_ini/chipseq.rorqual.ini quick_start.ini -j slurm -r readsets.chipseqTest.chr22.tsv -d designfile_chipseq.chr22.txt -s 1-18 > chipseqScript.sh",
      "bash chipseqScript.sh",
      "-u username\n-j jobid\n-p partition\n-q qos",
      "[shalz@ubuntu_srv:/$ squeue -u shaloo\nJOBID PARTITION  NAME         USER     ST       TIME  NODES NODELIST(REASON)\n92311  debug     test         shaloo   R        0:06      2 e06ne9s0e,c17n09\n88915  xyz       GPU_test     shaloo   PD       0:00      1 (Priority)\n91716  xyz       hell_te      shaloo   R        0:08      2 d19res0e,d16n08\n91791  xyz       hello_te     shaloo   PD       0:00      2 (Priority)\n91792  xyz       hello_te     shaloo   PD       0:00      2 (Priority)",
      "gcloud deployment-manager deployments delete slurm"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:53.082308",
    "content_length": 6533,
    "word_count": 987
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/what_is_genpipes.html",
    "title": "Introduction",
    "content": "Introduction\nGenPipes is a workflow management system (WMS) that facilitates building robust genomic workflows. It is a unique solution that combines both a framework for development and end-to-end analysis pipelines, covering a large scope of genomic applications and research domains. It offers researchers a simple method to analyze different types of data, customizable to their needs and resources. In addition, it provides flexibility to create customized workflows besides the ones that are already implemented and validated by GenPipes.\nWhat is GenPipes?\nGenPipes is an open source, flexible, scalable Python-based framework that facilitates\nthe development and deployment of multi-step computational workflows. These workflows\nare optimized for High-Performance Computing (HPC) clusters and the cloud.\nThe following genomics application pipelines are already implemented and validated through GenPipes:\nWhole Genome Sequencing (WGS)\nWhole Exome Sequencing (WES)\nRNA Sequencing / Unmapped RNA Quality Control\nSARS-CoV-2 genome sequencing\nDe-novo RNA Sequencing\nDeep Whole Genome Sequencing\nLong-Read DNA Sequencing Nanopore and Revio Analysis\nNanopore SARS-CoV2 sequencing using ARTIC Nanopolish protocol\nTranscriptomics Assembly\nDNA Sequencing for Tumour Analysis\nWhole Genome Bisulphate Sequencing (WGBS)/ Reduced Representation Bisulphate Sequencing (RRBS)\nIllumina raw data processing\nEpiQC Analysis\nMetagenomics\nGenPipes software is available under a LGPL open-source license and is continuously updated to follow recent advances in genomics and bioinformatics. The framework can be accessed through multiple deployment mechanisms. It has already been configured on several servers at C3G HPC computing facility. Its also supports Cloud deployment through GCP. Besides this,  a Docker image is also available to facilitate additional installations on local / individual machines for small dataset analysis.\nBasic Concepts\nGenPipes is a Python based object oriented workflow management system that comes pre-built with several genomic analysis pipelines. A  typical analysis workflow comprises several complex actions that are interdependent and need to be managed in terms of input, output, process configuration and pipeline tuning.\nGenPipes refer to four kinds of objects that are used to manage different components of a typical analysis workflow. These are:\nIt is the main object that controls the overall genomic analysis workflow. For each type of analysis, a specific Pipeline object is defined. Pipeline objects can inherit from one another.\nEach Pipeline object uses Step objects to define the flow of the analysis. It provides flexibility in choosing which steps are called during a pipeline execution. Pipeline instance calls all steps implemented in a pipeline or only a set of steps selected by the user. Each step of a pipeline is a unit of execution block that encapsulates a part of the analysis (e.g., trimming or alignment). The Step object is a central unit object that corresponds to a specific analysis task. The execution of the task is directly managed by the code defined in each Step instance; some steps may execute their task on each sample individually while other steps execute their task using all the samples collectively.\nThe key purpose of each Step object is to generate a list of “Job” objects, which correspond to the consecutive execution of single tasks. The Job object defines the commands that will be submitted to the system. It contains all the elements needed to execute the commands, such as input files, modules to be loaded, as well as job dependencies and temporary files. Jobs are submitted in a workflow management system which uses various algorithms to schedule various types of jobs and optimizes time to result and resource usage.\nEach Job object is submitted to the GenPipes workflow management system using a specific “Scheduler” object. The Scheduler object creates execution commands that are compatible with the user’s computing system. Four different Scheduler objects have already been implemented (PBS, SLURM, Batch, and Daemon).\nPBS scheduler creates a batch script that is compatible with a PBS (TORQUE) system.\nSLURM scheduler creates a batch script that is compatible with a SLURM system.\nBatch scheduler creates a batch script that contains all the instructions to run all the jobs one after the other.\nDaemon scheduler creates a log of the pipeline command in a JSON file.\nHow does GenPipes work?\nGenPipes is a Python based object-oriented framework that is available to users as a command line tool. Figure below shows the general workflow of GenPipes.\nEach GenPipes Pipeline can be launched using a command line instruction. There are three kinds of inputs for each such instruction as shown in the figure above.\nMandatory command options\nOptional command options\nInput Files\nThe input files can be of two kinds - mandatory ones, that are needed for every pipeline and pipeline specific input files.\nMandatory input files include Configuration files and Readset files. Configuration files contain details regarding machine environment where the pipeline is executed and parameters that need to be set for each step of the pipeline. Default values are provided and can be changed in case of specific genomic analysis. GenPipes can be deployed locally in your data center or users can access pre-installed GenPipes on the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, servers. For details regarding different kinds of GenPipes deployment, refer to GenPipes Deployment Guide. If you are using GenPipes pre-installed on DRAC servers, then the basic configuration files are installed along with GenPipes. These can be supplemented with additional configuration files provided using the ‘-c’ option while running the command line instruction.\nBesides the mandatory configuration files, some pipelines have their own specific input file that must be provided. These include Design Files and Test Dataset files. These files are not provided by default and users need to supply them while running the pipelines. For the pipelines that require test dataset files, if you do not have access to any test datasets, you can try out some of the available Sample Test Dataset Files that are available as additional GenPipes resources for users.\nWhen the GenPipes command is launched, required modules and files will be searched for and validated. If all required modules and files are found, the analysis commands will be produced. GenPipes will create a directed acyclic graph that defines job dependency based on input and output of each step.\nOnce launched, the jobs are sent to the scheduler and queued. As jobs complete successfully, their dependent jobs are released by the scheduler to run. If a job fails, all its dependent jobs are terminated and an email notification is sent to the user. When GenPipes is re-run, it will detect which steps have successfully completed, as described in section “Smart relaunch features,” and skip them but will create the command script for the jobs that were not completed successfully. To force the entire command generation, despite successful completion, the “-f” option should be added.\nThe output of the command line instruction are in the form of summary reports and job status. Depending upon the pipeline, there are varied tools that can be used to view and analyze the results.  See Viewing and Analyzing GenPipes Results for further information.\nFor details on GenPipes usage and various bioinformatics pipelines see GenPipes User Guide.\nBioinformatics and the role of GenPipes\nThere has been significant technological evolution in Next Generation Sequencing techniques from improvement in the processes themselves, better infrastructure and software availability as well as in terms of lowering of costs associated with NGS processing.  For a good primer on the topic, refer to Introduction to Next Generation Sequencing.\nGenPipes plays a role in creating data analysis and processing pipelines for all kinds of high-throughput sequencing data and cutting edge genomic analysis, as highlighted in the figure below:",
    "code_blocks": [
      "Introduction to Next Generation Sequencing"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:54.235044",
    "content_length": 8163,
    "word_count": 1244
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/using_gp.html",
    "title": "Genomic Analysis with GenPipes",
    "content": "Genomic Analysis with GenPipes\nThis document describes the GenPipes execution environment and how to use various genomic analysis pipelines.  It assumes you already have access to GenPipes through one of the available GenPipes Deployment Options.\nTopics covered in this document includes information regarding what is available in a typical GenPipes deployment and how to access it. Besides that it covers details on required software and environment configurations, various kinds of inputs required to run genomic analysis such as command options, configuration details, readset file and design file. There are example runs highlighting how to issue the pipeline commands with requisite inputs and pipeline specific configurations.\nGenPipes Executable\nThe GenPipes framework can be used to perform various genomic analyses corresponding to the available pipelines.  GenPipes is a command line tool. The underlying object-oriented framework is developed in Python. It simplifies the development of new features and its adaptation to new systems; new workflows can be created by implementing a Pipeline object that inherits features and steps from other existing Pipeline objects.\nSimilarly, deploying GenPipes on a new system may only require the development of the corresponding Scheduler object along with specific configuration files. GenPipes’ command execution details have been implemented using a shared library system, which allows the modification of tasks by simply adjusting input parameters. This simplifies code maintenance and makes changes in software versions consistent across all pipelines.\nRunning GenPipes\nPre-Requisites\nBefore running GenPipes, you may want to visit the checklist of pre-requisites for GenPipes.\nHere is how you can launch GenPipes. Following is the generic command to run GenPipes:\ngenpipes <pipeline-name> -c config -r readset-file -s 1-n -g list-of-commands.txt\nbash list-of-commands.txt\n<pipeline-name> refers to one of the available  GenPipes Pipelines\n-s <step-range-number-1-n> refers to the specific steps in the pipeline that need to be executed\n-c refers to the configuration file, multiple files can be specified, say one for cluster specific configuration and another that is pipeline specific configuration settings\n-r readset file, an input file required by the pipeline\n-g the commands for running the pipeline are output to this file.\nTerminology\nIn the context of GenPipes, you need to be familiar with the following terms.  These constitute inputs and configuration required before you can launch the pipelines.\nReadset File\nConfiguration files\nDesign files\nTest Datasets\nLaunching GenPipes\nTo launch GenPipes, the following is needed:\nName of the pipeline corresponding to one of the available  GenPipes Pipelines.\nA readset file that contains information about the samples, indicated using the flag “-r”. GenPipes can aggregate and merge samples as indicated by the readset file.\nConfiguration/ini files that contain parameters related to the cluster and the third-party tools, indicated using the flag “-c”. Configuration files are customizable, allowing users to adjust different parameters.\nThe specific steps to be executed, indicated by the flag “-s”.\nIn addition to the configuration files and the input readset file, certain pipelines such as ChIP-Seq and RNA sequencing (RNA-Seq), require a design file that describes each contrast. Custom sample groupings can be defined in the design file. Design files are indicated by the flag “-d”. More information on the design file and the content of each file type can be found in the GenPipes User Guide.\nExample Run\nThe following example shows how you can run the ChIP Sequencing pipeline using GenPipes installed on Compute Canada data centres. Please ensure you have login access to GenPipes servers.  Refer to checklist of pre-requisites for GenPipes before you run this example.\nWe will now run the pipeline using a test dataset.\nYou need to first download the test dataset by visiting this link:\nChiP Sequencing Test Dataset\nIn the downloaded tar file, you will find the fastq read files in folder “rawData” and will find the readset file (readset.chipseq.txt) that describes that dataset.\nPlease ensure you have access to the “Rorqual\" server in Digital Research Alliance of Canada (DRAC), formerly Compute Canada, data centre. We will run this analysis on Rorqual as follows:\ngenpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini $GENPIPES_INIS/common_ini/\\ |key_ccdb_server_cmd_name|\\.ini -r readset.chipseq.txt -s 1-15 -g chipseq_cmd.sh\nTo understand what $GENPIPES_INIS refers to, please see instructions on how to access GenPipes on Compute Canada servers.\nIn the command above,\n-c defines the ini configuration files\n-r defines the readset file\n-s defines the steps of the pipeline to execute, use genpipes chipseq -h to check steps\nBy default, Slurm scheduler is used when using the GenPipes deployment on the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, servers such as Rorqual, Nibi, Fir and Narval. On the abacus server, you need to use PBS scheduler. For that you need to specify “-j pbs” option as shown below:\ngenpipes chipseq -c $GENPIPES_INIS/chipseq/chiseq.base.ini $GENPIPES_INIS/common_ini/abacus.ini -r readset.chipseq.txt -s 1-15 -j pbs -g chipseq_cmd.sh\nThe above command generates a list of instructions that need to be executed to run the ChIP sequencing pipeline. These instructions are stored in the file:\nchipseq_cmd.sh\nTo execute these instructions, use:\nbash chipseq_cmd.sh\nYou will not see anything happen, but the commands will be sent to the server job queue. So do not run this more than once per job.\nTo confirm that the commands have been submitted, wait a minute or two depending on the server and type:\nsqueue -u <userID>\nwhere, <userID> is your login id for accessing the DRAC infrastructure.\nOn abacus, the equivalent command is:\nshowq -u <userID>\nIn case you ran the command to submit the jobs several times and launched too many commands you do not want, you can use the following line of code to cancel ALL commands:\nscancel -u <userID>\nOr on abacus:\nshowq -u <userID> | tr \"|\" \" \"| awk '{print $1}' | xargs -n1 canceljob\nCongratulations!\nYou just successfully issued the ChIP sequencing analysis pipeline commands!!!\nAfter the processing is complete, you can access quality control plots in the report/ directory and you can find peak data in the peak_call/ directory.\nFor more information about output formats please consult the webpage of the third party tool used.\nThe ChIP sequencing pipeline also analyzes ATAC-Seq data if the “-t atacseq” flag is used. For more information on the available steps in that pipeline use:\ngenpipes chipseq -h\nExample Run with Design File\nCertain pipelines that involve comparing and contrasting samples, need a Design File. The design file can contain more than one way to contrast and compare samples.  To see how this works with GenPipes pipelines, lets run a RNA-Sequencing experiment.\nRNA-Sequencing Test Dataset\nFirst, you need to download the test dataset from here.\nIn the downloaded tar file, you will find the fastq read files in folder rawData and will find the readset file (readset.rnaseq.txt) that describes that dataset. You will also find the design file (design.rnaseq.txt) that contains the contrast of interest.\nFollowing is the content of the Readset file (readset.rnaseq.txt):\nSample        Readset Library RunType Run     Lane    Adapter1        Adapter2        QualityOffset   BED     FASTQ1  FASTQ2  BAM\nGM12878_Rep1  GM12878_Rep1    myLibrary       PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_GM12878_chr19_Rep1_R1.fastq.gz  raw_data/rnaseq_GM12878_chr19_Rep1_R2.fastq.gz\nGM12878_Rep2  GM12878_Rep2    myLibrary       PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_GM12878_chr19_Rep2_R1.fastq.gz  raw_data/rnaseq_GM12878_chr19_Rep2_R2.fastq.gz\nH1ESC_Rep1    H1ESC_Rep1      myLibrary2      PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_H1ESC_chr19_Rep1_R1.fastq.gz    raw_data/rnaseq_H1ESC_chr19_Rep1_R2.fastq.gz\nH1ESC_Rep2    H1ESC_Rep2      myLibrary2      PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_H1ESC_chr19_Rep2_R1.fastq.gz    raw_data/rnaseq_H1ESC_chr19_Rep2_R2.fastq.gz\nThis analysis contains 4 samples with a single readset each. They are all PAIRED_END runs and have a pair of fastq files in the “rawData” folder.\nFollowing is the content of the Design file (design.rnaseq.txt):\nSample        H1ESC_GM12787\nH1ESC_Rep1    1\nH1ESC_Rep2    1\nGM12878_Rep1  2\nGM12878_Rep2  2\nWe see a single analysis that compares two replicates of H1ESC to two replicates of group GM12878.\nLet us now run this RNA-Sequencing analysis on the Rorqual server at Digital Research Alliance of Canada (DRAC), formerly Compute Canada. Use the following command:\ngenpipes rnaseq -c $GENPIPES_INIS/rnaseq/rnaseq.base.ini $GENPIPES_INIS/common_ini/\\ |key_ccdb_server_cmd_name|\\.ini -r readset.rnaseq.txt -d design.rnaseq.txt -g rnaseqScript.txt\nbash rnaseqScript.txt\nThe commands will be sent to the job queue and you will be notified once each step is done. If everything runs smoothly, you should get MUGQICexitStatus:0 or Exit_status=0. If that is not the case, then an error has occurred after which the pipeline usually aborts. To examine the errors, check the content of the job_output folder.\nSubmitting GenPipes Pipeline Runs\nHPC site policies typically limit the number of jobs that a user can submit in a queue. These sites deploy resource schedulers such as Slurm, or PBS/Torque for scheduling and sharing of HPC resources. Integrating with the resource schedulers and dealing with resource constraints are critical to ensuring productivity of HPC users. GenPipes caters to these user pain points through intelligent utilities that help in smartly chunking and submitting pipeline runs, resubmitting the jobs and ensuring that there are no errors in scheduler calls.\nGenPipes offers a utility scripts namely, `chunk_genpipes.sh` and `submit_genpipes` to enable better integration with resource schedulers (Slurm, PBS/Torque) deployed on HPC clusters.\nThe usage model is as follows. First, you need to issue GenPipes pipeline command with -g GENPIPES_FILE option to store all pipeline commands in a bash script.  Next, you need to use the utility called `chunk_genpipes.sh` that takes as input this bash script file GENPIPES_FILE and chunks scheduler jobs into a folder `job_chunks` (default) or the one you specify. Note that chunk_genpipes.sh utility is supposed to be run for a pipeline bash script  only once. After successful chunking, user can use the `submit_genpipes` utility to smartly submit the pipeline jobs to the scheduler without having to worry about scheduler integration and exceeding queue limits as these utilities take care of that.  Better HPC integration is offered by `submit_genpipes` as it looks for any error in the calls made to the scheduler and makes sure to auto-correct them based on chunking limits specified through `chunk_genpipes.sh` earlier.\nThe `submit_genpipes` script lets GenPipes users manage resource constraints in a flexible and robust manner. GenPipes user can delegate job submission to this script and use `watch` command to monitor the submitted jobs. At any time,  GenPipes users can stop monitoring the submitted jobs by issuing `Ctrl-C` to a running `watch` command in the terminal. After a clean `ctrl-C` stop of or if the watch command was killed in another manner, for example when a session is killed after ssh disconnection, users can restart monitoring GenPipes jobs to the queuing system by simply invoking the `watch` command again.\nThe `submit_genpipes` script comes with a fail safe mechanism that will resubmit jobs that failed to be sent to the scheduler up to 10 times (default).\nExample: Submitting ChipSeq jobs\nHere is an example of how to use the `submit_genpipes` script with the Chip Sequencing Pipeline:\nM_FOLDER=path_to_folder\ngenpipes chipseq <options> --genpipes_file chipseq_script.sh\nchunk_genpipes.sh chipseq_script.sh $M_FOLDER\nsubmit_genpipes $M_FOLDER\nThe `chunk_genpipes.sh` script is used to create job chunks of specified size that are submitted at a time. Please note that this script should be executed only once before using `submit_genpipes` to submit jobs.\nThe `submit_genpipes` script can be run for multiple GenPipes pipelines simultaneously, to `submit jobs` belonging to respective pipelines. You need to ensure that each submit_genpipes script invocation refers to a different job_chunks folder corresponding to the pipeline.\n`submit_genpipes` script runs can be stopped by `Ctrl-C` keystroke and restarted at will.\n`submit_genpipes` script has intelligent lock mechanism that prevents invoking two simultaneous runs of `submit_genpipes` in parallel, on the on the same job chunking folder or GenPipes pipeline run.\nThe figure below demonstrates how the `submit_genpipes` utility works. The pipeline command file output is fed into `chunk_genpipes.sh` script which creates the chunks folder as a one time activity. This chunk folder is monitored by the `submit_genpipes` script.\nFor a complete list of available GenPipes utilities, refer to the `genpipes/util` folder in the source tree.\nSample Output\nThis section demonstrates how a GenPipes user can chunk job submission and submit job, monitor their status using `chunk_genpipes.sh` and `submit_genpipes` utilities  and `watch` command.\nAfter generating GenPipes command file, say for GenPipes DNASeq Pipeline, ‘dnaseq.sh`, follow these two steps:\nStep 1: Use chunk size 20 to chunk command submission to the scheduler\nchunk_genpipes.sh dnaseq.sh job_chunks 20\nIn the command above, 20 specifies the number of jobs in a chunk\nFigure below shows the output of the command above:\nOutput of chunk_genpipes command\nStep 2: Invoke submit_genpipes script to monitor the submitted GenPipes jobs\nsubmit_genpipes job_chunks -n 800\nIn the command above, 800 refers to the total number of jobs that can be submitted simultaneously at a time to the scheduler.\nFigure below shows the output of the submit_genpipes command:\nOutput of submit_genpipes command\nFurther Information\nGenPipes pipelines are built around third party tools that the community uses in particular fields. To understand the output of each pipeline, please read the documentation pertaining to the tools that produced the output.\nYou can see all available GenPipes pipelines for a complete listing of all supported pipelines. To see examples of running other pipelines and also for figuring out how to run pipelines locally or in the cloud on your own GenPipes deployment, refer to GenPipes Tutorials.\nFor further information or help with particular pipelines, you can send us an email to:\ninfo@computationalgenomics.ca",
    "code_blocks": [
      "genpipes <pipeline-name> -c config -r readset-file -s 1-n -g list-of-commands.txt\nbash list-of-commands.txt",
      "genpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini $GENPIPES_INIS/common_ini/\\ |key_ccdb_server_cmd_name|\\.ini -r readset.chipseq.txt -s 1-15 -g chipseq_cmd.sh",
      "genpipes chipseq -c $GENPIPES_INIS/chipseq/chiseq.base.ini $GENPIPES_INIS/common_ini/abacus.ini -r readset.chipseq.txt -s 1-15 -j pbs -g chipseq_cmd.sh",
      "showq -u <userID> | tr \"|\" \" \"| awk '{print $1}' | xargs -n1 canceljob",
      "Sample        Readset Library RunType Run     Lane    Adapter1        Adapter2        QualityOffset   BED     FASTQ1  FASTQ2  BAM\nGM12878_Rep1  GM12878_Rep1    myLibrary       PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_GM12878_chr19_Rep1_R1.fastq.gz  raw_data/rnaseq_GM12878_chr19_Rep1_R2.fastq.gz\nGM12878_Rep2  GM12878_Rep2    myLibrary       PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_GM12878_chr19_Rep2_R1.fastq.gz  raw_data/rnaseq_GM12878_chr19_Rep2_R2.fastq.gz\nH1ESC_Rep1    H1ESC_Rep1      myLibrary2      PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_H1ESC_chr19_Rep1_R1.fastq.gz    raw_data/rnaseq_H1ESC_chr19_Rep1_R2.fastq.gz\nH1ESC_Rep2    H1ESC_Rep2      myLibrary2      PAIRED_END      1       1       AGATCGGAAGAGCACACGTCTGAACTCCAGTCA       AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT       33              raw_data/rnaseq_H1ESC_chr19_Rep2_R1.fastq.gz    raw_data/rnaseq_H1ESC_chr19_Rep2_R2.fastq.gz",
      "Sample        H1ESC_GM12787\nH1ESC_Rep1    1\nH1ESC_Rep2    1\nGM12878_Rep1  2\nGM12878_Rep2  2",
      "genpipes rnaseq -c $GENPIPES_INIS/rnaseq/rnaseq.base.ini $GENPIPES_INIS/common_ini/\\ |key_ccdb_server_cmd_name|\\.ini -r readset.rnaseq.txt -d design.rnaseq.txt -g rnaseqScript.txt\nbash rnaseqScript.txt",
      "M_FOLDER=path_to_folder\n\ngenpipes chipseq <options> --genpipes_file chipseq_script.sh\n\nchunk_genpipes.sh chipseq_script.sh $M_FOLDER\n\nsubmit_genpipes $M_FOLDER",
      "chunk_genpipes.sh dnaseq.sh job_chunks 20",
      "submit_genpipes job_chunks -n 800"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:55.459598",
    "content_length": 15168,
    "word_count": 2152
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/gp_job_results.html",
    "title": "Analyzing GenPipes Results",
    "content": "Analyzing GenPipes Results\nThis document contains generic insights related to viewing and analyzing results obtained after running GenPipes Pipelines.  It assumes that you are familiar with GenPipes Basics and have used one of the pipelines already as demonstrated in Using GenPipes.\nIf you are a new user of GenPipes and have successfully run it for the first time, you are likely to wonder, what is next?  How can you determine whether all the jobs that were part of one of the GenPipes Pipelines were executed successfully? Where are the results located once the pipelines are run and what is the best way to view and analyze GenPipes results in general.\nYou can find answers to such questions here.\nGenPipes Results\nWhen a pipeline is run successfully, by default, the output is saved to the same location the pipeline was launched in the first place. This behavior can be changed by modifying the output directory when launching the pipeline using -o or –output-dir, for example:\ngenpipes rnaseq -o /PATH/TO/OUTPUT ... (other options) -g genpipes_cmd_list.sh\nbash genpipes_cmd_list.sh\nFor most pipelines, GenPipes creates an html report with most of the results in the pipeline. To create the report, you need to rerun the same command you ran to create the GenPipes commands, but add –report to it.\nRefer to the ‘job_output’ directory. In that directory, you can find subdirectories that roughly correspond to each step in the pipeline, inside those are the log files. At the top, of the directory, there is a file that is named with the following convention:\njob_output/PIPELINE_job_list_YEAR-MM-DDTHH.MM.SS\nwhere, PIPELINE corresponds to the pipeline name and YEAR-MM-DDTHH.MM.SS to the date and time the pipeline was launched. This job list file in the job_output folder that can help you determine the status of each job and sub-jobs as well.\nThis job_list file can be used to check the status of only those jobs that are scheduled using PBS and Slurm schedulers.  Also, this feature is not supported when you run GenPipes in a container.\nAbacus Reports (PBS Scheduler)\nIn the latest GenPipes release v6.x, generating reports has changed. Earlier, there were two different scripts for generating reports when using PBS or Slurm scheduler. With version 6.x it is now a single command for both schedulers.\nVersion 6.x\nUse the following command to generate the tab-delimited report:\ngenpipes tools log_report --tsv log.out job_output/{PIPELINE}_job_list_{DATE}T{TIME}\nFor example:\ngenpipes tools log_report --tsv log.out job_output/DnaSeq_job_list_2018-06-26T12.54.27\nVersion 5.x, 4.x, 3.x\nUse the log_report.pl script to generate the tab-delimited report for Abacus:\nlog_report.pl job_output/{PIPELINE}_job_list_{DATE}T{TIME}\nFor example:\nlog_report.pl job_output/RnaSeq_job_list_2018-06-26T12.54.27\nThe log_report command returns the status of each job. In addition to the detailed report, it also outputs a summary file that includes the number of jobs that completed successfully, those that failed, and those that are still active/inactive.\nYou can save the reports as .csv or .tsv files and open them in Excel on your laptop.  For each job, there is an exit code that indicates job status.\nAnalysis Reports (Slurm Scheduler)\nIn the latest GenPipes release v6.x, generating reports has changed. Earlier, there were two different scripts for generating reports when using PBS or Slurm scheduler. With version 6.x it is now a single command for both schedulers.\nVersion 6.x\nUse the following command to generate the tab-delimited report:\ngenpipes tools log_report --tsv log.out job_output/{PIPELINE}_job_list_{DATE}T{TIME}\nFor example:\ngenpipes tools log_report --tsv log.out job_output/DnaSeq_job_list_2018-06-26T12.54.27\nVersion 5.x, 4.x, 3.x\nUse the log_report.py script to generate the html report for running Slurm Scheduler on the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, servers:\nlog_report.py job_output/{PIPELINE}_job_list_{DATE}T{TIME} --tsv log.out\nBy default, unlike the log_report.pl script, the script log_report.py does not provide detailed output.  Use the –tsv option to get a detailed output.\nThe log_report command returns the status of each job. In addition to the detailed report, it also outputs a summary file that includes the number of jobs that completed successfully, those that failed, and those that are still active/inactive.\nYou can save the reports as .csv or .tsv files and open them in Excel on your laptop.  For each job, there is an exit code that indicates job status.\nExit Codes\nFollowing are some of the common job exit codes:\n0 - Exit code of 0 means that the pipeline ran without any issues\n271 - This exit code typically means that there was insufficient RAM allocated and hence the job did not run successfully.\n-11 - Exit code -11 indicates that the job was prematurely killed as it exceeded the allocated walltime - basically insufficient compute resources were assigned for the job.\nFor every GenPipes Pipeline run, output is created in the default or specified location. However, please note that what is actually written in the output location varies significantly between each pipeline.  Refer to GenPipes User Guide, Pipelines Reference section for details regarding the processing performed by different pipelines.\nGenPipes Errors & Log Files\nWhen launched, GenPipes creates a job_output folder where it stores the logs and errors from all the jobs. If errors occur, you need to look into the job_output folder for the log of the step that failed to see what it last printed before it shut down. This usually helps to understand what potentially happened. When a job finishes successfully, it will create a file with the extension .done.\nGenPipes Logs are stored in the job_output folder under the appropriate folder for each step. For more details see GenPipes Error Logs.\nVisualization and Analysis\nGenPipes output results vary a lot depending upon each specific pipeline and the way it is configured to run. Also, the way results are analyzed is also dependent on the final objective of the analysis. For example, in case of visualizations, the results have to be imported to R or Python or some alternative visualization package.\nTools such as Integrative Genomics Viewer (IGV - Integrative Genomics Viewer), Genome Browser Gateway and several others are utilized for visualization of results. These tools vary from pipeline to pipeline.\nFigure below demonstrates one such tool used for RNA Sequencing Analysis.\nFigure: Genome Browser Gateway\nThe best way for new users and beginners to begin to explore the results is to look at the interactive MultiQC reports.  Most pipelines support this and generate an html report that is saved under the report directory.\nThe MultiQC report that is automatically generated when running the pipeline summarizes the most important results in the pipeline, while providing tables or plots per sample. More advanced users can use the output files used as input to MultiQC to generate their own visualizations or further analyze results using their own methods.\nAs mentioned earlier, visualization of results varies from pipeline to pipeline. As a reference, you can see RNA Sequencing Analysis Visualization of results.\nFigure below shows how the data is displayed once the alignment files are opened on IGV.\nFigure: Data Alignment visualizer using IGV Tool\nGenPipes Relaunch\nIf GenPipes fails, for any reason, you can recreate the commands and relaunch them.\nWhen recreating the commands, GenPipes can detect jobs that have completed successfully and will not rerun them.  That being said, unless you understand why a job failed and fix it, relaunched jobs might fail with the same error.\nGenPipes Clean\nGenPipes stores some temporary files that are useful to shorten potential reruns. To delete all these files, you can run the GenPipes command with –clean. This will delete a lot of files that were marked by GenPipe developers as “removable”. If you are interested in temporary files, avoid the –clean command.\nTracking GenPipes Environment Parameters for quoting in Publications\nIn order to keep track of all parameters used, GenPipes creates a final .config.trace.ini file each time it is run. It is a good idea to keep a copy of that file in order to keep track of software versions used when publishing your paper or publication.",
    "code_blocks": [
      "genpipes rnaseq -o /PATH/TO/OUTPUT ... (other options) -g genpipes_cmd_list.sh\n\nbash genpipes_cmd_list.sh",
      "job_output/PIPELINE_job_list_YEAR-MM-DDTHH.MM.SS",
      "genpipes tools log_report --tsv log.out job_output/{PIPELINE}_job_list_{DATE}T{TIME}",
      "genpipes tools log_report --tsv log.out job_output/DnaSeq_job_list_2018-06-26T12.54.27",
      "log_report.pl job_output/{PIPELINE}_job_list_{DATE}T{TIME}",
      "log_report.pl job_output/RnaSeq_job_list_2018-06-26T12.54.27",
      "genpipes tools log_report --tsv log.out job_output/{PIPELINE}_job_list_{DATE}T{TIME}",
      "genpipes tools log_report --tsv log.out job_output/DnaSeq_job_list_2018-06-26T12.54.27",
      "log_report.py job_output/{PIPELINE}_job_list_{DATE}T{TIME} --tsv log.out",
      "RNA Sequencing Analysis Visualization of results"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T09:59:56.680343",
    "content_length": 8378,
    "word_count": 1301
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/troubleshooting_rt_issues.html",
    "title": "Troubleshooting",
    "content": "Troubleshooting\nThis document lists some commonly faced issues related to GenPipes deployment and usage.  These could be related to GenPipes deployment, software dependencies, environment setup or usage options.\nThe objective here is to list some commonly encountered issues and their fixes so that new users can benefit and focus more on using GenPipes.  These are mostly run-time issues that a new user may face. If you are looking at troubleshooting GenPipes as part of feature development or contributing to GenPipes code, please refer to the Developer Guide and Troubleshooting Guide.\nAlso see the latest discussions on GenPipes Google Group.\nGenPipes User: Troubleshooting Guide",
    "code_blocks": [],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/get-started/trbs_runtime/gp_user_trbs.html"
    ],
    "scraped_at": "2025-08-26T09:59:57.903491",
    "content_length": 685,
    "word_count": 103
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/quickstart_gp.html",
    "title": "Quick Start",
    "content": "Quick Start\nTable of Contents\nGenPipes Check List\nChoose GenPipes Deployment Option\nGet GenPipes\nRun GenPipes\nGetting Help on GenPipes\nGetting Involved\nThis guide aims at helping you get started with GenPipes quickly (under 15 minutes!) and locate the information you require to meet any of the objectives listed below:\nObjective: GenPipes Check List\nFigure out what is needed to run GenPipes in terms of software, hardware, login accounts and other resources. See GenPipes Check List section in Quick Start Guide Table of Contents above.\nObjective: Familiarise with GenPipes basics\nUnderstand GenPipes basics, concepts and figure out how it works in order to apply it for your specific genomic analysis use case. See Introduction to GenPipes` and Why GenPipes? section of this documentation.\nObjective: Choose GenPipes deployment\nShould I deploy GenPipes on my local resources or use a pre-installed copy of GenPipes deployed on the Digital Research Alliance of Canada (DRAC),formerly Compute Canada, servers? Refer to Choose GenPipes deployment option section in Quick Start Guide Table of Contents above.\nObjective: Get Latest GenPipes\nHow to obtain the latest copy of GenPipes (sources, pre-packaged build, documentation etc.)?  Refer to Get GenPipes section in the Table of Contents above.\nObjective: Run GenPipes for the first time\nHow do I run / execute GenPipes in 3 simple steps? See Run GenPipes section of this guide as shown in Table of Contents above.\nObjective: Get Help!\nHow can I achieve this with GenPipes or I ran into issues - where do I find help? Check out Getting Help on GenPipes above.\nObjective: Solve commonly known issues in GenPipes Usage\nI ran into issues - is there a workaround or has it been resolved already? For this, you may want to check out GenPipes FAQ or browse GenPipes Channels for more insights.",
    "code_blocks": [],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/gp_checklist.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/choose_gp_dep.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/get_gp.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/run_gp.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/find_help.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/get_involved.html"
    ],
    "scraped_at": "2025-08-26T09:59:59.103926",
    "content_length": 1838,
    "word_count": 297
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/gp_checklist.html",
    "title": "GenPipes Check List",
    "content": "GenPipes Check List\nBefore you begin using GenPipes for the first time, it would be useful to quickly go over the basic requirements of using GenPipes.  This checklist is created to help you get started with GenPipes quickly and get your bearings correct before you embark upon genomic analysis with GenPipes.\nSkill sets and domain know-how\nThe following table lists some of the skill sets, technologies and know-how that a GenPipes user is expected to have.\nRequirements\nHigh Performance Computing (HPC) for genomics - environment and software modules used\nCluster Computing, Cloud environment for genomics\nDRAC formerly Compute Canada HPC infrastructure usage account and familiarity\nUse of shared, distributed File Systems such as CVMFS\nHPC Schedulers such as PBS, Slurm\nBioinformatics Software Dependencies (Local/Pre-installed on CCDB)\nGenPipes Basic Concepts - Pipelines, steps etc.\nPre-requisites for running GenPipes\nBesides the skills and technologies listed above, you need powerful hardware and some pre-installed software modules before you can run GenPipes.  For a complete list, please refer to Before Running GenPipes for details.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:00.495227",
    "content_length": 1148,
    "word_count": 170
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/choose_gp_dep.html",
    "title": "Choose GenPipes Deployment Option",
    "content": "Choose GenPipes Deployment Option\nBefore you can quickly get started with GenPipes, you need some clarity regarding your GenPipes deployment preference.\nThis choice governs where you will execute or run GenPipe pipelines for conducting genomic analysis. As you may be aware, genomic analysis is time consuming and resource intensive processing that requires significant amount of raw compute power, memory and reference dataset storage space.\nGenPipes is available as pre-deployed software that can be run using compute and storage resources available for C3G users through the partnership with Digital Research Alliance of Canada (DRAC), formerly Compute Canada datacenter.  If you already have a DRAC account, this may be your fastest option to use GenPipes.\nOtherwise, there are other options available as well but they may need additional time and expertise to download and deploy one of the GenPipes Release Builds that come in the form of a compressed file with some basic modules, test datasets and reference genomes that can be used for basic genomic analysis.  This kind of deployment works for simple analysis unless you have access to high end compute power or local HPC resources.\nLocal deployment can be either on bare metal server, on a VM or inside a container.\nFor details on available GenPipes deployment options and how to deploy, please visit How to deploy GenPipes? Guide.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:01.899805",
    "content_length": 1393,
    "word_count": 220
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/get_gp.html",
    "title": "Get GenPipes",
    "content": "Get GenPipes\nYou can choose one of the available GenPipes deployment options.\nIf you choose to use GenPipes deployed on the Digital Research Alliance of Canada (DRAC), formerly Compute Canada, servers, then you don’t need to get and setup any build or code. Just start using your DRAC account and access GenPipes deployed on DRAC servers.\nHowever, if you choose to deploy GenPipes on your own instead of using the pre-installed setup managed by C3G using DRAC servers, you can use either of the options listed below.  You can either obtain a downloadable GenPipes Release Build or obtain a copy of GenPipes sources and build, deploy them yourself.\nPre-built, packaged GenPipes via the GenPipes Download Page\nFor detailed instructions on how to setup and configure GenPipes locally on your server refer to Deploying GenPipes locally.\nIf you wish to deploy and setup GenPipes inside a container, see here for instructions.\nYou can also choose to deploy GenPipes on Google Compute Cloud. See here for details.\nBuild your own version of GenPipes by downloading GenPipes Source Code\nFor details on GenPipes build process, see GenPipes Developer Guide. Once you generate the build, you can follow the same instructions as for using a downloadable release to setup and configure GenPipes.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:03.644766",
    "content_length": 1282,
    "word_count": 208
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/run_gp.html",
    "title": "Run GenPipes",
    "content": "Run GenPipes\nUsage Change Effective v5.x onward\nUsers of GenPipes v4.x or prior releases must carefully read the usage changes listed below.\nWhat has Changed?\nEffective v5.0 onward, the GenPipes distribution is based on Python packaging and not on Python modules as before. Also Python 2.7 users must upgrade to Python v3.11.1 or higher to use v5.x GenPipes pipelines.\nThe latest GenPipes v6.0 supports only Python v3.12.0 and above.\nTo run any GenPipes pipelines, follow the new command syntax (effective GenPipes v5.x onward):\nuser@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nuser@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nWhen using GenPipes deployed on the Digital Research Alliance Canada servers such as Rorqual, a new variable ‘GENPIPES_INIS’ is introduced for streamlining access to the config files in the genpipes commands. In the future, ‘MUGQIC_PIPELINES_HOME’ will be deprecated.\n$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini\n$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini\nPlease note that the old variable, ‘MUGQIC_PIPELINES_HOME’ will still be accessible and is still in use for instructions on how to deploy GenPipes locally, in the cloud, or in a container.\nA new Long Read DNA Sequencing pipeline is now available in v6.0 that supports two protocols:\nThe Nanopore pipeline available in v5.0 and earlier releases is no longer supported. The same functionality is now available as one of the supported protocols in the new Long Read DNA Sequencing pipeline.\nThe RNA Sequencing (De Novo) pipeline has been updated in v5.0 release.\nThe EpiQC pipeline, HiC-Seq pipeline, and AmpliconSeq qiime protocol have been deprecated starting v5.0 onward.\nThe DNA-Seq high coverage pipeline and the TumorPair pipeline have been merged into a single workflow DNA-Seq.\nThe Methylseq pipeline has a new protocol option using the gemBS aligner in addition to Bismark.\nGenome build GRCh38 (human) is now the default reference genome for all pipelines, but other versions or species can be selected via config files, as before.\nMarkdown style reports have been deprecated for all pipelines starting v5.0 onward and replaced entirely with MultiQC reports.\nAfter selecting a GenPipes deployment option you must setup and configure your GenPipes environment as per your deployment choice. After you have configured it, you are ready to execute GenPipes.\nFirst, test that GenPipes is correctly deployed and it works, without actually running a pipeline. Use genpipes <pipeline_cmd> with the  --help or -h option.\nFor details, please refer to the GenPipes User Guide User Guide section.\nEarlier, you could run GenPipes by invoking individual <pipeline_cmd>. With the release of version 5.0.0 onward, there is a new format to run GenPipes pipelines.\nuser@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nuser@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nSee Pipeline Reference Guide for details on how to run a specific pipeline in the example section.\nNew users may benefit from the GenPipes Tutorial section of this documentation that provides step by step instructions on how to execute a few sample GenPipes pipelines.  There is also a tutorial available for running GenPipes in the cloud using Google Compute Platform.\nIn case you run into any runtime issues or errors, do refer to Troubleshooting GenPipes Runtime Issues or browse the GenPipes Support sections.\nHappy analysis with GenPipes! We would love to hear your feedback on GenPipes or the documentation.\nContributions to GenPipes or its documentation are most welcome!",
    "code_blocks": [
      "user@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "user@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini",
      "$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini",
      "genpipes <pipeline_cmd>",
      "user@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "user@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:05.422268",
    "content_length": 3740,
    "word_count": 544
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/find_help.html",
    "title": "Getting Help on GenPipes",
    "content": "Getting Help on GenPipes\nIf you need help on GenPipes, you can refer to Support section in the left hand navigation pane.\nAlternatively, it is recommended that you check out various Channels. It is quite possible that the issue you are facing is already being discussed and resolved there with some workaround.\nIf you are locally situated near C3G centre in Montreal or Toronto, you can avail the Open Door sessions at C3G.\nBesides these, if you need deeper consultation regarding usage of GenPipes Pipelines for a real life research scenario, you can save time and money by requesting for C3G Bioinformatics Services tailored to your specific needs.\nFor any support issues, you can also drop an email to the GenPipes Help Desk on the address:\npipelines%40computationalgenomics.ca",
    "code_blocks": [
      "pipelines%40computationalgenomics.ca"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:06.925584",
    "content_length": 781,
    "word_count": 127
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/get_involved.html",
    "title": "Getting Involved",
    "content": "Getting Involved\nGenPipes bioinformatics pipelines were originally developed at the Canadian Centre for Computational Genomics (C3G), as part of the GenAP project. These, are now available for public use. GenPipes includes a wide array of pipelines, including RNA-Seq, ChIP-Seq, WGS, exome sequencing, Bisulfite sequencing, Hi-C, capture Hi-C, metagenomics and the latest SARS-CoV-2 genome sequencing pipeline.\nContributing to GenPipes\nWe love your input!\nThe objective of this document is to make contributing to GenPipes as easy and transparent as possible, for any of the following types of contributions from the wider community:\nReporting a bug or issue\nSubmitting a fix\nProposing new features\nDiscussing code optimizations\nBecoming a maintainer\nTracking GenPipes Issues and Feature Requests\nEarlier, GenPipes used Bitbucket.\nNow GenPipes has moved to GitHub as the preferred source code repository.\nStarting version 6.0.0, GitHub is used for hosting the latest source code, tracking issues and feature requests, as well as for issuing pull requests for contributions.\nPull requests are the best way to propose changes to the code base. Please ensure that you follow the following process as GenPipes is complex and requires careful validation of any updates.  Also, you may need computational resources as GenPipes is demanding in terms of compute resources.\nFork the repo and create your branch from master.\nIf you have added code that should be tested, add tests.\nIf applicable, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIssue that pull request!\nGenPipes is available under the LGPL license. Any contributions you make will be under the LGPLv3 license.\nReporting a bug or issue\nYou can report a bug or an issue related to GenPipes using the following template:\nSingle line summary briefly describing the issue\nSteps to reproduce\nSpecific instructions\nIf required, provided commands and code that caused the issue\nMake sure you specify GenPipes version number, deployment type\nWhat was expected and what actually happened\nOther insights or logs that can help debug the issue.\nThis guide is adapted from FaceBook Draft.js Guide.",
    "code_blocks": [],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:08.496315",
    "content_length": 2179,
    "word_count": 335
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipeline_ref.html",
    "title": "Pipelines Reference",
    "content": "Pipelines Reference\nUsage Change Effective v5.x onward\nUsers of GenPipes v4.x or prior releases must carefully read the usage changes listed below.\nWhat has Changed?\nEffective v5.0 onward, the GenPipes distribution is based on Python packaging and not on Python modules as before. Also Python 2.7 users must upgrade to Python v3.11.1 or higher to use v5.x GenPipes pipelines.\nThe latest GenPipes v6.0 supports only Python v3.12.0 and above.\nTo run any GenPipes pipelines, follow the new command syntax (effective GenPipes v5.x onward):\nuser@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nuser@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh\nWhen using GenPipes deployed on the Digital Research Alliance Canada servers such as Rorqual, a new variable ‘GENPIPES_INIS’ is introduced for streamlining access to the config files in the genpipes commands. In the future, ‘MUGQIC_PIPELINES_HOME’ will be deprecated.\n$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini\n$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini\nPlease note that the old variable, ‘MUGQIC_PIPELINES_HOME’ will still be accessible and is still in use for instructions on how to deploy GenPipes locally, in the cloud, or in a container.\nA new Long Read DNA Sequencing pipeline is now available in v6.0 that supports two protocols:\nThe Nanopore pipeline available in v5.0 and earlier releases is no longer supported. The same functionality is now available as one of the supported protocols in the new Long Read DNA Sequencing pipeline.\nThe RNA Sequencing (De Novo) pipeline has been updated in v5.0 release.\nThe EpiQC pipeline, HiC-Seq pipeline, and AmpliconSeq qiime protocol have been deprecated starting v5.0 onward.\nThe DNA-Seq high coverage pipeline and the TumorPair pipeline have been merged into a single workflow DNA-Seq.\nThe Methylseq pipeline has a new protocol option using the gemBS aligner in addition to Bismark.\nGenome build GRCh38 (human) is now the default reference genome for all pipelines, but other versions or species can be selected via config files, as before.\nMarkdown style reports have been deprecated for all pipelines starting v5.0 onward and replaced entirely with MultiQC reports.\nGenPipes implements standardized genomics workflows, including  DNA-Seq, tumour analysis, RNA-Seq, de novo RNA-Seq, ChIP-Seq, SARS-CoV-2 genome sequencing, methylation sequencing, Hi-C, capture Hi-C, and metagenomics. All pipelines have been implemented following a robust design and development routine by following established best practices standard operating protocols. The pipelines accept a binary sequence alignment map (BAM) or a FASTQ file as input.\nThis guide contains usage manual and reference information for all available GenPipes genomic analysis pipelines. Visit GenPipes Real-life Applications and use cases to see how these pipelines can be deployed for complex next-generation genomic analysis in real life scenarios.\nActive Pipelines\nAmplicon Sequencing Pipeline\nChIP Sequencing Pipeline\nCoV Sequencing Pipeline\nDNA Sequencing Pipeline\nLong Read DNA Sequencing Pipeline\nMethylation Sequencing Pipeline\nNanopore CoVSeQ Pipeline\nRNA Sequencing Pipeline\nRNA Sequencing (De-Novo) Pipeline\nRNA Sequencing (Light) Pipeline\nDeprecated/Merged Pipelines\nDNA Sequencing (High Coverage) Pipeline (Merged)\nEpiQC Pipeline (Deprecated)\nHiC Sequencing Pipeline  (Deprecated)\nTumor Pair Sequencing Pipeline (Merged)",
    "code_blocks": [
      "user@rorqual% <pipeline name>.py [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "user@rorqual% genpipes <pipeline name> [options] -g genpipes_cmd.sh\nuser@rorqual% bash genpipes_cmd.sh",
      "$MUGQIC_PIPELINES_HOME/pipelines/<pipeline>/<pipeline>.base.ini",
      "$GENPIPES_INIS/<pipeline>/<pipeline>.base.ini"
    ],
    "internal_links": [
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_ampliconseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_chipseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_covseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_dnaseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_longread_dnaseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_wgs_methylseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_nanopore_covseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_rnaseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_rnaseq_denovo.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_rnaseq_light.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_dnaseq_highcov.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_epiqc.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_hicseq.html",
      "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_tumourpair.html"
    ],
    "scraped_at": "2025-08-26T10:00:10.071097",
    "content_length": 3499,
    "word_count": 480
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_ampliconseq.html",
    "title": "Amplicon Sequencing Pipeline",
    "content": "Amplicon Sequencing Pipeline\nVersion 6.0.0\nAmplicon supports only dada2 protocol by default. The Amplicon QIIME protocol is deprecated from GenPipes v5.x onward.\nTo use QIIME` protocol, try an older version of GenPipes.\ngenpipes ampliconseq [options] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh\n-d DESIGN, --design DESIGN\ndesign file\n-r READSETS, --readsets READSETS\nreadset file\n-h                        show this help message and exit\n--help                    show detailed description of pipeline and steps\n-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\nconfig INI-style list of files; config parameters\nare overwritten based on files order\n-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'\n-o OUTPUT_DIR, --output-dir OUTPUT_DIR\noutput directory (default: current)\n-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\njob scheduler type (default: slurm)\n-f, --force               force creation of jobs even if up to date (default:\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--force_mem_per_cpu       FORCE_MEM_PER_CPU\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--json-pt                 create JSON file for project_tracking database\ningestion (default: false i.e. JSON file will NOT be\n--report                  create 'pandoc' command to merge all job markdown\nreport files in the given step range into HTML, if\nthey exist; if --report is set, --job-scheduler,\n--force, --clean options and job up-to-date status\nare ignored (default: false)\n--clean                   create 'rm' commands for all job removable files in\nthe given step range, if they exist; if --clean is\nset, --job-scheduler, --force options and job up-to-\ndate status are ignored (default: false)\nNote: Do not use -g option with --clean, use '>' to redirect\nthe output of the --clean command option\n-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\nlog level (default: info)\n--sanity-check            run the pipeline in `sanity check mode` to verify\nall the input files needed for the pipeline to run\nare available on the system (default: false)\n--container {wrapper, singularity} <IMAGE PATH>\nrun pipeline inside a container providing a container\nimage path or accessible singularity hub path\n--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\ngenpipes/ressources/container/bin/container_wrapper.sh,\na convenience option for using GenPipes in a container)\n-v, --version             show the version information and exit\n-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\nCommands for running the pipeline are output to this\nfile pathname. The data specified to pipeline command\nline is processed and pipeline run commands are\nstored in GENPIPES_FILE, if this option is specified\n. Otherwise, the output will be redirected to stdout\n. This file can be used to actually \"run the\nGenPipes Pipeline\"\nDo not use -g option with -clean.\nUse ‘>’ to redirect the output of the genpipes command to a file when using -clean option.\ngenpipes ampliconseq -c $GENPIPES_INIS/ampliconseq/ampliconseq.base.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.ampliconseq.txt \\\n-s 1-8 -g ampliconseqCommands.sh\nbash ampliconseqCommands.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\nIt is recommended that you use the -g GENPIPES_CMD.sh option instead of redirecting the output of the pipeline command to a file via > GENPIPES_CMD.sh.\ngenpipes [pipeline] [options] -g genpipes_cmd.sh\nbash genpipes_cmd.sh\ngenpipes [pipeline] [options] > genpipes_cmd.sh\nbash genpies_cmd.sh\nThe > scriptfile method is supported but will be deprecated in a future GenPipes release.\nTest Dataset\nYou can download the test dataset for this pipeline here.\nTest Datasets\nFigure: Schema of DADA2 Amplicon Sequencing protocol\nTrimmomatic16S Step\nMerge Trimmomatic Stats\nFlash Pass 1\nAmplicon Length Parser\nFlash Pass 2\nMerge Flash Stats\nRun MultiQC\nTrimmomatic16S\nMiSeq raw reads adapter & primers trimming and basic QC is performed using Trimmomatic. If an adapter FASTA file is specified in the config file (section ‘trimmomatic’, param ‘adapter_fasta’), it is used first. Else, Adapter1, Adapter2, Primer1 and Primer2 columns from the readset file are used to create an adapter FASTA file, given then to Trimmomatic. Sequences are reversed-complemented and swapped.\nThis step takes as input files:\nMiSeq paired-End FASTQ files from the readset file.\nMerge Trimmomatic Stats\nThe trim statistics per readset are merged in this step.\nFlash Pass 1\nPerform first pass of FLASH. FLASH is a fast computational tool to extend the length of short reads by overlapping paired-end reads from fragment libraries that are sufficiently short.\nFlash Pass 2\nPerform second pass of FLASH to find the correct overlap between paired-end reads and extend the reads by stitching them together.\nAmplicon Length Parser\nIn this step, we look at FLASH output to set amplicon lengths input for DADA2. As minimum eligible length, a given length needs to have at least 1% of the total number of amplicons.\nMerge Flash Stats\nThe paired end merge statistics per readset are merged in this step.\nThis step checks for the design file required for the principal component analysis (PCA) based on amplicon sequence variant (ASV).\nRun MultiQC\nA quality control report for all samples is generated.\nSee MultiQC documentation for details.\nAmplicon sequencing (ribosomal RNA gene amplification analysis) is a highly targeted metagenomic pipeline used to analyze genetic variation in specific genomic regions. Amplicons are Polymerase Chain Reaction (PCR) products and the ultra-deep sequencing allows for efficient variant identification and characterization.\nUses of Amplicon sequencing\nDiagnostic microbiology utilizes amplicon-based profiling that allows to sequence selected amplicons such as regions encoding 16S rRNA that are used for species identification.\nDiscovery of rare somatic mutations in complex samples such as tumors mixed with germline DNA.\nGenPipes supports the DADA2 Amplicon sequencing protocol for recovering single-nucleotide resolved Amplicon Sequence Variants (ASVs) from the Amplicon data.\nSee Schema tab for the pipeline workflow. Check the README.md file for implementation details.\nAmplicon sequencing techniques\nAmplicon Sequencing Primer\nHigh-throughput amplicon sequencing.\nTrimmomatic - flexible trimming.\nAmplicon Sequencing Readset File\nPlease make sure you use the special Amplicon Readset file format and not the general readset file format.",
    "code_blocks": [
      "genpipes ampliconseq [options] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh",
      "-d DESIGN, --design DESIGN\n\n                          design file",
      "-r READSETS, --readsets READSETS\n\n                          readset file",
      "-h                        show this help message and exit",
      "--help                    show detailed description of pipeline and steps",
      "-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\n\n                          config INI-style list of files; config parameters\n                          are overwritten based on files order",
      "-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'",
      "-o OUTPUT_DIR, --output-dir OUTPUT_DIR\n\n                          output directory (default: current)",
      "-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\n\n                          job scheduler type (default: slurm)",
      "-f, --force               force creation of jobs even if up to date (default:\n                          false)\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--force_mem_per_cpu       FORCE_MEM_PER_CPU\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--json-pt                 create JSON file for project_tracking database\n                          ingestion (default: false i.e. JSON file will NOT be\n                          created)",
      "--report                  create 'pandoc' command to merge all job markdown\n                          report files in the given step range into HTML, if\n                          they exist; if --report is set, --job-scheduler,\n                          --force, --clean options and job up-to-date status\n                          are ignored (default: false)",
      "--clean                   create 'rm' commands for all job removable files in\n                          the given step range, if they exist; if --clean is\n                          set, --job-scheduler, --force options and job up-to-\n                          date status are ignored (default: false)\n\n                          Note: Do not use -g option with --clean, use '>' to redirect\n                          the output of the --clean command option",
      "-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\n\n                          log level (default: info)",
      "--sanity-check            run the pipeline in `sanity check mode` to verify\n                          all the input files needed for the pipeline to run\n                          are available on the system (default: false)",
      "--container {wrapper, singularity} <IMAGE PATH>\n\n                          run pipeline inside a container providing a container\n                          image path or accessible singularity hub path",
      "--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\n                          genpipes/ressources/container/bin/container_wrapper.sh,\n                          a convenience option for using GenPipes in a container)",
      "-v, --version             show the version information and exit",
      "-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\n\n                          Commands for running the pipeline are output to this\n                          file pathname. The data specified to pipeline command\n                          line is processed and pipeline run commands are\n                          stored in GENPIPES_FILE, if this option is specified\n                          . Otherwise, the output will be redirected to stdout\n                          . This file can be used to actually \"run the\n                          GenPipes Pipeline\"",
      "genpipes ampliconseq -c $GENPIPES_INIS/ampliconseq/ampliconseq.base.ini \\\n    $GENPIPES_INIS/common_ini/rorqual.ini \\\n    -r readset.ampliconseq.txt \\\n    -s 1-8 -g ampliconseqCommands.sh\n\nbash ampliconseqCommands.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes [pipeline] [options] -g genpipes_cmd.sh\n\nbash genpipes_cmd.sh",
      "genpipes [pipeline] [options] > genpipes_cmd.sh\n\nbash genpies_cmd.sh"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:11.474433",
    "content_length": 6884,
    "word_count": 976
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_chipseq.html",
    "title": "ChIP Sequencing Pipeline",
    "content": "ChIP Sequencing Pipeline\nVersion 6.0.0\ngenpipes chipseq [options] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh\n-t {chipseq, atacseq}, --type {chipseq, atacseq}\nType of pipeline (default=chipseq)\n-d DESIGN, --design DESIGN\ndesign file\n-r READSETS, --readsets READSETS\nreadset file\n-h                        show this help message and exit\n--help                    show detailed description of pipeline and steps\n-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\nconfig INI-style list of files; config parameters\nare overwritten based on files order\n-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'\n-o OUTPUT_DIR, --output-dir OUTPUT_DIR\noutput directory (default: current)\n-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\njob scheduler type (default: slurm)\n-f, --force               force creation of jobs even if up to date (default:\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--force_mem_per_cpu       FORCE_MEM_PER_CPU\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--json-pt                 create JSON file for project_tracking database\ningestion (default: false i.e. JSON file will NOT be\n--report                  create 'pandoc' command to merge all job markdown\nreport files in the given step range into HTML, if\nthey exist; if --report is set, --job-scheduler,\n--force, --clean options and job up-to-date status\nare ignored (default: false)\n--clean                   create 'rm' commands for all job removable files in\nthe given step range, if they exist; if --clean is\nset, --job-scheduler, --force options and job up-to-\ndate status are ignored (default: false)\nNote: Do not use -g option with --clean, use '>' to redirect\nthe output of the --clean command option\n-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\nlog level (default: info)\n--sanity-check            run the pipeline in `sanity check mode` to verify\nall the input files needed for the pipeline to run\nare available on the system (default: false)\n--container {wrapper, singularity} <IMAGE PATH>\nrun pipeline inside a container providing a container\nimage path or accessible singularity hub path\n--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\ngenpipes/ressources/container/bin/container_wrapper.sh,\na convenience option for using GenPipes in a container)\n-v, --version             show the version information and exit\n-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\nCommands for running the pipeline are output to this\nfile pathname. The data specified to pipeline command\nline is processed and pipeline run commands are\nstored in GENPIPES_FILE, if this option is specified\n. Otherwise, the output will be redirected to stdout\n. This file can be used to actually \"run the\nGenPipes Pipeline\"\nDo not use -g option with -clean.\nUse ‘>’ to redirect the output of the genpipes command to a file when using -clean option.\ngenpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readsets.chipseq.txt -d design.chipseq.txt \\\n-s 1-19 -g chipseqCommands.sh\nbash chipseqCommands.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\nThe commands will be sent to the job queue and you will be notified once each step is done. If everything runs smoothly, you should get MUGQICexitStatus:0 or Exit_status=0. If that is not the case, then an error has occurred after which the pipeline usually aborts. To examine the errors, check the content of the job_output folder.\nIt is recommended that you use the -g GENPIPES_CMD.sh option instead of redirecting the output of the pipeline command to a file via > GENPIPES_CMD.sh.\ngenpipes [pipeline] [options] -g genpipes_cmd.sh\nbash genpipes_cmd.sh\ngenpipes [pipeline] [options] > genpipes_cmd.sh\nbash genpies_cmd.sh\nThe > scriptfile method is supported but will be deprecated in a future GenPipes release.\nTest Dataset\nYou can download the test dataset for this pipeline here.\nTest Datasets\nFigure: Schema of ChIP Sequencing protocol\nFigure: Schema of ChIP Sequencing -t atacseq protocol\nPicard Sam to Fastq\nTrimmomatic\nMerge Trimmomatic Stats\nMapping BWA Mem Sambamba\nSAMbamba Merge BAM\nSAMbamba Mark Duplicates\nSAMbamba View Filter\nBedtools Blacklist Filter\nHomer Make Tag Directory\nDeeptools QC\nHomer Make UCSC file\nMACS2 call peak\nHomer annotate peaks\nHomer find motifs genome\nAnnotation Graphs\nDifferential Binding\nIHEC Metrics\nMultiQC Report\nCRAM Output\nGATK Haplotype Caller\nMerge and Call Individual GVCF\nPicard Sam to Fastq\nTrimmomatic\nMerge Trimmomatic Stats\nMapping BWA Mem Sambamba\nSAMbamba Merge BAM\nSAMbamba Mark Duplicates\nSAMbamba View Filter\nBedtools Blacklist Filter\nHomer Make Tag Directory\nHomer Make UCSC file\nMACS2 ATAC-seq call peak\nhomer_annotate_peaks|\nHomer find motifs genome\nAnnotation Graphs\nDifferential Binding\nIHEC Metrics\nMultiQC Report\nCRAM Output\nGATK Haplotype Caller\nMerge and Call Individual GVCF\nPicard Sam to Fastq\nIf FASTQ files are not already specified in the Readset file, then this step converts SAM/BAM files from the input Readset into FASTQ format. Otherwise, it does nothing.\nTrimmomatic\nRaw reads quality trimming and removing of Illumina adapters is performed using Trimmomatic Process.  If an adapter FASTA file is specified in the config file (section ‘trimmomatic’, param ‘adapter_fasta’), it is used first. Else, ‘Adapter1’ and ‘Adapter2’ columns from the readset file are used to create an adapter FASTA file, given then to Trimmomatic. For PAIRED_END readsets, readset adapters are reversed-complemented and swapped, to match Trimmomatic Palindrome strategy. For SINGLE_END readsets, only Adapter1 is used and left unchanged.\nIf available, trimmomatic step in Hi-C analysis takes FASTQ files from the readset file as input. Otherwise, it uses the FASTQ output file generated from the previous Picard Sam to Fastq step conversion of the BAM files.\nMerge Trimmomatic Stats\nThe trim statistics per Readset file are merged at this step.\nMapping BWA Mem Sambamba\nThis step takes as input files trimmed FASTQ files, if available. Otherwise it takes FASTQ files from the readset. If readset is not supplied then it uses FASTQ output files from the previous Picard Sam to Fastq conversion of BAM files.\nSAMbamba Merge BAM\nBAM readset files are merged into one file per sample. Merge is done using Sambamba.\nIf available, the aligned and sorted BAM output files from previous Mapping BWA Mem Sambamba step are used as input. Otherwise, BAM files from the readset file is used as input.\nSAMbamba Mark Duplicates\nMark duplicates. Aligned reads per sample are duplicates if they have the same 5’ alignment positions (for both mates in the case of paired-end reads). All but the best pair (based on alignment score) will be marked as a duplicate in the BAM file. Marking duplicates is done using SAMbamba.\nSAMbamba View Filter\nFilter unique reads by mapping quality using SAMbamba.\nThe number of raw/filtered and aligned reads per sample are computed at this stage.\nHomer Make Tag Directory\nThe Homer Tag directories, used to check for quality metrics, are computed at this step.\nSequencing quality metrics as tag count, tag auto-correlation, sequence bias and GC bias are generated.\nDeeptools QC\nFingerplot quality control answers the question “Did my ChIP work?” for ChIP-seq sample analysis. The tool processes indexed BAM files, creating a profile of cumulative read coverage. Reads in a specified window (bin) are counted, sorted, and plotted as a cumulative sum.\nThe Correlation Matrix tool analyzes and visualizes sample correlations using multiBamSummary or multiBigwigSummary output. It supports Pearson or Spearman methods to calculate correlation coefficients.\nHomer Make UCSC files\nWiggle Track Format files are generated from the aligned reads using Homer.  The resulting files can be loaded in browsers like IGV or UCSC.\nMACS2 call peak\nPeaks are called using the MACS2 software. Different calling strategies are used for narrow and broad peaks.  The mfold parameter used in the model building step is estimated from a peak enrichment diagnosis run.  The estimated mfold lower bound is 10 and the estimated upper bound can vary between 15 and 100.\nThe default mfold parameter of MACS2 is [10,30].\nMACS2 ATAC-seq call peak\nAssay for Transposon Accessible Chromatin (ATAC-seq) analysis enables measurement of chromatin structure modifications (nucleosome free regions) on gene regulation. This step involves calling peaks using the MACS2 software. Different calling strategies are used for narrow and broad peaks. The mfold parameter used in the model building step is estimated from a peak enrichment diagnosis run. The estimated mfold lower bound is 10 and the estimated upper bound can vary between 15 and 100.\nThe default mfold parameter of MACS2 is [10,30].\nHomer annotate peaks\nThe peaks called previously are annotated with HOMER tool using RefSeq annotations for the reference genome.  Gene ontology and genome ontology analysis are also performed at this stage.\nHomer find motifs genome\nIn this step, De novo and known motif analysis per design are performed using HOMER.\nAnnotation Graphs\nThis step focuses on peak location statistics. The following peak location statistics are generated per design: proportions of the genomic locations of the peaks. The locations are: Gene (exon or intron), Proximal ([0;2] kb upstream of a transcription start site), Distal ([2;10] kb upstream of a transcription start site), 5d ([10;100] kb upstream of a transcription start site), Gene desert (>= 100 kb upstream or downstream of a transcription start site), Other (anything not included in the above categories); The distribution of peaks found within exons and introns; The distribution of peak distance relative to the transcription start sites (TSS); the Location of peaks per design.\nThis step runs spp to estimate NSC and RSC ENCODE metrics. For more information - see quality enrichment of ChIP sequence data, phantompeakqualtools.\nDifferential Binding\nDifferential Binding step is meant for processing DNA data enriched for genomic loci, including ChIP- seq data enriched for sites where specific protein binding occurs, or histone marks are enriched. It uses DiffBind package that helps in identifying sites that are differentially bound between sample groups.\nGenPipes Chipseq pipeline performs differential binding based on the provided treatments and controls as per the particular comparison specified in the design file. The differential analysis results are generated separately for each specified comparison in the Design File, with correctly specified treatments (2) and controls (1) samples.\nSamples with ‘0’ (zero) are ignored during the comparison. For details regarding how to specify sample group membership in the design file, refer to Design File Format details.\nFor comparison, at least two samples for each group must be specified. If two samples per group are not specified, the differential binding step will be skipped during the pipeline run.\nIncorrect Design File Format\nIf the specified design file does not follow the specified Design File Format for ChIPseq pipeline, the differential binding step will be skipped during the pipeline run.\nThe output of differential analysis containing differentially bound peaks are saved as a TSV. In addition, for each comparison done using DiffBind, an html report is also generated for QC differential analysis.\nLimitation of Differential Binding\nDifferential Binding analysis currently supports pair-wise (two groups) comparisons only. If you have a more complex experimental design, you may manually conduct the analysis.\nThe Chip Sequencing protocol expects an input for the differential binding step. If pipeline users want to run this protocol without an input, they should skip the  Differential Binding step and run it themselves locally.\nIHEC Metrics\nThis step generates IHEC’s standard metrics.\nMultiQC Report\nA quality control report for all samples is generated.\nFor more detailed information see MultiQC documentation.\nBedtools Blacklist Filter\nRemove reads in blacklist regions from BAM with Bedtools intersect, if blacklist file is supplied, otherwise, do nothing.\nGATK Haplotype Caller\nUse GATK Haplotype caller for SNPs and small indels.\nMerge & Call Individual GVCF\nMerges the GVCFs of haplotype caller and also generates a per sample VCF containing genotypes.\nCRAM Output\nGenerate long term storage version of the final alignment files in CRAM format. Using this function will include the original final BAM file into the removable file list.\nChromatin Immunoprecipitation (ChIP) sequencing technique is used for mapping DNA-protein interactions. It is a powerful method for identifying genome-wide DNA binding sites for transcription factors and other proteins.\nThe ChIP-Seq workflow is based on the ENCODE Project workflow. The pipeline starts by trimming adapters and low quality bases and mapping the reads (single end or paired end ) to a reference genome using Burrows-Wheeler Aligner (BWA). Reads are filtered by mapping quality and duplicate reads are marked. It creates tag directories using Homer routines. Peaks are called using Model based Analysis for Chip Sequencing (MACS2) and annotated using Homer. Binding motifs are also identified using Homer. Metrics are calculated based on IHEC requirements.\nThe ChIP-Seq pipeline can also be used for assay for transposase-accessible chromatin using sequencing ATAC-Seq samples. Use ‘ATAC’ protocol option to run the pipeline using ATAC-Seq.\nSee Schema tab for the pipeline workflow. Check the README.md file for implementation details.\nFigure:  Schematic representation of major methods used to detect functional elements in DNA (Source PLOS)\nChIP-Seq Guidelines\nMUGQIC_Bioinfo_Chip-Seq.pptx\nChIP-Seq and Beyond\nSchematic representation of major methods to detect functional elements in DNA\nChIP Sequencing and ATAC Sequencing\nInput requirements\nThe Chip Sequencing protocol expects an input for the Differential Binding step. If pipeline users want to run this protocol without an input, they should skip the differential binding step and run it themselves locally.\nChip Sequencing Readset File\nPlease make sure you use the special ChIPSeq Pipeline Readset file format and not the general readset file format.\nChip Sequencing Design File Format\nChIPSeq Pipeline Design File Format\nThe ChIPSeq Pipeline has two protocols: atac-seq and chip-seq. Each of these protocols requires a specific design file.\nChIPseq Protocol Format\nSample        MarkName        EW22_EW3_vs_EW7_TC71\nEW22          H3K27ac         1\nEW3           H3K27ac         1\nEW7           H3K27ac         2\nTC71          H3K27ac         2\nATACseq Protocol Format\nNote that the MarkName value for ATACseq protocol should be atac unlike the ChIPseq protocol.\nSample        MarkName        EW22_EW3_vs_EW7_TC71\nEW22          atac            1\nEW3           atac            1\nEW7           atac            2\nTC71          atac            2\nwhere, the numbers specify the sample group membership for this contrast:\n'0' or '': the sample does not belong to any group;\n'1': the sample belongs to the control group;\n'2': the sample belongs to the treatment test case group.\nThe design file only accepts 1 for control, 2 for treatment and 0 for other samples that do not need to compare.\nIncorrect Design File Format\nPlease note that the first and second column in the design file must be sample name and histone mark/binding protein respectively.\nIf the user specifies any value other than the permitted ones above in the design file, the pipeline will fail.",
    "code_blocks": [
      "genpipes chipseq [options] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh",
      "-t {chipseq, atacseq}, --type {chipseq, atacseq}\n\n                          Type of pipeline (default=chipseq)",
      "-d DESIGN, --design DESIGN\n\n                          design file",
      "-r READSETS, --readsets READSETS\n\n                          readset file",
      "-h                        show this help message and exit",
      "--help                    show detailed description of pipeline and steps",
      "-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\n\n                          config INI-style list of files; config parameters\n                          are overwritten based on files order",
      "-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'",
      "-o OUTPUT_DIR, --output-dir OUTPUT_DIR\n\n                          output directory (default: current)",
      "-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\n\n                          job scheduler type (default: slurm)",
      "-f, --force               force creation of jobs even if up to date (default:\n                          false)\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--force_mem_per_cpu       FORCE_MEM_PER_CPU\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--json-pt                 create JSON file for project_tracking database\n                          ingestion (default: false i.e. JSON file will NOT be\n                          created)",
      "--report                  create 'pandoc' command to merge all job markdown\n                          report files in the given step range into HTML, if\n                          they exist; if --report is set, --job-scheduler,\n                          --force, --clean options and job up-to-date status\n                          are ignored (default: false)",
      "--clean                   create 'rm' commands for all job removable files in\n                          the given step range, if they exist; if --clean is\n                          set, --job-scheduler, --force options and job up-to-\n                          date status are ignored (default: false)\n\n                          Note: Do not use -g option with --clean, use '>' to redirect\n                          the output of the --clean command option",
      "-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\n\n                          log level (default: info)",
      "--sanity-check            run the pipeline in `sanity check mode` to verify\n                          all the input files needed for the pipeline to run\n                          are available on the system (default: false)",
      "--container {wrapper, singularity} <IMAGE PATH>\n\n                          run pipeline inside a container providing a container\n                          image path or accessible singularity hub path",
      "--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\n                          genpipes/ressources/container/bin/container_wrapper.sh,\n                          a convenience option for using GenPipes in a container)",
      "-v, --version             show the version information and exit",
      "-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\n\n                          Commands for running the pipeline are output to this\n                          file pathname. The data specified to pipeline command\n                          line is processed and pipeline run commands are\n                          stored in GENPIPES_FILE, if this option is specified\n                          . Otherwise, the output will be redirected to stdout\n                          . This file can be used to actually \"run the\n                          GenPipes Pipeline\"",
      "genpipes chipseq -c $GENPIPES_INIS/chipseq/chipseq.base.ini \\\n    $GENPIPES_INIS/common_ini/rorqual.ini \\\n    -r readsets.chipseq.txt -d design.chipseq.txt \\\n    -s 1-19 -g chipseqCommands.sh\n\nbash chipseqCommands.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes [pipeline] [options] -g genpipes_cmd.sh\n\nbash genpipes_cmd.sh",
      "genpipes [pipeline] [options] > genpipes_cmd.sh\n\nbash genpies_cmd.sh",
      "Sample        MarkName        EW22_EW3_vs_EW7_TC71\nEW22          H3K27ac         1\nEW3           H3K27ac         1\nEW7           H3K27ac         2\nTC71          H3K27ac         2",
      "Sample        MarkName        EW22_EW3_vs_EW7_TC71\nEW22          atac            1\nEW3           atac            1\nEW7           atac            2\nTC71          atac            2",
      "'0' or '': the sample does not belong to any group;\n'1': the sample belongs to the control group;\n'2': the sample belongs to the treatment test case group."
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:12.765867",
    "content_length": 15975,
    "word_count": 2362
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_covseq.html",
    "title": "CoV Sequencing Pipeline",
    "content": "CoV Sequencing Pipeline\nVersion 6.0.0\ngenpipes covseq [options] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh\n-d DESIGN, --design DESIGN\ndesign file\n-r READSETS, --readsets READSETS\nreadset file\n-h                        show this help message and exit\n--help                    show detailed description of pipeline and steps\n-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\nconfig INI-style list of files; config parameters\nare overwritten based on files order\n-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'\n-o OUTPUT_DIR, --output-dir OUTPUT_DIR\noutput directory (default: current)\n-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\njob scheduler type (default: slurm)\n-f, --force               force creation of jobs even if up to date (default:\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--force_mem_per_cpu       FORCE_MEM_PER_CPU\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--json-pt                 create JSON file for project_tracking database\ningestion (default: false i.e. JSON file will NOT be\n--report                  create 'pandoc' command to merge all job markdown\nreport files in the given step range into HTML, if\nthey exist; if --report is set, --job-scheduler,\n--force, --clean options and job up-to-date status\nare ignored (default: false)\n--clean                   create 'rm' commands for all job removable files in\nthe given step range, if they exist; if --clean is\nset, --job-scheduler, --force options and job up-to-\ndate status are ignored (default: false)\nNote: Do not use -g option with --clean, use '>' to redirect\nthe output of the --clean command option\n-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\nlog level (default: info)\n--sanity-check            run the pipeline in `sanity check mode` to verify\nall the input files needed for the pipeline to run\nare available on the system (default: false)\n--container {wrapper, singularity} <IMAGE PATH>\nrun pipeline inside a container providing a container\nimage path or accessible singularity hub path\n--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\ngenpipes/ressources/container/bin/container_wrapper.sh,\na convenience option for using GenPipes in a container)\n-v, --version             show the version information and exit\n-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\nCommands for running the pipeline are output to this\nfile pathname. The data specified to pipeline command\nline is processed and pipeline run commands are\nstored in GENPIPES_FILE, if this option is specified\n. Otherwise, the output will be redirected to stdout\n. This file can be used to actually \"run the\nGenPipes Pipeline\"\nDo not use -g option with -clean.\nUse ‘>’ to redirect the output of the genpipes command to a file when using -clean option.\ngenpipes covseq -c $GENPIPES_INIS/covseq/covseq.base.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n$GENPIPES_INIS/covseq/ARTIC_v4.1.ini \\\n-r readset.covseq.txt -g covseqCommands_mugqic.sh\nbash covseqCommands_mugqic.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\nIt is recommended that you use the -g GENPIPES_CMD.sh option instead of redirecting the output of the pipeline command to a file via > GENPIPES_CMD.sh.\ngenpipes [pipeline] [options] -g genpipes_cmd.sh\nbash genpipes_cmd.sh\ngenpipes [pipeline] [options] > genpipes_cmd.sh\nbash genpies_cmd.sh\nThe > scriptfile method is supported but will be deprecated in a future GenPipes release.\nTest Dataset\nThe CoV Seq test dataset is not publicly available due to privacy restrictions. Contact support for further queries about CoV Seq Test dataset.\nTest Datasets\nFigure: Schema of CoVSeq Sequencing protocol\nHost Reads Removal\nKraken Analysis\nMapping BWA Mem Sambamba\nSambamba Merge SAM Files\nSambamba Filtering\niVar Trim Primers\nCoVSeq Metrics\nFreebayes Calling\niVar Calling\nSNPEff Annotate\niVar Create Consensus\nBCFTools Create Consensus\nQUAST Consensus Metrics\nRename Consensus Header ivar\nRename Consensus Header freebayes\nncovtools Quickalign\nPrepare Table\nPrepare Report ivar\nPrepare Report Freebayes\nMultiQC Report\nHost Reads Removal\nIn this step, the filtered reads are aligned to a reference genome. The alignment is done per sequencing readset using the BWA Software and BWA-Mem Algorithm. BWA output BAM files are then sorted by coordinate using Sambamba.\nInput files for this step include:\nTrimmed FASTQ files, if available\nIf no FASTQ files then FASTQ files from supplied readset is used.\nIf no readset is supplied then FASTQ output from Picard SAM to FASTQ conversion of BAM files is used.\nKraken Analysis\nKraken is an ultra fast and highly accurate mechanism for assigning taxonomic labels to metagenomic DNA sequences. It achieves high sensitivity and speed by utilizing exact alignments of k-mers and a novel classification algorithm. This step performs Kraken analysis using output of the Sambamba processing in previous step.\nCutadapt processing cleans data by finding and removing adapter sequences, primers, poly-A tails and other types of unwanted sequence from high throughput sequencing reads obtained after Kraken analysis. In this step, quality trimming of raw reads and removing of adapters is performed by giving ‘Adapter1’ and ‘Adapter2’ columns from the readset file to Cutadapt. For PAIRED_END readsets, both adapters are used. For SINGLE_END readsets, only Adapter1 is used and left unchanged.\nTo trim the front of the read, use adapter_5p_fwd and adapter_5p_rev (For PAIRED_END only) in cutadapt section of the .ini file.\nThis step takes as input files:\nFASTQ files from the readset file, if available.\nOtherwise, FASTQ output files from previous Picard SAM to FASQ conversion of BAM files is used.\nMapping BWA Mem Sambamba\nThis step takes as input files trimmed FASTQ files, if available. Otherwise it takes FASTQ files from the readset. If readset is not supplied then it uses FASTQ output files from the previous Picard SAM to FASTQ conversion of BAM files.\nHere, the filtered reads are aligned to a reference genome. The alignment is done per sequencing readset. BWA Software is used for alignment with BWA-Mem Algorithm. BWA output BAM files are then sorted by coordinate using Sambamba.\nSambamba Merge SAM Files\nThis step uses Sambamba-Merge Tool to merge several BAM files into one. SAM headers are merged automatically similar to how it is done in Picard merging tool.\nSambamba Filtering\nIn this step, raw BAM files are filtered using Sambamba and and `awk` command is run to filter by insert size.\niVar Trim Primers\niVar uses primer positions supplied in a BED file to soft clip primer sequences from an aligned and sorted BAM file. Following this, the reads are trimmed based on a quality threshold(Default: 20). To do the quality trimming, iVar uses a sliding window approach(Default: 4). The windows slides from the 5’ end to the 3’ end and if at any point the average base quality in the window falls below the threshold, the remaining read is soft clipped.\nIn this step, primer sequences are removed from individual BAM files using iVar.\nCoVSeq Metrics\nIn this step, multiple metrics are computed from sequencing:\nDNA Sample Qualimap to facilitate quality control of alignment sequencing\nSambamba-flagstat for obtaining flag statistics from BAM file\nBED Tools GenomeCov is used for computing histograms(default), per-base reports and BEDGRAPH summaries of feature coverage of aligned sequences for a given genome.\nPicard HS Metrics are picked from SAM/BAM files. Only those metrics are collected that are specific for sequence datasets generated through hybrid-selection. Hybrid-selection (HS) is the most commonly used technique to capture exon-specific sequences for targeted sequencing experiments such as exome sequencing.\nFreebayes Calling\nFreebayes is a haplotype-based variant detector designed to find small polymorphisms, specifically SNPs (single-nucleotide polymorphisms), indels (insertions and deletions), MNPs (multi-nucleotide polymorphisms), and complex events (composite insertion and substitution events) smaller than the length of a short-read sequencing alignment.\nThis method avoids one of the core problems with alignment-based variant detection— that identical sequences may have multiple possible alignments. See Freebayes details here.\niVar Calling\nIn this step, iVar is used for creating a trimmed BAM file after trimming aligned reads in the input BAM file using primer positions specified in the BED input file. Besides the Freebayes Calling tool, ivar calling is also used in covseq pipeline as part of the latest release.\nSNPEff Annotate\nThis step uses SNPEff to annotate the data.\niVar Create Consensus\nIn this step, iVar is used to create consensus. Also, it removes primer sequences to individual BAM files using fgbio.\nBCFTools Create Consensus\nBCFtools consensus is created in this step. BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF\nQUAST Consensus Metrics\nIn this step, QUAST is used to compare and evaluate assemblies to rule out misassemblies.\nRename Consensus Header ivar\nConsensus sequence is the calculated order of most frequent residues found at each position in a sequence alignment. This information is important when considering sequence-dependent enzymes such as RNA Polymerase which is important for SAR-CoV-2 studies. In this step, header sequence can be modified in various ways as specified in rename type parameter: Multipart header, Replace word, Replace interval, and Add prefix/suffix.\nRename Consensus Header freebayes\nTwo variant calling tools are used in covseq pipeline - ivar and freebayes. In this step, the consensus header rename for freebayes is done.\nncovtools Quickalign\nUses quickalign to provides summary statistics, which can be used to determine the sequencing quality and evolutionary novelty of input genomes (e.g. number of new mutations and indels).\nIt uses ivar consensus as well as freebayes consensus to arrive at the alignment decisions.\nPrepare Table\nGathers all analysis data for quast, kraken and other metrics and module details.\nPrepare Report ivar\nPrepare ivar analysis report.\nPrepare Report Freebayes\nPrepare Freebayes analysis report.\nMultiQC Report\nA quality control report for all samples is generated.\nFor more detailed information see MultiQC documentation.\nCoVSeQ pipeline is designed as part of the partnership for Québec SARS-CoV-2 sequencing. It is funded by the CanCOGeN initiative through Genome Canada and from the Ministere de la santé et des services sociaux du Québec. For more details, see CoVSeQ website.\nThe COVID-19 pandemic required surveillance of the SARS-CoV-2 variants and fast spreading mutants through rapid and near real-time sequencing of the viral genome.  This is critical for effective health policy decision making. Gene sequencing pipelines require to be focused on specific characteristics of the COVID genome such as spike protein. To that effect, SARS-CoV-2 sequencing has been standardized through initiatives such as the Advancing Real-Time Infection Control Network (ARTIC) international initiative in which Illumina or Oxford Nanopore sequencing is carried out prior to whole viral genome amplification by tiling PCR or metagenomic approaches.\nSARS-CoV-2 whole genome sequencing data can help researchers in the following ways:\ncharacterize viral variants that occur within a given host\nunderstand variant fixation in a given population\nunderstand how the virus changes over time.\nGenPipes offers CoVSeQ Pipeline, a bioinformatics workflow that incorporates Illumina and ONT sequence. This pipeline is intended to address both short as well as long reads and input data obtained from Illumina as well as ONT Nanopore instruments.\nCoVSeQ pipeline helps in the genomic epidemiology of SARS-CoV-2 that output sequence alignment analysis and/or variants in various formats. It uses Kraken2, FreeBayes, SnpEff for genomic processing and bcftools, QUAST for consensus. The latest version of the pipeline uses ncov-tools v1.8 for alignment and quality control. SAM Tools are used for sorting and indexing BAM files. It performs variant calling on every sorted BAM file, obtaining major frequency viral variants per genome in VCF format using the Freebayes variant calling program, as frequency-based pooled caller. Merged variants are annotated using SnpEff.\nIt can be used for SARS-CoV-2 whole genome sequencing as per ARTIC protocol V4 or V4.1 by specifying appropriate .ini file as described below in usage and example sections.\nSee Schema tab for the pipeline workflow. Refer to CoVSeq Pipeline implementation README.md for details.\nCoVSeq Cost Effective Workflow\nCoVSeq Genome Analysis and Visualization\nARTIC v4 vs v4.1 selection\nThis CoVSeQ pipeline uses ARTIC v4 amplicon scheme as a default. If ARTIC v4.1 is required, use the appropriate .ini file. For all other amplicon schemes, add the appropriate primer and amplicon BED files and use a custom .ini for processing.",
    "code_blocks": [
      "genpipes covseq [options] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh",
      "-d DESIGN, --design DESIGN\n\n                          design file",
      "-r READSETS, --readsets READSETS\n\n                          readset file",
      "-h                        show this help message and exit",
      "--help                    show detailed description of pipeline and steps",
      "-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\n\n                          config INI-style list of files; config parameters\n                          are overwritten based on files order",
      "-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'",
      "-o OUTPUT_DIR, --output-dir OUTPUT_DIR\n\n                          output directory (default: current)",
      "-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\n\n                          job scheduler type (default: slurm)",
      "-f, --force               force creation of jobs even if up to date (default:\n                          false)\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--force_mem_per_cpu       FORCE_MEM_PER_CPU\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--json-pt                 create JSON file for project_tracking database\n                          ingestion (default: false i.e. JSON file will NOT be\n                          created)",
      "--report                  create 'pandoc' command to merge all job markdown\n                          report files in the given step range into HTML, if\n                          they exist; if --report is set, --job-scheduler,\n                          --force, --clean options and job up-to-date status\n                          are ignored (default: false)",
      "--clean                   create 'rm' commands for all job removable files in\n                          the given step range, if they exist; if --clean is\n                          set, --job-scheduler, --force options and job up-to-\n                          date status are ignored (default: false)\n\n                          Note: Do not use -g option with --clean, use '>' to redirect\n                          the output of the --clean command option",
      "-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\n\n                          log level (default: info)",
      "--sanity-check            run the pipeline in `sanity check mode` to verify\n                          all the input files needed for the pipeline to run\n                          are available on the system (default: false)",
      "--container {wrapper, singularity} <IMAGE PATH>\n\n                          run pipeline inside a container providing a container\n                          image path or accessible singularity hub path",
      "--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\n                          genpipes/ressources/container/bin/container_wrapper.sh,\n                          a convenience option for using GenPipes in a container)",
      "-v, --version             show the version information and exit",
      "-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\n\n                          Commands for running the pipeline are output to this\n                          file pathname. The data specified to pipeline command\n                          line is processed and pipeline run commands are\n                          stored in GENPIPES_FILE, if this option is specified\n                          . Otherwise, the output will be redirected to stdout\n                          . This file can be used to actually \"run the\n                          GenPipes Pipeline\"",
      "genpipes covseq -c $GENPIPES_INIS/covseq/covseq.base.ini \\\n  $GENPIPES_INIS/common_ini/rorqual.ini \\\n  $GENPIPES_INIS/covseq/ARTIC_v4.1.ini \\\n  -r readset.covseq.txt -g covseqCommands_mugqic.sh\n\nbash covseqCommands_mugqic.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes [pipeline] [options] -g genpipes_cmd.sh\n\nbash genpipes_cmd.sh",
      "genpipes [pipeline] [options] > genpipes_cmd.sh\n\nbash genpies_cmd.sh"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:14.144697",
    "content_length": 13387,
    "word_count": 1973
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_dnaseq.html",
    "title": "DNA Sequencing Pipeline",
    "content": "DNA Sequencing Pipeline\nVersion 6.0.0\nDNA Sequencing Pipeline Revamp!\nStarting from GenPipes v5.x onward, the DNA Sequencing Pipeline has been completely revamped. It is enhanced to include the functionality that was earlier provided in GenPipes v4.6.1 by the following standalone pipelines:\nTumor Pair Sequencing Pipeline Deprecated\nDNA Sequencing (High Coverage) Pipeline. Deprecated\ngenpipes dnaseq [-options ] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh\n-t {germline_snv, germline_sv, germline_high_cov, somatic_tumor_only, somatic_fastpass, somatic_ensemble, somatic_sv},\n--type {germline_snv, germline_sv, germline_high_cov, somatic_tumor_only, somatic_fastpass, somatic_ensemble, somatic_sv}\nDNAseq analysis type (default=germline_snv)\n-p PAIRS, --pairs PAIRS\nformat - patient_name,normal_sample_name,tumor_sample_name\n-d DESIGN, --design DESIGN\ndesign file\n-r READSETS, --readsets READSETS\nreadset file\n-h                        show this help message and exit\n--help                    show detailed description of pipeline and steps\n-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\nconfig INI-style list of files; config parameters\nare overwritten based on files order\n-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'\n-o OUTPUT_DIR, --output-dir OUTPUT_DIR\noutput directory (default: current)\n-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\njob scheduler type (default: slurm)\n-f, --force               force creation of jobs even if up to date (default:\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--force_mem_per_cpu       FORCE_MEM_PER_CPU\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--json-pt                 create JSON file for project_tracking database\ningestion (default: false i.e. JSON file will NOT be\n--report                  create 'pandoc' command to merge all job markdown\nreport files in the given step range into HTML, if\nthey exist; if --report is set, --job-scheduler,\n--force, --clean options and job up-to-date status\nare ignored (default: false)\n--clean                   create 'rm' commands for all job removable files in\nthe given step range, if they exist; if --clean is\nset, --job-scheduler, --force options and job up-to-\ndate status are ignored (default: false)\nNote: Do not use -g option with --clean, use '>' to redirect\nthe output of the --clean command option\n-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\nlog level (default: info)\n--sanity-check            run the pipeline in `sanity check mode` to verify\nall the input files needed for the pipeline to run\nare available on the system (default: false)\n--container {wrapper, singularity} <IMAGE PATH>\nrun pipeline inside a container providing a container\nimage path or accessible singularity hub path\n--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\ngenpipes/ressources/container/bin/container_wrapper.sh,\na convenience option for using GenPipes in a container)\n-v, --version             show the version information and exit\n-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\nCommands for running the pipeline are output to this\nfile pathname. The data specified to pipeline command\nline is processed and pipeline run commands are\nstored in GENPIPES_FILE, if this option is specified\n. Otherwise, the output will be redirected to stdout\n. This file can be used to actually \"run the\nGenPipes Pipeline\"\nDo not use -g option with -clean.\nUse ‘>’ to redirect the output of the genpipes command to a file when using -clean option.\nThe -p option is mandatory for running the following protocols:\nsomatic_fastpass\nsomatic_ensemble\nThe pair file specified via the -p option is a comma-separated file with the following format:\n<patient_name>,<normal_sample_name>,<tumor_sample_name>\nThe patient name value specified in the first column of the pair file (the patient/sample pair name) must be unique. You can specify multiple patients/sample pairs data, one per line, in the pair file.\nFor example:\ntumorPair_CEPHmixture_chr19,tumorPair_CEPHmixture_chr19_normal,tumorPair_CEPHmixture_chr19_tumor\njohn_doe_tumorPair_CEPHmixture_chr19,john_doe_tumorPair_CEPHmixture_chr19_normal,john_doe_tumorPair_CEPHmixture_chr19_tumor\ngenpipes dnaseq -t germline_snv -c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.dnaseq.txt \\\n-s 1-27 -j slurm -g dnaseq_germline_snv_cmd.sh\nbash dnaseq_germline_snv_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\ngenpipes dnaseq -t germline_sv -c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/dnaseq/dnaseq.sv.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.dnaseq.txt \\\n-s 1-25 -g dnaseq_germ_sv_cmd.sh\nbash dnaseq_germ_sv_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\nHigh Coverage\ngenpipes dnaseq -t germline_high_cov \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/dnaseq/dnaseq.high_cov.ini  \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.dnaseq.txt \\\n-s 1-15 -g dnaseq_germ_high_cov_cmd.sh\nbash dnaseq_germ_high_cov_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\ngenpipes dnaseq -t somatic_tumor_only \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.somatic_tumor_only.txt \\\n-s 1-38 -j slurm -g dnaseq_somatic_tumor_cmd.sh\nbash dnaseq_somatic_tumor_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\ngenpipes dnaseq -t somatic_fastpass \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/dnaseq/dnaseq.cancer.ini  \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.somatic_fastpass.txt \\\n-p pairs.somatic_fastpass.csv \\\n-s 1-38 -j slurm -g dnaseq_somatic_fastpass_cmd.sh\nbash dnaseq_somatic_fastpass_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\ngenpipes dnaseq -t somatic_ensemble \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/dnaseq/dnaseq.cancer.ini  \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.somatic_ensemble.txt \\\n-p pairs.somatic_ensemble.csv \\\n-s 1-38 -j slurm -g dnaseq_somatic_ensemble_cmd.sh\nbash dnaseq_somatic_ensemble_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\ngenpipes dnaseq -t somatic_sv \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/dnaseq/dnaseq.cancer.ini  \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.somatic_sv.txt \\\n-p pairs.somatic_sv.csv \\\n-s 1-38 -j slurm -g dnaseq_somatic_sv_cmd.sh\nbash dnaseq_somatic_sv_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\nIt is recommended that you use the -g GENPIPES_CMD.sh option instead of redirecting the output of the pipeline command to a file via > GENPIPES_CMD.sh.\ngenpipes [pipeline] [options] -g genpipes_cmd.sh\nbash genpipes_cmd.sh\ngenpipes [pipeline] [options] > genpipes_cmd.sh\nbash genpies_cmd.sh\nThe > scriptfile method is supported but will be deprecated in a future GenPipes release.\nTest Dataset\nUse the DNA sequencing test dataset for this pipeline.\nTest Datasets\nFigure below shows the schema of the DNA sequencing Germline SNV protocol.\nFigure: Schema - Germline SNV DNA Sequencing protocol\nFigure below shows the schema of the DNA sequencing Germline SV protocol.\nFigure: Schema - Germline SV DNA Sequencing protocol\nHigh Coverage\nFigure below shows the schema of the DNA sequencing Germline High Coverage protocol.\nFigure: Schema - Germline High Coverage DNA Sequencing protocol\nFigure below shows the schema of the DNA sequencing Somatic Tumor only protocol.\nFigure: Schema - Somatic Tumor Only DNA Sequencing protocol\nFigure below shows the schema of the DNA sequencing Somatic Fastpass protocol.\nFigure: Schema - Somatic Fastpass DNA Sequencing protocol\nFigure below shows the schema of the DNA sequencing Somatic Ensemble protocol.\nFigure: Schema - Somatic Ensemble DNA Sequencing protocol\nFigure below shows the schema of the DNA sequencing Somatic SV protocol.\nFigure: Schema - Somatic SV DNA Sequencing protocol\nGATK SAM to FastQ\nBWA Mem2 SAMTools Sort\nGATK Mark Duplicates\nSet Interval List\nGATK Haplotype Caller\nMerge and call individual GVCF\nCombine GVCF\nMerge and call combined GVCF\nVariant Recalibrator\nHaplotype caller decompose and normalize\nHaplotype caller flag mappability\nHaplotype caller SNP ID annotation\nHaplotype caller SNP Effect\nHaplotype caller dbNSFP annotation\nHaplotype caller Gemini annotation\nMetrics DNA Picard\nDNA Sample MosDepth Metrics\nPicard Calculate HS Metrics\nMetrics Verify BAM ID\nRun MultiQC\nSym Link FastQ\nSym Link Final BAM\nMetrics VCFTools Missing Individual\nMetrics VCFTools Depth Individual\nMetrics GATK Sample Fingerprint\nMetrics GATK Cluster Fingerprint\nGATK SAM to FastQ\nBWA Mem2 SAMTools Sort\nGATK Mark Duplicates\nSym Link Final BAM\nSet Interval List\nGATK Haplotype Caller\nMerge and call individual GVCF\nMetrics DNA Picard\nDNA Sample MosDepth Metrics\nPicard Calculate HS Metrics\nRun MultiQC\nDelly2 Call Filter\nDelly2 SV Annotation\nGermline Manta\nManta SV Annotation\nLumpy Paired SV\nLumpy SV Annotation\nWham SV Call\nWham SV Annotation\nCNVkit Batch\nCNVkit SV Annotation\nRun BreakSeq2\nEnsemble MetaSV\nMetaSV Annotation\nHigh Coverage\nGATK SAM to FastQ\nBWA Mem2 SAMTools Sort\nSAMTools Merge Files\nGATK Fixmate\nMetrics DNA Picard\nDNA Sample MosDepth Metrics\nPicard Calculate HS Metrics\nMetrics Verify BAM ID\nGermline Varscan2\nPreProcess VCF\nHaplotype caller Gemini annotation\nRun MultiQC\nCRAM Output\nGATK SAM to FastQ\nBWA Mem2 SAMTools Sort\nGATK Mark Duplicates\nSym Link Final BAM\nMetrics DNA Picard\nDNA Sample MosDepth Metrics\nPicard Calculate HS Metrics\nMetrics Verify BAM ID\nRun MultiQC\nSet Interval List\nGATK Haplotype Caller\nMerge and call individual GVCF\nCombine GVCF\nMerge and call combined GVCF\nVariant Recalibrator\nHaplotype caller decompose and normalize\nCNVkit Batch\nSplit Tumor Only\nFilter Tumor Only\nReport CPSR\nReport PCGR\nGATK SAM to FastQ\nBWA Mem2 SAMTools Sort\nGATK Mark Duplicates\nSet Interval List\nSequenza Step\nRaw Mpileup\nPaired Var Scan 2\nMerge Var Scan 2\nPreProcess VCF\nCNVkit Batch\nFilter Germline\nReport CPSR\nFilter Somatic\nReport PCGR\nConpair Concordance Contamination\nMetrics DNA Picard\nDNA Sample MosDepth Metrics\nRun MultiQC\nSym Link Report\nSym Link FASTQ Pair\nSym Link Panel\nCRAM Output\nGATK SAM to FastQ\nBWA Mem2 SAMTools Sort\nGATK Mark Duplicates\nSet Interval List\nConpair Concordance Contamination\nMetrics DNA Picard\nDNA Sample MosDepth Metrics\nSequenza Step\nManta SV Calls\nStrelka2 Paired Somatic\nStrelka2 Paired Germline\nStrelka2 Paired SnpEff\nPurple Ploidy Estimator\nRaw Mpileup\nPaired Var Scan 2\nMerge Var Scan 2\nPaired Mutect2\nMerge Mutect2\nVarDict Paired\nMerge Filter Paired VarDict\nEnsemble Somatic\nGATK Variant Annotator Somatic\nMerge GATK Variant Annotator Somatic\nEnsemble Germline Loh\nGATK Variant Annotator Germline\nMerge GATK Variant Annotator Germline\nCNVkit Batch\nFilter Germline\nReport CPSR\nFilter Somatic\nReport PCGR\nReport Djerba\nRun MultiQC\nSym Link FASTQ Pair\nSym Link Final BAM\nSym Link Report\nSym Link Ensemble\nCRAM Output\nGATK SAM to FastQ\nBWA Mem2 SAMTools Sort\nGATK Mark Duplicates\nSet Interval List\nManta SV Calls\nStrelka2 Paired Somatic\nGRIDSS Paired Somatic\nLinx Annotations Somatic\nLinx Annotations Germline\nRun MultiQC\nCRAM Output\nGATK SAM to FastQ\nConvert SAM/BAM files from the input readset file into FASTQ format if FASTQ files are not already specified in the readset file. Do nothing otherwise.\nFastP: A tool designed to provide fast all-in-one preprocessing for FastQ files. This tool is developed in C++ with multi-threading supported to afford high performance.\nBWA Mem2 SAMTools Sort\nThe filtered reads are aligned to a reference genome. The alignment is done per sequencing readset. The alignment software used is BWA-MEM2 with algorithm: bwa mem2. BWA output BAM files are then sorted by coordinate using SAMTools This step takes as input files:\nTrimmed FASTQ files if available\nElse, FASTQ files from the readset file if available\nElse, FASTQ output files from previous picard_sam_to_fastq conversion of BAM files\nGATK Mark Duplicates\nThis step marks duplicates. Aligned reads per sample are duplicates if they have the same 5’ alignment positions (for both mates in the case of paired-end reads). All but the best pair (based on alignment score) will be marked as a duplicate in the BAM file. Marking duplicates is done using GATK.\nSet Interval List\nCreate an interval list with ScatterIntervalsByNs from GATK: GATK.\nUsed for creating a broken-up interval list that can be used for scattering a variant-calling pipeline in a way that will not cause problems at the edges of the intervals. By using large enough N blocks (so that the tools will not be able to anchor on both sides) we can be assured that the results of scattering and gathering the variants with the resulting interval list will be the same as calling with one large region.\nGATK Haplotype Caller\nGATK Haplotype Caller is used for SNPs and small indels.\nMerge and call individual GVCF\nMerges the gvcfs of haplotype caller and also generates a per sample vcf containing genotypes.\nCombine GVCF\nCombine the per sample gvcfs of haplotype caller into one main file for all sample.\nMerge and call combined GVCF\nMerges the combined gvcfs and also generates a general vcf containing genotypes.\nVariant Recalibrator\nThis step involves GATK variant recalibrator. The purpose of the variant recalibrator is to assign a well-calibrated probability to each variant call in a call set. You can then create highly accurate call sets by filtering based on this single estimate for the accuracy of each call. The approach taken by variant quality score recalibration is to develop a continuous, covarying estimate of the relationship between SNP call annotations (QD, MQ, HaplotypeScore, and ReadPosRankSum, for example) and the probability that a SNP is a true genetic variant versus a sequencing or data processing artifact. This model is determined adaptively based on “true sites” provided as input, typically HapMap 3 sites and those sites found to be polymorphic on the Omni 2.5M SNP chip array. This adaptive error model can then be applied to both known and novel variation discovered in the call set of interest to evaluate the probability that each call is real. The score that gets added to the INFO field of each variant is called the VQSLOD. It is the log odds ratio of being a true variant versus being false under the trained Gaussian mixture model. Using the tranche file generated by the previous step the ApplyRecalibration walker looks at each variant’s VQSLOD value and decides which tranche it falls in. Variants in tranches that fall below the specified truth sensitivity filter level have their filter field annotated with its tranche level. This will result in a call set that simultaneously is filtered to the desired level but also has the information necessary to pull out more variants for a higher sensitivity but a slightly lower quality level.\nHaplotype caller decompose and normalize\nPerforms decompose/normalization for variant comparison at the haplotype level. Replay the variants from the VCF into the reference and determine whether variants match by whether the resulting Haplotype match.\nHaplotype caller flag mappability\nMappability annotation applied to haplotype caller vcf. An in-house database identifies regions in which reads are confidently mapped to the reference genome.\nHaplotype caller SNP ID annotation\ndbSNP annotation applied to haplotype caller vcf. The .vcf files are annotated for dbSNP using the software SnpSift (from the SNPEff Suite).\nHaplotype caller SNP Effect\nVariant effect annotation applied to haplotype caller vcf. The .vcf files are annotated for variant effects using the SNPEff Suite software. SnpEff annotates and predicts the effects of variants on genes (such as amino acid changes).\nHaplotype caller dbNSFP annotation\nAdditional SVN annotations applied to haplotype caller vcf. Provides extra information about SVN by using numerous published databases. Applicable to human samples. Databases available include Biomart (adds GO annotations based on gene information) and dbNSFP (an integrated database of functional annotations from multiple sources for the comprehensive collection of human non-synonymous SNPs. It compiles prediction scores from four prediction algorithms (SIFT, Polyphen2, LRT and MutationTaster), three conservation scores (PhyloP, GERP++ and SiPhy) and other function annotations).\nHaplotype caller Gemini annotation\nUses Haplotype caller to call germline SNPs and indels via local re-assembly of Haplotype for exploring genetic variations using the Gemini annotations.\nMetrics DNA Picard\nThis step generates metrics with picard, including CollectMultipleMetrics, CollectOxoGMetrics, CollectGcBiasMetrics, CollectWgsMetrics, CollectHsMetrics, CollectInsertSizeMetrics, CollectSequencingArtifactMetrics, CollectQualityYieldMetrics, CollectQualityByCycle, and  CollectBaseDistributionByCycle.\nDNA Sample MosDepth Metrics\nCalculate depth stats for captured regions with Mosdepth.\nPicard Calculate HS Metrics\nCompute on target percent of hybridization based capture happens in this step.\nMetrics Verify BAM ID\nIn this step, VerifyBAMID software is used to verify whether the reads in particular file match previously known genotypes for an individual (or group of individuals), and checks whether the reads are contaminated as a mixture of two samples.\nRun MultiQC\nAggregate results from bioinformatics analyses across many samples into a single report. MultiQC searches a given directory for analysis logs and compiles a HTML report. It’s a general use tool, perfect for summarizing the output from numerous bioinformatics tools.\nReport Djerba\nThis step generates the Djerba <https://github.com/oicr-gsi/djerba> report for the analysis.\nSym Link FastQ\nAutomated symbolic linking of FASTQ files.\nSym Link Final BAM\nCreates symbolic links of final BAM for delivery of data to the clients.\nMetrics VCFTools Missing Individual\nThis step uses VCFtools and –missing_indv option to generate a file reporting the missingness factor in the analysis on a per-individual basis. It takes bgzipped .vcf file as input and creates .imiss flat file indicating missingness.\nMetrics VCFTools Depth Individual\nThis step uses VCFtools and –depth option to generate a file containing the mean depth per individual. It takes as input bgzipped .vcf file and creates a .idepth flat file.\nMetrics GATK Sample Fingerprint\nCrosscheckFingerprints (Picard) functionality in GATK toolkit is used to cross-check readgroups, libraries, samples, or files to determine if all data in the set of input files appears to come from the same individual. In this step, sample SAM/BAM or VCF file is taken as input and a fingerprint file is generated using CrosscheckFingerprints (Picard) in GATK. It checks the sample identity of the sequence/genotype data in the provided file (SAM/BAM or VCF) against a set of known genotypes in the supplied genotype file (in VCF format).\nMetrics GATK Cluster Fingerprint\nIn this step, ClusterCrosscheckMetrics function from GATK is used as a follow-up step to running CrosscheckFingerprints created in the Metrics GATK Sample Fingerprint step earlier. There are cases where one would like to identify a few groups out of a collection of many possible groups (say to link a bam to it’s correct sample in a multi-sample vcf. In this step, sample SAM/BAM or VCF file is taken as input and a fingerprint file is generated.\nDelly2 Call Filter\nThis step uses normal and tumor final BAMs as input and generates a binary variant call format (BCF) file as output. It utilizes Delly2, an integrated structural variant (SV) prediction method that can discover genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. It uses paired-ends, split-reads and read-depth to sensitively and accurately delineate genomic rearrangements throughout the genome. Structural variants can be annotated using Delly-sansa and visualized using Delly-maze or Delly-suave.\nDelly2 SV Annotation\nSV Annotation step utilizes the BCF file generated in previous Delly2 Call Filter step and performs genome annotation at various levels. At the nucleotid level it tries to identify the physical location of the SV dna sequences. Next, at the protein level the annotation process tries to determine the possible functions of the SV genes. Lastly, at the process-level annotation, it tries to identify the pathways and processes in which different SV genes interact, assembling an efficient functional annotation. For more details on annotation see Genome Annotations.\nGermline Manta\nManta calls structural variants (SVs) and indels from mapped paired-end sequencing reads. It is optimized for analysis of germline variation in small sets of individuals and somatic variation in tumor/normal sample pairs. Manta discovers, assembles and scores large-scale SVs, medium-sized indels and large insertions within a single efficient workflow. Manta accepts input read mappings from BAM or CRAM files and reports all SV and indel inferences in VCF 4.1 format.\n** Manta SV Annotation**\nThis step uses the VCF file generated in previous step and performs SV annotations to compares types and breakpoints for candidate SVs from different callsets and enables fast comparison of SVs to genomic features such as genes and repetitive regions, as well as to previously established SV datasets.\nLumpy Paired SV\nThis step uses Lumpy for structural variant discovery in the given input file. The output is available in BAM format.\nComprehensive discovery of structural variation (SV) from whole genome sequencing data requires multiple detection signals including read-pair, split-read, read-depth and prior knowledge. Owing to technical challenges, extant SV discovery algorithms either use one signal in isolation, or at best use two sequentially. Lumpy is a novel SV discovery framework that naturally integrates multiple SV signals jointly across multiple samples. It yields improved sensitivity, especially when SV signal is reduced owing to either low coverage data or low intra-sample variant allele frequency.\nLumpy SV Annotation\nThis step performs LumPy SV Annotation for mapping and characterization of SVs.\nWham SV Call\nWham (Whole-genome Alignment Metrics) provides a single, integrated framework for both structural variant calling and association testing, thereby bypassing many of the difficulties that currently frustrate attempts to employ SVs in association testing.  This step returns a VCF file.\nWham SV Annotation\nThis step uses the VCF file generated in the previous step Wham SV Call and performs SV annotations.\nCNVkit Batch\nA copy number variation (CNV) is when the number of copies of a particular gene varies from one individual to the next. Copy-number variation (CNV) is a large category of structural variation, which includes insertions, deletions and duplications. For copy number variation analysis, GenPipes DNA Sequencing pipeline (-t sv option) uses CNVkit that allows for CNV calling on single samples (e.g., tumor samples).\nCNVkit provides an advantageous way to run the entire pipeline using the batch option to run various stages in copy number calling pipeline such as:\nCreate target/anti-target bed files\nGather read depths for those regions\nCompile a copy number reference\nCorrect biases in tumor samples while calculating copy ratios\nMark copy number segments\nCNVkit SV Annotation\nThis step performs CNVkit SV annotation.\nRun BreakSeq2\nIn this step, BreakSeq2 is used to combine DNA double-strand breaks (DSBs) labeling with next generation sequencing (NGS) to map chromosome breaks with improved sensitivity and resolution. It is an ultra fast and accurate nucleotide-resolution analysis of structural variants.\nEnsemble MetaSV\nMetaSV uses highly effective ensemble approach for calling SVs. It is an integrated SV caller which leverages multiple orthogonal SV signals for high accuracy and resolution. MetaSV proceeds by merging SVs from multiple tools for all types of SVs. It also analyzes soft-clipped reads from alignment to detect insertions accurately since existing tools underestimate insertion SVs. Local assembly in combination with dynamic programming is used to improve breakpoint resolution. Paired-end and coverage information is used to predict SV genotypes.\nMetaSV Annotation\nThis step uses output from previous step and performs SV annotations.\nSAMTools Merge Files\nBAM readset files are merged into one file per sample. Merge is done using SAMTools.\nThis step takes as input files:\nAligned and sorted BAM output files from previous bwa_mem_picard_sort_sam step if available\nElse, BAM files from the readset file\nGATK Fixmate\nVerify mate-pair information between mates and fix if needed. This ensures that all mate-pair information is in sync between each read and its mate pair. Fix is done using Picard.\nGermline Varscan2\nThis step uses VarScan caller for insertions and deletions.\nPreprocess VCF\nPreprocess VCF for loading into an annotation database - Gemini. Processes include normalization and decomposition of MNPs by Vt and VCF format modification for correct loading into Gemini.\nVariant effect annotation. The .vcf files are annotated for variant effects using the SnpEff software. SnpEff annotates and predicts the effects of variants on genes (such as amino acid changes).\ninput_file (str)\nThe input vcf file to annotate for variant effects. Default is allSamples.hc.vqsr.vt.mil.snpId.vcf.gz.\noutput (str)\nThe output vcf file. Default is allSamples.hc.vqsr.vt.mil.snpId.snpeff.vcf. job_name (str): The name of the job. Default is snp_effect.hc.\nSplit Tumor Only\nSplits the merged VCF produced in previous steps to generate a report on a per-patient basis. The merged VCF is split using the BCFTools split function with the removal of homozygous reference calls. Creates one VCF per patient to be used for downstream reporting.\nFilter Tumor Only\nApplies custom script to inject FORMAT information - tumor/normal DP and VAP into the INFO field the filter on those generated fields.\nReport CPSR\nCreates a CPSR Germline Report where input is the filtered ensemble germline vcf and output is the html report and additional flat files.\nReport PCGR\nCreates a PCGR Somatic Germline Report where input is the filtered ensemble germline vcf and the output is the html report and additional flat files.\nSequenza Step\nSequenza is a novel set of tools providing a fast python script to genotype cancer samples, and an R package to estimate cancer cellularity, ploidy, genome wide copy number profile and infer for mutated alleles.\nRaw Mpileup\nFull pileup (optional). A raw Mpileup file is created using samtools Mpileup and compressed in gzipped format. One packaged Mpileup file is created per sample/chromosome.\nPaired Var Scan 2\nVariant calling and somatic mutation/CNV detection for next-generation sequencing data. Uses VarScan 2 for Somatic mutation and copy number alteration discovery in cancer by exome sequencing VarScan 2 thresholds based on DREAM3 results generated by SC INFO field remove to prevent collision with Samtools output during ensemble.\nMerge Var Scan 2\nMerge Mpileup files per sample/chromosome into one compressed gzip file per sample.\nFilter Germline\nApplies custom script to inject FORMAT information - tumor/normal DP and VAP into the INFO field the filter on those generated fields.\nFilter Somatic\nApplies custom script to inject FORMAT information - tumor/normal DP and VAP into the INFO field the filter on those generated fields.\nConpair Concorance Contamination\nConpair is a concordance and contamination estimator for tumor–normal pairs. This step is a quality control process to ensure the normal sample and cancer sample come from the same patient. It estimates this by determining the amount of normal sample in the tumor and the amount of tumor in normal sample.\nSym Link Report\nGenerates symbolic links for Tumor Pair pipeline report.\nSym Link FASTQ Pair\nGenerates symbolic links for FASTQ Pair output files.\nSym Link Panel\nThis step create symbolic links of panel variants for generating reports and deliverable to the clients.\nSym Link Ensemble\nGenerates symbolic links for Ensemble processing output.\nManta SV Calls\nManta calls structural variants (SVs) and indels from mapped paired-end sequencing reads. It is optimized for analysis of germline variation in small sets of individuals and somatic variation in tumor/normal sample pairs.  Manta discovers, assembles and scores large-scale SVs, medium-sized indels and large insertions within a single efficient workflow.\nManta accepts input read mappings from BAM or CRAM files and reports all SV and indel inferences in VCF 4.1 format\nStrelka2 Paired Somatic\nStrelka2 is a fast and accurate small variant caller optimized for analysis of germline variation in small cohorts and somatic variation in tumor/normal sample pairs This implementation is optimized for somatic calling.\nStrelka2 Paired Germline\nStrelka2 is a fast and accurate small variant caller optimized for analysis of germline variation in small cohorts and somatic variation in tumor/normal sample pairs This implementation is optimized for germline calling in cancer pairs.\nStrelka2 Paired SnpEff\nStrelka2 is a fast and accurate small variant caller optimized for analysis of germline variation in small cohorts and somatic variation in tumor/normal sample pairs This implementation is optimized for germline calling in cancer pairs.\nPurple Ploidy Estimator\nPurple is a purity ploidy estimator for whole genome sequenced (WGS) data. It combines B-allele frequency (BAF) from AMBER, read depth ratios from COBALT, somatic variants and structural variants to estimate the purity and copy number profile of a tumor sample.\nRuns PURPLE with the optional structural variant input VCFs. PURPLE is a purity ploidy estimator for whole genome sequenced (WGS) data.\nIt combines B-allele frequency (BAF) from AMBER, read depth ratios from COBALT, somatic variants and structural variants to estimate the purity and copy number profile of a tumor sample.\nPaired Mutect2\nGATK Mutect2 Overview caller for SNVs and Indels.\nMerge Mutect2\nMerge SNVs and indels for GATK MuTect2 Overview Replace TUMOR and NORMAL sample names in VCF to the exact tumor/normal sample names Generate a somatic VCF containing only PASS variants\nVarDict Paired\nIn this step, VarDict caller is used for SNVs and Indels. Note: variants are filtered to remove instance where REF == ALT and REF modified to ‘N’ when REF is AUPAC nomenclature\nMerge Filter Paired VarDict\nThe fully merged VCF is filtered using following steps:\nRetain only variants designated as somatic by VarDict: either StrongSomatic or LikelySomatic\nSomatics identified in step 1 must have PASS filter.\nEnsemble Somatic\nApply Bcbio.variations ensemble approach for GATK MuTect2 Overview, VarDict, Samtools and VarScan 2 calls Filter ensemble calls to retain only calls overlapping 2 or more callers\nGATK Variant Annotator Somatic\nAdd VCF annotations to ensemble VCF: Standard and Somatic annotations.\nMerge GATK Variant Annotator Somatic\nMerge annotated somatic VCFs.\nEnsemble Germline Loh\nThis step applies Bcbio.variations ensemble approach for VarDict, Samtools and VarScan 2 calls. It also filters ensemble calls to retain only calls overlapping 2 or more callers.\nGATK Variant Annotator Germline\nAdd VCF annotations to ensemble VCF: most importantly the AD field.\nMerge GATK Variant Annotator Germline\nMerge annotated germline and LOH VCFs.\nGRIDSS Paired Somatic\nGRIDSS is a module software suite containing tools useful for the detection of genomic rearrangements. GRIDSS includes a genome-wide break-end assembler, as well as a structural variation caller for Illumina sequencing data. GRIDSS calls variants based on alignment-guided positional de Bruijn graph genome-wide break-end assembly, split read, and read pair evidence.\nLinx Annotations Somatic\nThe LINX algorithm classifies somatic structural variation in tumors. It is an annotation, interpretation and visualization tool for structural variants. The primary function is grouping together individual SV calls into distinct events and properly classify and annotating the event to understand both its mechanism and genomic impact.\nLINX helps in the interpretation of structural variant and copy number data derived from short-read, whole-genome sequencing. It classifies raw structural variant calls into distinct events and predicts their effect on the local structure of the derivative chromosome and the functional impact on affected genes. Visualizations facilitate further investigation of complex rearrangements. LINX allows insights into a diverse range of structural variation events and can reliably detect pathogenic rearrangements, including gene fusions, immunoglobulin enhancer rearrangements, intragenic deletions, and duplications.\nLinx Annotations Germline\nThough LINX is designed primarily for somatic SV, it can also be run in a more limited germline mode to annotate and interpret germline rearrangements\nLINX Plot offers visualization methods that provide insight into complex genomic rearrangements. It leverages the integrated structural variant and copy number calling to cluster individual structural variants into higher order events and chains them together to predict local derivative chromosome structure.\nCRAM Output\nGenerate long term storage version of the final alignment files in CRAM format. Using this function will include the original final BAM file into the removable file list.\nDNA sequencing is the process of determining the sequence of nucleotides in a section of DNA. Over the past decade, long-read, single-molecule DNA sequencing technologies have emerged as powerful players in genomics. With the ability to generate reads tens to thousands of kilobases in length with an accuracy approaching that of short-read sequencing technologies, these platforms have proven their ability to resolve some of the most challenging regions of the human genome, detect previously inaccessible structural variants and generate some of the first telomere-to-telomere assemblies of whole chromosomes.\nBurrows-Wheeler Alignment tool, BWA, is a new read alignment package that is based on backward search with Burrows–Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA is ∼10–20 times faster than Mapping and Assembly with Quality, MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open-source SAMtools software package.\nDNA Sequencing GenPipes pipeline has been implemented by optimizing the Genome Analysis Toolkit, GATK best practices standard operating protocols. This procedure entails trimming raw reads derived from whole-genome or exome data followed by alignment to a known reference, post-alignment refinements, and variant calling. Trimmed reads are aligned to a reference by the Burrows-Wheeler Aligner (BWA), bwa-mem. Refinements of mismatches near insertions and deletions (indels) and base qualities are performed using GATK indels realignment and base recalibration to improve read quality after alignment. Processed reads are marked as fragment duplicates using Picard MarkDuplicates and single-nucleotide polymorphisms and small indels are identified using either GATK haplotype callers or SAMtools mpileup.\nThe Genome in a Bottle dataset was used to select steps and parameters minimizing the false-positive rate and maximizing the true-positive variants to achieve a sensitivity of 99.7%, precision of 99.1%, and F1 score of 99.4%. Finally, additional annotations are incorporated using dbNSFP and / or Gemini and QC metrics are collected at various stages and visualized using MultiQC.\nThe standard MUGQIC DNA-Seq pipeline uses BWA to align reads to the reference genome. Treatment and filtering of mapped reads approaches as INDEL realignment, mark duplicate reads, recalibration and sort are executed using Picard and GATK. Samtools MPILEUP and bcftools are used to produce the standard SNP and indels variants file (VCF). Additional SVN annotations mostly applicable to human samples include mappability flags, dbSNP annotation and extra information about SVN by using published databases. The SNPeff tool is used to annotate variants using an integrated database of functional predictions from multiple algorithms (SIFT, Polyphen2, LRT and MutationTaster, PhyloP and GERP++, etc.) and to calculate the effects they produce on known genes. Refer to the list of SnpEff effects.\nGenPipes DNA sequencing pipeline offers the following protocol options:\nDefault, Germline Single Nucleotide Variant Analysis uses GATK HaplotypeCaller caller (-t germline_snv)\nGermline Structural Variations for cancer predisposition analysis Another  (-t germline_sv)\nGermline High Coverage for comprehensive variant detection (-t germline_high_cov)\nSomatic Tumor only analysis (-t somatic_tumor_only)\nQuick assessment using exome capture regions and the 1000bp flanking regions with Somatic FastPass (-t somatic_fastpass)\nSomatic Ensemble for detecting somatic mutations via the best combination of callers for both SNV and SV  (-t somatic_ensemble)\nSomatic Structural Variant (SV) detection (-t somatic_sv)\nSee Schema tab for the pipeline workflow. For the latest implementation and usage details refer to DNA Sequencing implementation README.md file.\nThree-stage quality control strategies for DNA sequencing\nNGS Mapping, errors and quality control\ndbNSFP: a lightweight database of human nonsynonymous SNPs and their functional predictions\nDNA-Seq Pipeline\nManta - Rapid Detection of Structural Variants, Using Manta for filtering and annotations.",
    "code_blocks": [
      "genpipes dnaseq [-options ] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh",
      "-t {germline_snv, germline_sv, germline_high_cov, somatic_tumor_only, somatic_fastpass, somatic_ensemble, somatic_sv},\n\n--type {germline_snv, germline_sv, germline_high_cov, somatic_tumor_only, somatic_fastpass, somatic_ensemble, somatic_sv}\n\n                          DNAseq analysis type (default=germline_snv)",
      "-p PAIRS, --pairs PAIRS\n\n                          pairs file\n                          format - patient_name,normal_sample_name,tumor_sample_name",
      "-d DESIGN, --design DESIGN\n\n                          design file",
      "-r READSETS, --readsets READSETS\n\n                          readset file",
      "-h                        show this help message and exit",
      "--help                    show detailed description of pipeline and steps",
      "-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\n\n                          config INI-style list of files; config parameters\n                          are overwritten based on files order",
      "-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'",
      "-o OUTPUT_DIR, --output-dir OUTPUT_DIR\n\n                          output directory (default: current)",
      "-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\n\n                          job scheduler type (default: slurm)",
      "-f, --force               force creation of jobs even if up to date (default:\n                          false)\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--force_mem_per_cpu       FORCE_MEM_PER_CPU\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--json-pt                 create JSON file for project_tracking database\n                          ingestion (default: false i.e. JSON file will NOT be\n                          created)",
      "--report                  create 'pandoc' command to merge all job markdown\n                          report files in the given step range into HTML, if\n                          they exist; if --report is set, --job-scheduler,\n                          --force, --clean options and job up-to-date status\n                          are ignored (default: false)",
      "--clean                   create 'rm' commands for all job removable files in\n                          the given step range, if they exist; if --clean is\n                          set, --job-scheduler, --force options and job up-to-\n                          date status are ignored (default: false)\n\n                          Note: Do not use -g option with --clean, use '>' to redirect\n                          the output of the --clean command option",
      "-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\n\n                          log level (default: info)",
      "--sanity-check            run the pipeline in `sanity check mode` to verify\n                          all the input files needed for the pipeline to run\n                          are available on the system (default: false)",
      "--container {wrapper, singularity} <IMAGE PATH>\n\n                          run pipeline inside a container providing a container\n                          image path or accessible singularity hub path",
      "--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\n                          genpipes/ressources/container/bin/container_wrapper.sh,\n                          a convenience option for using GenPipes in a container)",
      "-v, --version             show the version information and exit",
      "-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\n\n                          Commands for running the pipeline are output to this\n                          file pathname. The data specified to pipeline command\n                          line is processed and pipeline run commands are\n                          stored in GENPIPES_FILE, if this option is specified\n                          . Otherwise, the output will be redirected to stdout\n                          . This file can be used to actually \"run the\n                          GenPipes Pipeline\"",
      "tumorPair_CEPHmixture_chr19,tumorPair_CEPHmixture_chr19_normal,tumorPair_CEPHmixture_chr19_tumor\njohn_doe_tumorPair_CEPHmixture_chr19,john_doe_tumorPair_CEPHmixture_chr19_normal,john_doe_tumorPair_CEPHmixture_chr19_tumor\n\n...",
      "genpipes dnaseq -t germline_snv -c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n  $GENPIPES_INIS/common_ini/rorqual.ini \\\n  -r readset.dnaseq.txt \\\n  -s 1-27 -j slurm -g dnaseq_germline_snv_cmd.sh\n\nbash dnaseq_germline_snv_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes dnaseq -t germline_sv -c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n    $GENPIPES_INIS/dnaseq/dnaseq.sv.ini \\\n    $GENPIPES_INIS/common_ini/rorqual.ini \\\n    -r readset.dnaseq.txt \\\n    -s 1-25 -g dnaseq_germ_sv_cmd.sh\n\nbash dnaseq_germ_sv_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes dnaseq -t germline_high_cov \\\n    -c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n    $GENPIPES_INIS/dnaseq/dnaseq.high_cov.ini  \\\n    $GENPIPES_INIS/common_ini/rorqual.ini \\\n    -r readset.dnaseq.txt \\\n    -s 1-15 -g dnaseq_germ_high_cov_cmd.sh\n\nbash dnaseq_germ_high_cov_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes dnaseq -t somatic_tumor_only \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.somatic_tumor_only.txt \\\n-s 1-38 -j slurm -g dnaseq_somatic_tumor_cmd.sh\n\nbash dnaseq_somatic_tumor_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes dnaseq -t somatic_fastpass \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/dnaseq/dnaseq.cancer.ini  \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.somatic_fastpass.txt \\\n-p pairs.somatic_fastpass.csv \\\n-s 1-38 -j slurm -g dnaseq_somatic_fastpass_cmd.sh\n\nbash dnaseq_somatic_fastpass_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes dnaseq -t somatic_ensemble \\\n    -c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n    $GENPIPES_INIS/dnaseq/dnaseq.cancer.ini  \\\n    $GENPIPES_INIS/common_ini/rorqual.ini \\\n    -r readset.somatic_ensemble.txt \\\n    -p pairs.somatic_ensemble.csv \\\n    -s 1-38 -j slurm -g dnaseq_somatic_ensemble_cmd.sh\n\nbash dnaseq_somatic_ensemble_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes dnaseq -t somatic_sv \\\n-c $GENPIPES_INIS/dnaseq/dnaseq.base.ini \\\n$GENPIPES_INIS/dnaseq/dnaseq.cancer.ini  \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r readset.somatic_sv.txt \\\n-p pairs.somatic_sv.csv \\\n-s 1-38 -j slurm -g dnaseq_somatic_sv_cmd.sh\n\nbash dnaseq_somatic_sv_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes [pipeline] [options] -g genpipes_cmd.sh\n\nbash genpipes_cmd.sh",
      "genpipes [pipeline] [options] > genpipes_cmd.sh\n\nbash genpies_cmd.sh"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:15.415317",
    "content_length": 39570,
    "word_count": 5660
  },
  {
    "url": "https://genpipes.readthedocs.io/en/genpipes-v6.0.0/user_guide/pipelines/gp_longread_dnaseq.html",
    "title": "Long Read DNA Sequencing Pipeline",
    "content": "Long Read DNA Sequencing Pipeline\nVersion 6.0.0\nThe GenPipes v6.x release introduces the new Long Read DNA Sequencing Pipeline. It supports two protocols, Nanopore and Revio.\nThe Nanopore protocol offers the same sequencing pipeline that was earlier available as the Nanopore sequencing pipeline in the previous GenPipes releases (v5.x or older).\ngenpipes longread_dnaseq [-options ] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh\n-t {nanopore, revio},\n--type {nanopore, revio}\nLong Read DNA sequencing analysis type (default=nanopore)\n-d DESIGN, --design DESIGN\ndesign file\n-r READSETS, --readsets READSETS\nreadset file\n-h                        show this help message and exit\n--help                    show detailed description of pipeline and steps\n-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\nconfig INI-style list of files; config parameters\nare overwritten based on files order\n-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'\n-o OUTPUT_DIR, --output-dir OUTPUT_DIR\noutput directory (default: current)\n-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\njob scheduler type (default: slurm)\n-f, --force               force creation of jobs even if up to date (default:\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--force_mem_per_cpu       FORCE_MEM_PER_CPU\nTake the mem input in the ini file and force to have a\nminimum of mem_per_cpu by correcting the number of cpu\n(default: None)\n--json-pt                 create JSON file for project_tracking database\ningestion (default: false i.e. JSON file will NOT be\n--report                  create 'pandoc' command to merge all job markdown\nreport files in the given step range into HTML, if\nthey exist; if --report is set, --job-scheduler,\n--force, --clean options and job up-to-date status\nare ignored (default: false)\n--clean                   create 'rm' commands for all job removable files in\nthe given step range, if they exist; if --clean is\nset, --job-scheduler, --force options and job up-to-\ndate status are ignored (default: false)\nNote: Do not use -g option with --clean, use '>' to redirect\nthe output of the --clean command option\n-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\nlog level (default: info)\n--sanity-check            run the pipeline in `sanity check mode` to verify\nall the input files needed for the pipeline to run\nare available on the system (default: false)\n--container {wrapper, singularity} <IMAGE PATH>\nrun pipeline inside a container providing a container\nimage path or accessible singularity hub path\n--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\ngenpipes/ressources/container/bin/container_wrapper.sh,\na convenience option for using GenPipes in a container)\n-v, --version             show the version information and exit\n-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\nCommands for running the pipeline are output to this\nfile pathname. The data specified to pipeline command\nline is processed and pipeline run commands are\nstored in GENPIPES_FILE, if this option is specified\n. Otherwise, the output will be redirected to stdout\n. This file can be used to actually \"run the\nGenPipes Pipeline\"\nDo not use -g option with -clean.\nUse ‘>’ to redirect the output of the genpipes command to a file when using -clean option.\ngenpipes longread_dnaseq -t nanopore \\\n-c $GENPIPES_INIS/longread/longread.base.ini $GENPIPES_INIS/common_ini/rorqual.ini \\\n-r /cvmfs/soft.mugqic/root/testdata/nanopore/readset.nanopore.txt \\\n-o nanopore -g longread_nanopore_cmd.sh\nbash longread_nanopore_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\ngenpipes longread_dnaseq -t revio \\\n-c $GENPIPES_INIS/longread/longread.base.ini \\\n$GENPIPES_INIS/common_ini/rorqual.ini \\\n-r /cvmfs/soft.mugqic/root/testdata/revio/readset.revio.txt \\\n-o revio -g longread_revio_cmd.sh\nbash longread_revio_cmd.sh\nDepending upon the cluster where you are executing the pipeline, substitute the file name rorqual.ini in the command with the appropriate <DRAC server cluster name>.ini file located in the $GENPIPES_INIS/common_ini folder.\nFor e.g., narval.ini, fir.ini, or graham.ini.\nIt is recommended that you use the -g GENPIPES_CMD.sh option instead of redirecting the output of the pipeline command to a file via > GENPIPES_CMD.sh.\ngenpipes [pipeline] [options] -g genpipes_cmd.sh\nbash genpipes_cmd.sh\ngenpipes [pipeline] [options] > genpipes_cmd.sh\nbash genpies_cmd.sh\nThe > scriptfile method is supported but will be deprecated in a future GenPipes release.\nTest Dataset\nUse the Long Read DNA sequencing test dataset for this pipeline.\nTest Datasets\nThe following figure shows the schema for Nanopore sequencing pipeline:\nFigure: Schema of Nanopore Long Read DNA Sequencing Protocol\nThe following figure shows the schema for Revio Long Read DNA Sequencing Protocol:\nFigure: Schema of Nanopore Sequencing protocol\nMinimap2 Align\nPicard Merge SAM Files\nStructural Variant Identification using Mapped Long Reads\nMetrics Nanoplot\npbmm2 Align\nPicard Merge SAM Files\nMetrics Mosdepth\nSet DeepVariant Regions\nDeepVariant Germline VC\nMerge Filter Deepvariant\nTarget Genotyping\nAnnotate SV\nReport CPSR\nIn this step, Blast-QC utility is used for sequence alignment and identification. It performs a basic QC test by aligning 1000bp of randomly selected reads to the NCBI Nucleotide Database in order to detect potential contamination.\nMinimap2 Align\nMinimap2 Align Program is a fast, general purpose sequencing alignment program that maps DNA and long mRNA sequences against a large reference database.  It can be used for Nanopore sequencing for mapping 1kb genomic reads at an error rate of 15% (e.g., PacBio or Oxford Nanopore genomic reads), among other uses.\nIn this step,  minimap2 to align the FastQ reads that passed the minimum QC threshold to the provided reference genome. By default, it aligns to the human genome reference (GRCh38) with Minimap2.\nIn this step, pycoQC Software is used  produce an interactive quality report based on the summary file and alignment outputs. PycoQC relies on the sequencing_summary.txt file generated by Guppy. If needed, it can also generate a summary file from basecalled FAST5 files. PycoQC computes metrics and generates interactive QC plots for Oxford Nanopore technologies sequencing data.\nPicard Merge SAM Files\nBAM readset files are merged into one file per sample in this step. Using aligned and sorted BAM output files from Minimap2 Align step, it performs the merge using Picard.\nStructural Variant Identification using Mapped Long Reads\nIn this step, Structural Variant Identification using Mapped Long Reads (SVIM methodology), is used to perform structural variant (SV) analysis on each sample.\nMetrics Nanoplot\nThis step collects QC metrics on unaligned BAM or FastQ files with Nanoplot.\npbmm2 Align\nUses pbmm2 to align fastq files or the raw HiFi BAM to the reference. Sorted output can be used directly for polishing using GenomicConsensus, if BAM has been used as input to pbmm2.\nMetrics Mosdepth\nCalculate depth stats with mosdepth.\nSet DeepVariant Regions\nCreate an interval list with ScatterIntervalsByNs from GATK. Used for creating a broken-up interval list that can be used for scattering a variant-calling pipeline in a way that will not cause problems at the edges of the intervals. By using large enough N blocks (so that the tools will not be able to anchor on both sides) we can be assured that the results of scattering and gathering the variants with the resulting interval list will be the same as calling with one large region.\nDeepVariant Germline VC\nGermline variant calling with DeepVariant.\nMerge Filter DeepVariant\nMerge DeepVariant outputs from the previous step, if applicable, and filter vcf.\nCall copy number variation and visualize results with HiFiCNV.\nTarget Genotyping\nCall tandem repeats for pathogenic and full repeats with trgt for targeted genotyping.\nCall structural variants from mapped HiFi sequencing reads with Sawfish SV Caller.\nAnnotate SV\nAnnotate and rank structural variants with AnnotSV.\nPhase variant calls with HiPhase.\nReport CPSR\nCreates a CPSR germline report. It takes annotated/filter VCF as the input and outputs an html report along with additional flat files.\nThe MultiQC aggregator collates results from bioinformatics analyses across many samples into a single report. It searches for a given directory for analysis logs and compiles a HTML report. This is a tool for general usage, perfect for summarizing the output from numerous bioinformatics tools.\nOver the past decade, long-read, single-molecule DNA sequencing technologies have emerged as powerful players in genomics. With the ability to generate reads tens to thousands of kilobases in length with an accuracy approaching that of short-read sequencing technologies, these platforms have proven their ability to resolve some of the most challenging regions of the human genome, detect previously inaccessible structural variants, and generate some of the first telomere-to-telomere assemblies of whole chromosomes.\nThe LongRead Pipeline is used to analyze long reads produced by the Oxford Nanopore Technologies (ONT) and PacBio Revio sequencers. It supports the following protocols:\nBoth protocols require a readset file as input. The readset file for the Long Read DNA Seq pipeline has a specific structure and format containing the sample metadata and paths to input data (FASTQ, FAST5 or BAM).\nThe Nanopore protocol of the pipeline uses minimap2 to align reads to the reference genome. Additionally, it produces a QC report that includes an interactive dashboard with data from the basecalling summary file as well as the alignment. A step aligning random reads to the NCBI nt database and reporting the species of the highest hits is also done as QC.\nOnce the QC and alignments have been produced, Picard is used to merge readsets coming from the same\nsample. Finally, SVIM is used to detect Structural Variants (SV) including deletions, insertions and\ntranslocations.\nFor a full summary of the types of SVs detected, refer to this site.\nThe SV calls produced by SVIM are saved as VCFs for each sample, which can then be used in downstream\nanalyses. No filtering is performed on the SV calls.\nThis pipeline currently does not perform base calling and requires both FASTQ and a sequencing_summary\nfile produced by a ONT supported basecaller (we recommend Guppy). Additionally, the testing and\ndevelopment of the pipeline were focused on genomics applications, and functionality has not been tested\nfor transcriptomics or epigenomics datasets.\nFor more information on using ONT data for structural variant detection, as well as an alternative\napproach, refer to Structural Variant Pipeline GitHub repository.\nThe Revio protocol uses pbmm2 to align reads to the reference genome, followed by variant calling with DeepVariant\nand structural variant calling with HiFiCNV, TRGT, and Sawfish. Variants are annotated with AnnotSV and phased\nwith HiPhase. A CPSR report can be produced from the phased variants. Metrics on the raw and mapped reads are\ncollected with NanoPlot and mosdepth, respectively.\nSee Schema tab for the pipeline workflow. For the latest implementation and usage details refer to the Long Read DNA Sequencing implementation README.md file.\nEvaluating nanopore sequencing data processing pipelines for structural variation identification.\nMinimap2: Pairwise alignment for nucleotide sequences.\nBasecalling using Guppy.",
    "code_blocks": [
      "genpipes longread_dnaseq [-options ] [--genpipes_file GENPIPES_FILE.sh]\nbash GENPIPES_FILE.sh",
      "-t {nanopore, revio},\n\n--type {nanopore, revio}\n\n                          Long Read DNA sequencing analysis type (default=nanopore)",
      "-d DESIGN, --design DESIGN\n\n                          design file",
      "-r READSETS, --readsets READSETS\n\n                          readset file",
      "-h                        show this help message and exit",
      "--help                    show detailed description of pipeline and steps",
      "-c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]\n\n                          config INI-style list of files; config parameters\n                          are overwritten based on files order",
      "-s STEPS, --steps STEPS   step range e.g. '1-5', '3,6,7', '2,4-8'",
      "-o OUTPUT_DIR, --output-dir OUTPUT_DIR\n\n                          output directory (default: current)",
      "-j {pbs,batch,daemon,slurm}, --job-scheduler {pbs,batch,daemon,slurm}\n\n                          job scheduler type (default: slurm)",
      "-f, --force               force creation of jobs even if up to date (default:\n                          false)\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--force_mem_per_cpu       FORCE_MEM_PER_CPU\n\n                          Take the mem input in the ini file and force to have a\n                          minimum of mem_per_cpu by correcting the number of cpu\n                          (default: None)",
      "--json-pt                 create JSON file for project_tracking database\n                          ingestion (default: false i.e. JSON file will NOT be\n                          created)",
      "--report                  create 'pandoc' command to merge all job markdown\n                          report files in the given step range into HTML, if\n                          they exist; if --report is set, --job-scheduler,\n                          --force, --clean options and job up-to-date status\n                          are ignored (default: false)",
      "--clean                   create 'rm' commands for all job removable files in\n                          the given step range, if they exist; if --clean is\n                          set, --job-scheduler, --force options and job up-to-\n                          date status are ignored (default: false)\n\n                          Note: Do not use -g option with --clean, use '>' to redirect\n                          the output of the --clean command option",
      "-l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical}\n\n                          log level (default: info)",
      "--sanity-check            run the pipeline in `sanity check mode` to verify\n                          all the input files needed for the pipeline to run\n                          are available on the system (default: false)",
      "--container {wrapper, singularity} <IMAGE PATH>\n\n                          run pipeline inside a container providing a container\n                          image path or accessible singularity hub path",
      "--wrap [WRAP]             path to the GenPipes cvmfs wrapper script (default:\n                          genpipes/ressources/container/bin/container_wrapper.sh,\n                          a convenience option for using GenPipes in a container)",
      "-v, --version             show the version information and exit",
      "-g GENPIPES_FILE, --genpipes_file GENPIPES_FILE\n\n                          Commands for running the pipeline are output to this\n                          file pathname. The data specified to pipeline command\n                          line is processed and pipeline run commands are\n                          stored in GENPIPES_FILE, if this option is specified\n                          . Otherwise, the output will be redirected to stdout\n                          . This file can be used to actually \"run the\n                          GenPipes Pipeline\"",
      "genpipes longread_dnaseq -t nanopore \\\n-c $GENPIPES_INIS/longread/longread.base.ini $GENPIPES_INIS/common_ini/rorqual.ini \\\n-r /cvmfs/soft.mugqic/root/testdata/nanopore/readset.nanopore.txt \\\n-o nanopore -g longread_nanopore_cmd.sh\n\nbash longread_nanopore_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes longread_dnaseq -t revio \\\n    -c $GENPIPES_INIS/longread/longread.base.ini \\\n    $GENPIPES_INIS/common_ini/rorqual.ini \\\n    -r /cvmfs/soft.mugqic/root/testdata/revio/readset.revio.txt \\\n    -o revio -g longread_revio_cmd.sh\n\nbash longread_revio_cmd.sh",
      "<DRAC server cluster name>.ini",
      "$GENPIPES_INIS/common_ini",
      "genpipes [pipeline] [options] -g genpipes_cmd.sh\n\nbash genpipes_cmd.sh",
      "genpipes [pipeline] [options] > genpipes_cmd.sh\n\nbash genpies_cmd.sh"
    ],
    "internal_links": [],
    "scraped_at": "2025-08-26T10:00:16.687606",
    "content_length": 11784,
    "word_count": 1710
  }
]